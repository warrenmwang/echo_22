{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8df8484-02e5-419c-a0ae-6ae546ea9317",
   "metadata": {},
   "source": [
    "## 1. preliminary work, load in data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe1dfa0-7788-44cb-aed3-d0eec8ab5ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.28it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.84it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Pretrained_R2plus1DMotionSegNet.pth has 31575731 parameters.\n",
      "1\n",
      "16\n",
      "[117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n",
      "delta_ed_es: 16\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"Original_Pretrained_R2plus1DMotionSegNet.pth\"]\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\")\n",
    "print(os.getcwd())\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v1 dropout, not in place dropout\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)\n",
    "\n",
    "\n",
    "loaded_in_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_save_path = f\"save_models/{model_name}\"\n",
    "     \n",
    "    \n",
    "    if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = R2plus1D_18_MotionNet()\n",
    "    elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "        \n",
    "    elif model_name == \"dropout_v3_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "\n",
    "    model = torch.nn.DataParallel(model_template_obj)\n",
    "\n",
    "\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))\n",
    "\n",
    "\n",
    "\n",
    "# test_pat_index = np.random.randint(len(test_dataset))\n",
    "test_pat_index = 0 \n",
    "\n",
    "video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "\n",
    "\n",
    "\n",
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "# goes thru a video and annotates where we can start clips given video length, clip length, etc.\n",
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "print(len(possible_starts))\n",
    "print(possible_starts)\n",
    "\n",
    "\n",
    "delta_ed_es = es_index - ed_index\n",
    "print(f'delta_ed_es: {delta_ed_es}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49840a83-dea6-4824-a6e7-387c4ff129af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 248, 112, 112), 132, 148)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape, ed_index, es_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c4b98d-3d25-46b0-bb55-9de120399c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a841d0a9-abf3-4667-95d3-3f46f67d9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2dmotion_field_PLAY(x, offset):\n",
    "    # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "    # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "    # print(x.shape)\n",
    "    \n",
    "    \n",
    "    x_shape = x.shape\n",
    "    # print(f'x_shape: {x_shape}')\n",
    "    \n",
    "    grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "    # print(f'grid_w.shape (meshgrid): {grid_w.shape}')\n",
    "    # print(f'grid_h.shape (meshgrid): {grid_h.shape}')\n",
    "    \n",
    "    # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "    # floating point precision\n",
    "    grid_w = grid_w.cuda().float()\n",
    "    grid_h = grid_h.cuda().float()\n",
    "    \n",
    "    # print(f'grid_w.shape .cuda().float(): {grid_w.shape}')\n",
    "    # print(f'grid_h.shape .cuda().float(): {grid_h.shape}')\n",
    "\n",
    "    grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "    grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "    # print(f'grid_w.shape (nn.Param): {grid_w.shape}')\n",
    "    # print(f'grid_h.shape (nn.Param): {grid_h.shape}')\n",
    "\n",
    "\n",
    "    # OLD \n",
    "    # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "    # NEW , TESTING\n",
    "    offset_h, offset_w = torch.split(offset, 1)\n",
    "    \n",
    "    # print(f'offset_h.shape (split): {offset_h.shape}')\n",
    "    # print(f'offset_w.shape (split): {offset_w.shape}')\n",
    "    \n",
    "    offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "    offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "    \n",
    "    # print(f'offset_h.shape (contiguous): {offset_h.shape}')\n",
    "    # print(f'offset_w.shape (contiguous): {offset_w.shape}')\n",
    "    \n",
    "    offset_w = grid_w + offset_w\n",
    "    offset_h = grid_h + offset_h\n",
    "    \n",
    "    # print(f'offset_w (grid_w + offset_w): {offset_w.shape}')\n",
    "    # print(f'offset_h (grid_h + offset_h): {offset_h.shape}')\n",
    "    \n",
    "    offsets = torch.stack((offset_h, offset_w), 3)\n",
    "    \n",
    "    # print(f'offsets (stack): {offsets.shape}')\n",
    "\n",
    "    # print('leaving generate_2dmotion_field')\n",
    "    return offsets\n",
    "\n",
    "def categorical_dice(prediction, truth, k, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "        Compute the dice overlap between the predicted labels and truth\n",
    "        Not a loss\n",
    "    \"\"\"\n",
    "    # Dice overlap metric for label value k\n",
    "    A = (prediction == k)\n",
    "    B = (truth == k)\n",
    "    return 2 * np.sum(A * B) / (np.sum(A) + np.sum(B) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b5259-1bb3-4408-85f2-ac738e3af66c",
   "metadata": {},
   "source": [
    "## 2. Pass first video's data thru the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314223ce-2e30-41f0-9601-b56b370bdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment using all models\n",
    "all_segmentation_outputs = []\n",
    "all_motion_outputs = []\n",
    "\n",
    "# for each model, segment the clips\n",
    "for name, model in loaded_in_models:\n",
    "    \n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "    \n",
    "    # save \n",
    "    all_segmentation_outputs.append(segmentation_outputs)\n",
    "    all_motion_outputs.append(motion_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff9652-8136-4d78-84fb-460c39c54a7c",
   "metadata": {},
   "source": [
    "### 3. The fun part, we need to make sure our objects have the right shapes in order to pass them thru the motion manipulation functions that do the actual warping of the seg out images via the motion tracking information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ea716d-4176-4d2d-a797-36a071fe21fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 2, 32, 112, 112), (16, 4, 32, 112, 112))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_trackings = all_motion_outputs[0]\n",
    "segmentations = all_segmentation_outputs[0]\n",
    "segmentations.shape, motion_trackings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919629cc-66e1-48c5-b0e7-45e1ae12abac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 32, 112, 112), (4, 32, 112, 112))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_clip_segmentations = segmentations[-1]\n",
    "last_clip_motions = motion_trackings[-1]\n",
    "last_clip_segmentations.shape, last_clip_motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fab4046-0e1f-45f2-94e6-675775a1beb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 112, 112)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember, we will want to continuously warp the previous frame that has been warped forward in time\n",
    "# only the first frame that we start with will be the actual seg out frame\n",
    "\n",
    "flow_source = None\n",
    "\n",
    "# warping FORWARD from ED -> ES\n",
    "for frame_index in range(delta_ed_es):\n",
    "    # grab forward motion\n",
    "    forward_motion = last_clip_motions[:2, frame_index,...]\n",
    "    \n",
    "    # grab the ED seg out frame to warp\n",
    "    if frame_index == 0:\n",
    "        flow_source = np.array([last_clip_segmentations[:, frame_index, ...]])\n",
    "        flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "    else:\n",
    "        pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "    \n",
    "    # convert to tensors and move to gpu with float dtype\n",
    "    forward_motion = torch.from_numpy(forward_motion).to(device).float()\n",
    "    # generate motion field for forward motion\n",
    "    motion_field = generate_2dmotion_field_PLAY(flow_source, forward_motion)\n",
    "    # create frame i+1 relative to curr frame i \n",
    "    next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "    # use i+1 frame as next loop iter's i frame\n",
    "    flow_source = next_label\n",
    "    \n",
    "#     # looking at it\n",
    "#     next_frame = next_label.cpu().detach().numpy()\n",
    "    \n",
    "#     next_frame = np.argmax(next_frame, 1)\n",
    "\n",
    "#     plt.imshow(next_frame[0], cmap='gray')\n",
    "#     plt.colorbar()\n",
    "\n",
    "es_created_from_warping_ed = flow_source.cpu().detach().numpy()\n",
    "es_created_from_warping_ed.shape\n",
    "\n",
    "# warping BACKWARD from ED <- ES\n",
    "for frame_index in range(delta_ed_es, -1, -1):\n",
    "    # grab backward motion\n",
    "    backward_motion = last_clip_motions[2:, frame_index,...]\n",
    "\n",
    "    # grab the ES seg out frame to start\n",
    "    if frame_index == delta_ed_es:\n",
    "        flow_source = np.array([last_clip_segmentations[:, frame_index, ...]])\n",
    "        flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "    else:\n",
    "        pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "    \n",
    "    # convert to tensors and move to gpu with float dtype\n",
    "    backward_motion = torch.from_numpy(backward_motion).to(device).float()\n",
    "    # generate motion field for backward motion\n",
    "    motion_field = generate_2dmotion_field_PLAY(flow_source, backward_motion)\n",
    "    # create frame i-1 relative to curr frame i \n",
    "    next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "    # use i-1 frame as next loop iter's i frame\n",
    "    flow_source = next_label\n",
    "\n",
    "ed_created_from_warping_es = flow_source.cpu().detach().numpy()\n",
    "ed_created_from_warping_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c28459c-c328-433b-98a6-5ea75b757088",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_created_from_warping_es_display_version = np.argmax(ed_created_from_warping_es, 1)[0]\n",
    "\n",
    "es_created_from_warping_ed_display_version = np.argmax(es_created_from_warping_ed, 1)[0]\n",
    "\n",
    "# plt.imshow(es_created_from_ed_warping_display_version, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57b744-3502-4c0e-8813-1b936a405c79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare the 3 ES that we now have. \n",
    "#### 1. ES Ground Truth <br> 2. ES Seg Out <br> 3. ES warped from ED Seg Out\n",
    "\n",
    "#### Do the same for the different ED's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bacb1935-68f2-4bf3-a381-434919c7b12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice (GT, Seg): 0.9272270958141337\n",
      "Dice (GT, Warp): 0.9288860860359592\n",
      "Dice (Seg, Warp): 0.9899467712007879\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "es_seg_out = np.argmax(last_clip_segmentations, 0)[delta_ed_es]\n",
    "\n",
    "ax1.set_title('ES Ground Truth')\n",
    "ax1.imshow(es_label, cmap='gray')\n",
    "\n",
    "ax2.set_title('ES Seg Out')\n",
    "ax2.imshow(es_seg_out, cmap='gray')\n",
    "\n",
    "ax3.set_title('ES from warping ED')\n",
    "ax3.imshow(es_created_from_warping_ed_display_version, cmap='gray')\n",
    "\n",
    "print(f'Dice (GT, Seg): {categorical_dice(es_seg_out, es_label, k=1, epsilon=1e-5)}')\n",
    "print(f'Dice (GT, Warp): {categorical_dice(es_created_from_warping_ed_display_version, es_label, k=1, epsilon=1e-5)}')\n",
    "print(f'Dice (Seg, Warp): {categorical_dice(es_created_from_warping_ed_display_version, es_seg_out, k=1, epsilon=1e-5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97817eb8-425a-4df0-b26f-c6c24335d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice (GT, Seg): 0.9661082938601157\n",
      "Dice (GT, Warp): 0.9603998425205695\n",
      "Dice (Seg, Warp): 0.9855732688474819\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "ed_seg_out = np.argmax(last_clip_segmentations, 0)[0]\n",
    "\n",
    "ax1.set_title('ED Ground Truth')\n",
    "ax1.imshow(ed_label, cmap='gray')\n",
    "\n",
    "ax2.set_title('ED Seg Out')\n",
    "ax2.imshow(ed_seg_out, cmap='gray')\n",
    "\n",
    "ax3.set_title('ED from warping ED')\n",
    "ax3.imshow(ed_created_from_warping_es_display_version, cmap='gray')\n",
    "\n",
    "print(f'Dice (GT, Seg): {categorical_dice(ed_seg_out, ed_label, k=1, epsilon=1e-5)}')\n",
    "print(f'Dice (GT, Warp): {categorical_dice(ed_created_from_warping_es_display_version, ed_label, k=1, epsilon=1e-5)}')\n",
    "print(f'Dice (Seg, Warp): {categorical_dice(ed_created_from_warping_es_display_version, ed_seg_out, k=1, epsilon=1e-5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a879c32-4f1b-475a-b4b7-5eef9172e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
