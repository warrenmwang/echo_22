{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1e6d7a-0aa5-4f39-8acf-52bd536927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Original_Pretrained_R2plus1DMotionSegNet.pth\"\n",
    "\n",
    "# model_name = \"dropout_v2_0_10_R2plus1DMotionSegNet.pth\"\n",
    "# model_name = \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd8f76d-8f31-48fb-af7d-347f1f527edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\")\n",
    "print(os.getcwd())\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads, forgot to define in forward pass function, but still saw diff, weird.)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_50_R2plus1D_18_MotionNet import dropout_v2_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_75_R2plus1D_18_MotionNet import dropout_v2_0_75_R2plus1D_18_MotionNet\n",
    "# v3 dropout (one dropout layer defined in forward pass func, this should've been the correct way to do it.)\n",
    "from src.model.dropout_v3_0_00_R2plus1D_18_MotionNet import dropout_v3_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_10_R2plus1D_18_MotionNet import dropout_v3_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_25_R2plus1D_18_MotionNet import dropout_v3_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_50_R2plus1D_18_MotionNet import dropout_v3_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_75_R2plus1D_18_MotionNet import dropout_v3_0_75_R2plus1D_18_MotionNet\n",
    "# v4 dropout (4 dropout layers in different places in the forward func, I'm going to guess more \"generalizable\")\n",
    "from src.model.dropout_v4_0_00_R2plus1D_18_MotionNet import dropout_v4_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_10_R2plus1D_18_MotionNet import dropout_v4_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_25_R2plus1D_18_MotionNet import dropout_v4_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_50_R2plus1D_18_MotionNet import dropout_v4_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_75_R2plus1D_18_MotionNet import dropout_v4_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# for finding lv seg borders\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "# random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8764bd-8ccc-45f5-bc7d-be9b82cebb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Pretrained_R2plus1DMotionSegNet.pth has 31575731 parameters.\n"
     ]
    }
   ],
   "source": [
    "model_save_path = f\"save_models/{model_name}\"\n",
    "    \n",
    "if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_10_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_10_R2plus1D_18_MotionNet()\n",
    "\n",
    "elif model_name == \"dropout_v3_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "\n",
    "model = torch.nn.DataParallel(model_template_obj)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a9544e-a57a-4658-95f1-084f36746594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    ''' return root mean square error difference between the two values passed in'''\n",
    "    return np.sqrt((x - y) ** 2)\n",
    "\n",
    "def strain_value(l_0, l_i):\n",
    "    '''\n",
    "    inputs: l_0, l_i -- original length and new length at some time point, respectively\n",
    "    output: e -- strain value (positive for elongation, negative for compressing/shortening) as a percentage (e.g. output 0.155 == 15.5 %)\n",
    "    \n",
    "    examples: \n",
    "        l_i = 10\n",
    "        l_0 = 5\n",
    "        e == (10 - 5) / 5 = 1, factor of lengthening relative to original value\n",
    "        \n",
    "        l_i = 5\n",
    "        l_0 = 5\n",
    "        e == (5 - 5) / 5 = 0, no strain\n",
    "    '''\n",
    "    return (l_i - l_0) / l_0\n",
    "\n",
    "def give_boundary(x):\n",
    "    '''\n",
    "    input: \n",
    "        x (112, 112) one-hot encoded lv mask/segmentation\n",
    "        unique values (0,1)\n",
    "        has to be numpy ndarray on cpu mem\n",
    "    output: \n",
    "        y (112, 112) black and white picture of boundary of lv\n",
    "        unique vals (0,1)\n",
    "    '''\n",
    "    foo = np.uint8(x * 255)\n",
    "    ret, thresh = cv.threshold(foo, 127, 255, 0)\n",
    "    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    blank = np.zeros((112,112))\n",
    "    cv.drawContours(blank, contours, -1, (255,255,255), 1)\n",
    "    \n",
    "    # return boundary image with unique values of (0,1)\n",
    "    blank = blank / 255\n",
    "    \n",
    "    return blank\n",
    "\n",
    "def boundaries_to_strain(before, after):\n",
    "    '''\n",
    "    input:\n",
    "        before (112, 112) boundary of lv\n",
    "        after (112, 112) boundary of lv\n",
    "        expect unique values of (0,1)\n",
    "        \n",
    "    output: \n",
    "        y - floating point number representing strain value, left as a decimal, NOT multiplied by 100\n",
    "    '''\n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(before)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        before = before / 255\n",
    "    check_unique_vals = np.unique(after)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        after = after / 255\n",
    "        \n",
    "    # cut basal plane out\n",
    "    before = cut_basal_plane_out(before)\n",
    "    after = cut_basal_plane_out(after)\n",
    "    \n",
    "    # count lv pixels\n",
    "    l_0 = np.count_nonzero(before == 1)\n",
    "    l_i = np.count_nonzero(after == 1)\n",
    "    \n",
    "    return strain_value(l_0, l_i)\n",
    "    \n",
    "def images_to_strain(ed_frame, es_frame):\n",
    "    '''\n",
    "    input:\n",
    "        ed_frame (112, 112)\n",
    "        es_frame (112, 112)\n",
    "        \n",
    "        expect vals to be one-hot encoded (1's for lv, 0's for not lv)\n",
    "    output:\n",
    "        x - floating point number\n",
    "        strain value (some decimal, should be negative)\n",
    "    '''\n",
    "    # get boundaries, then cut basal plane out\n",
    "    ed_bound = cut_basal_plane_out(give_boundary(ed_frame))\n",
    "    es_bound = cut_basal_plane_out(give_boundary(es_frame))\n",
    "    # compute strain and return\n",
    "    return boundaries_to_strain(ed_bound, es_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc4a28e-52ed-43a8-9dea-60e1e4cde9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blot_out_given_rect(rectangle_corners, I, replace_val = 0):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        I - shape (112, 112) \n",
    "        \n",
    "    output:\n",
    "        I_copy -- deepcopy of original image I with the defined rectangle section given zeroed out in the image\n",
    "    '''\n",
    "    \n",
    "     # i = row, j = col, we're thinking in ij/row col instead of xy\n",
    "    if rectangle_corners[0][0] < rectangle_corners[1][0]:\n",
    "        i_start = int(rectangle_corners[0][0])\n",
    "        i_end = int(rectangle_corners[1][0])\n",
    "    else:\n",
    "        i_end = int(rectangle_corners[0][0])\n",
    "        i_start = int(rectangle_corners[1][0])\n",
    "        \n",
    "    if rectangle_corners[1][1] < rectangle_corners[0][1]:\n",
    "        j_start = int(rectangle_corners[1][1])\n",
    "        j_end = int(rectangle_corners[0][1])\n",
    "    else:\n",
    "        j_end = int(rectangle_corners[1][1])\n",
    "        j_start = int(rectangle_corners[0][1])\n",
    "        \n",
    "    # make copy and alter then return\n",
    "    import copy \n",
    "    I_copy = copy.deepcopy(I)\n",
    "    \n",
    "    # zero out everything at and below the highest of the two index points (i_start)\n",
    "    I_copy[i_start : I.shape[0], 0 : I.shape[1]] = replace_val\n",
    "    \n",
    "    return I_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b04f675-7071-4758-a30c-fca2644d6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_with_rect(rectangle_corners, I, replace_val=127):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2, 2)\n",
    "    output:\n",
    "        empty image with the rectangle as defined blotted out with values of 127 -- shape (112, 112)\n",
    "    '''\n",
    "    I_rect = np.zeros(I.shape)\n",
    "    I_rect = blot_out_given_rect(rectangle_corners, I_rect, replace_val=replace_val)\n",
    "    return I_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3635e471-a5a0-476a-bc9b-05f06d1d5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(rectangle_corners, I=None, show_applied=False):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        Well, technically rectangle corners could even have shape of (4, 2)\n",
    "    \n",
    "    plot a rectangle based on the coords given (in format of the skimage functions that get the corner pixels coords)\n",
    "    also if an image is given, plot the rectangle on top of it\n",
    "    '''\n",
    "   \n",
    "    # plot the rectangle\n",
    "    I_rect = blank_with_rect(rectangle_corners, I)\n",
    "    \n",
    "    # plot the original image if given\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if I is not None:\n",
    "        # assuming I has unique values (0,1)\n",
    "        plt.imshow(I * 255, cmap='gray', zorder=1)\n",
    "        plt.imshow(I_rect, cmap='gray', zorder=2, alpha=0.5)\n",
    "        # plt.colorbar()\n",
    "    else:\n",
    "        plt.imshow(I_rect, cmap='gray')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # show another figure of using the shown rectangle to zero out original image if asked\n",
    "    if show_applied:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(blot_out_given_rect(rectangle_corners, I), cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfab0541-984d-4a60-a0f5-1abbad94e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_corner_pixels_detected(I, show_plot=True, return_vars=False):\n",
    "    '''\n",
    "    input: I (112, 112), unique vals of 0,1\n",
    "    output (if requested):\n",
    "        coords - numpy ndarray\n",
    "        coords_subpix - nump ndarray\n",
    "            shapes of 2 objects above may vary...\n",
    "    \n",
    "    Prints the matplotlib figure with the corner pixels detected attached to the image\n",
    "    \n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)\n",
    "    coords_subpix = corner_subpix(I, coords, window_size=13)\n",
    "\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        ax.imshow(I, cmap=plt.cm.gray)\n",
    "        ax.plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "                linestyle='None', markersize=6)\n",
    "        ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)\n",
    "        plt.show()\n",
    "    \n",
    "    # return quantities if requested\n",
    "    if return_vars:\n",
    "        return coords, coords_subpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0161ba8-d1ce-47d4-9b87-d42de05fec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangle_corners(I):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (112, 112)\n",
    "    output:\n",
    "        rectange_corners - shape (2,2)\n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # get corner pixels to form rectangle from\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)    \n",
    "    \n",
    "    # descending order sort all possible values along axis=0 for sorting by i / row index\n",
    "    coords[::-1].sort(axis=0)\n",
    "    \n",
    "    rectangle_corners = coords[0:2]\n",
    "\n",
    "    return rectangle_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63dc950d-06e9-4138-af4f-698fb23a924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_basal_plane_out(I):\n",
    "    '''\n",
    "    input: I (112, 112) expected to have unique values of (0,1)...1 for lv, 0 for not lv\n",
    "    output: I_new (112, 112) with unique values of (0,1)... 1 for lv, 0 for not lv...this has the basal plane removed\n",
    "    \n",
    "    https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py\n",
    "    \n",
    "    When choosing which 2 points to create a rough rectangle to zero out the bottom mitral valve / basal plane boundary\n",
    "    we will go with this ordering of points to use:\n",
    "    1. Both points from corner_subpix\n",
    "    2. One point from corner_subpix, one point from corner_peaks\n",
    "    3. Both points from corner_peaks\n",
    "    \n",
    "    If we can't get two points at the area of the basal plane from any of the three situations described above, we are out of luck. \n",
    "    We will have to do something else, but this won't work then. \n",
    "    '''  \n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "    \n",
    "    rectangle_corners = get_rectangle_corners(I)\n",
    "    return blot_out_given_rect(rectangle_corners, I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f302e5f9-ba6c-418a-b452-c504e593ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# taken from stough's dip vis_utils.py\n",
    "def vis_pair(I, J, figsize = (8,3), shared = True, \n",
    "             first_title = 'Original', second_title = 'New',\n",
    "             show_ticks = True, **kwargs):\n",
    "    '''\n",
    "    vis_pair(I, J, figsize = (8,3), shared = True, first_title = 'Original', second_title = 'New'):\n",
    "    produce a plot of images I and J together. By default takes care of sharing axes to provide\n",
    "    a little 1x2 plot without all the coding.\n",
    "    '''\n",
    "    f, ax = plt.subplots(1,2, figsize=figsize, sharex = shared, sharey = shared)\n",
    "    ax[0].imshow(I, **kwargs)\n",
    "    ax[0].set_title(first_title)\n",
    "    ax[1].imshow(J, **kwargs)\n",
    "    ax[1].set_title(second_title)\n",
    "    \n",
    "    if not show_ticks:\n",
    "        [a.axes.get_xaxis().set_visible(False) for a in ax];\n",
    "        [a.axes.get_yaxis().set_visible(False) for a in ax];\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe0fa0-b44b-4f0f-86a4-b092c967a349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "885db8d7-ddcc-4440-bee8-6aa1481e45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_and_warp_data(model, test_dataset, test_pat_index):\n",
    "    '''\n",
    "    input: \n",
    "         model, dataset, and video index in dataset\n",
    "    output:\n",
    "        segmentation and motion tracking info on a specified video\n",
    "        delta_ed_es = index difference between the ed and es clip\n",
    "        returned seg/mot information is for a 32 frame clip where ed is at index 0\n",
    "        clip_index - index of clip \n",
    "    '''\n",
    "    ########################### Helper functions ###########################\n",
    "    # goes thru a video and annotates where we can start clips given video length, clip length, etc.\n",
    "    def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "        assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "        possible_shift = clip_length - (es_index - ed_index)\n",
    "        allowed_right = video_length - es_index\n",
    "        if allowed_right < possible_shift:\n",
    "            return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "        if possible_shift < 0:\n",
    "            return np.array([ed_index])\n",
    "        elif ed_index < possible_shift:\n",
    "            return np.arange(ed_index + 1)\n",
    "        else:\n",
    "            return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "        \n",
    "    def generate_2dmotion_field_PLAY(x, offset):\n",
    "        # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "        # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "        x_shape = x.shape\n",
    "        # print(f'x_shape: {x_shape}')\n",
    "\n",
    "        grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "\n",
    "        # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "        # floating point precision\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_h = grid_h.cuda().float()\n",
    "\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "\n",
    "        # OLD \n",
    "        # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "        # NEW , TESTING\n",
    "        offset_h, offset_w = torch.split(offset, 1)\n",
    "\n",
    "        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "\n",
    "        offset_w = grid_w + offset_w\n",
    "        offset_h = grid_h + offset_h\n",
    "\n",
    "        offsets = torch.stack((offset_h, offset_w), 3)\n",
    "        return offsets\n",
    "\n",
    "    def categorical_dice(prediction, truth, k, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "            Compute the dice overlap between the predicted labels and truth\n",
    "            Not a loss\n",
    "        \"\"\"\n",
    "        # Dice overlap metric for label value k\n",
    "        A = (prediction == k)\n",
    "        B = (truth == k)\n",
    "        return 2 * np.sum(A * B) / (np.sum(A) + np.sum(B) + epsilon)\n",
    "    \n",
    "    ########################################################################\n",
    "    # initial grabbing of the data that we'll use\n",
    "    video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "    \n",
    "    # get all possible start indices for 32 frame clip with ed/es in right order\n",
    "    possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "    \n",
    "    # for now, let's use the last clip from our set of all possible clips to use\n",
    "    clip_index = len(possible_starts) - 1\n",
    "    # print(f'clip_index: {clip_index}')\n",
    "\n",
    "    # get the diff in frame len from ed to es\n",
    "    delta_ed_es = es_index - ed_index\n",
    "    \n",
    "    # use model to segment frames\n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "\n",
    "    # grab whatever clip we want\n",
    "    curr_clip_segmentations = segmentation_outputs[clip_index]\n",
    "    curr_clip_motions = motion_outputs[clip_index]\n",
    "    \n",
    "    return curr_clip_segmentations, curr_clip_motions, delta_ed_es, clip_index, ed_label, es_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0115a7a-8788-445c-b77d-703c3f95befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(I):\n",
    "    '''\n",
    "    input: I - shape (2, 112, 112)\n",
    "    convert a raw segmentation output into a one_hot_encoded image\n",
    "    '''\n",
    "    import copy\n",
    "    I_copy = copy.deepcopy(I)\n",
    "    return np.argmax(I_copy, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f858d0-dc29-4ad8-b8f8-b5043e3dc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dice(prediction, truth):\n",
    "    '''\n",
    "    assume both images are one_hot_encoded\n",
    "    '''\n",
    "    k = 1\n",
    "    return categorical_dice(prediction, truth, k, epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eadca1c-1ee6-4410-8ce5-0d4bf3efb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_forward(I, motions, delta_ed_es, clip_index):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (1, 2, 112, 112), not one-hot encoded, must be the raw model segmentation output\n",
    "        motions - shape ()\n",
    "        delta_ed_es - integer defining how many forward iterations to take (max 31)\n",
    "                        we are only interested in warping to/from ED/ES\n",
    "    output:\n",
    "        I_1 - shape (1, 2, 112, 112), not one-hot encoded, raw ES image, if want one-hot encoded need to apply np.argmax\n",
    "        \n",
    "    for now, try to do things all on the cpu\n",
    "    '''\n",
    "    def generate_2dmotion_field_custom(x, offset):\n",
    "        # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "        # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "        x_shape = x.shape\n",
    "        # print(f'x_shape: {x_shape}')\n",
    "\n",
    "        grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "\n",
    "        # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "        # floating point precision\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_h = grid_h.cuda().float()\n",
    "\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "\n",
    "        # OLD \n",
    "        # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "        # NEW , TESTING\n",
    "        offset_h, offset_w = torch.split(offset, 1)\n",
    "\n",
    "        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "\n",
    "        offset_w = grid_w + offset_w\n",
    "        offset_h = grid_h + offset_h\n",
    "\n",
    "        offsets = torch.stack((offset_h, offset_w), 3)\n",
    "        return offsets\n",
    "    \n",
    "    # convert numpy ndarrays into tensor objects moved onto device\n",
    "    flow_source = torch.from_numpy(I).to(device).float()\n",
    "    motions = torch.from_numpy(motions).to(device).float()\n",
    "\n",
    "    # warping FORWARD from ED -> ES\n",
    "    # python range is [x, y), inclusive start and exclusive end\n",
    "    for frame_index in range(31 - delta_ed_es - clip_index, 31 - clip_index + 1, 1):\n",
    "        # grab forward motion\n",
    "        forward_motion = motions[:2, frame_index,...]\n",
    "\n",
    "        # # grab the ED seg out frame to warp\n",
    "        # if frame_index == 0:\n",
    "        #     # flow_source = np.array([curr_clip_segmentations[:, frame_index, ...]])\n",
    "        #     flow_source = I\n",
    "        #     print(f'type(flow_source): {type(flow_source)}')\n",
    "        #     # flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "        # else:\n",
    "        #     pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "\n",
    "        # convert to tensors and move to gpu with float dtype\n",
    "        # forward_motion = torch.from_numpy(forward_motion).to(device).float()\n",
    "        # generate motion field for forward motion\n",
    "        motion_field = generate_2dmotion_field_custom(flow_source, forward_motion)\n",
    "        # create frame i+1 relative to curr frame i \n",
    "        next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "        # use i+1 frame as next loop iter's i frame\n",
    "        flow_source = next_label\n",
    "\n",
    "    flow_source = flow_source.cpu().detach().numpy()\n",
    "    \n",
    "    return flow_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02312e94-d8a7-4ab0-a11f-4a46c795b20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292d2fd6-f49e-42f6-bc14-bc2380c95f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88fa607-0f3c-465a-809b-f0f84c40b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_clip_segmentations, curr_clip_motions, delta_ed_es, clip_index, ed_label, es_label = get_seg_and_warp_data(model = model,\n",
    "                                                                                            test_dataset = test_dataset,\n",
    "                                                                                            test_pat_index = test_pat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec12667-a82a-482a-8d86-c9448c43e1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205c7443-0e12-4fb8-a389-cb988da761f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 112, 112)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_segmentations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbdff2c3-77ae-4249-9fca-4232045ec66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_raw_seg_out = curr_clip_segmentations[:,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84858ea3-af77-4a22-a871-03ef8c10cff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 112, 112)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_raw_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4188cc0-9992-4a31-8b5f-74940da922fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d679b551-c592-4d32-b31a-ca15460e2b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311f97a145304141981ff70424ab78d0",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=800.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_pair(one_hot(ed_raw_seg_out), ed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a0f2fc0-45c0-42e9-acd9-efa3408b2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_, height, width = ed_raw_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6128e0f-46ca-4722-ab2b-58d80fc4332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.86601448, -9.85149574, -9.79540539, ..., 10.27981186,\n",
       "       10.39611626, 10.40559196])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ed_raw_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3bff719-be31-413a-9f4a-437166a0a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_one_hot_seg_out = one_hot(ed_raw_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83ba5376-6cf2-484b-a298-e15dd5e8fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_one_hot_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7196ba3b-5602-4ede-9db2-d9eb9d3d84fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ed_one_hot_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0e79dc7-6b8a-4437-b0a6-3b65765c7ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9661)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(ed_one_hot_seg_out, ed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "353dc3cd-b01a-4792-81e1-45f2c0dfd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ed_raw_seg_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6ffc5d8-134d-450c-8af7-3c1f04cc08f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 112, 112)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fe3667a-3408-486a-bb48-886602204584",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_warped = warp_forward(foo, curr_clip_motions, delta_ed_es, clip_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eeb7416-4db9-4589-a54b-5fc670ade3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 112, 112)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_warped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e11b12e-6901-432b-9621-cd3fcf9eba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_warped_vis = one_hot(I_warped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb0b78f-b017-4c92-8481-608dc0032644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_warped_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc3259b1-d8de-4d7c-924f-76aaf6e416c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9277)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(I_warped_vis, es_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f808f6-1e4f-4a57-b964-b8d33852649a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curr_clip_motions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-190254137d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr_clip_motions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_clip_motions' is not defined"
     ]
    }
   ],
   "source": [
    "curr_clip_motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b231cb-071e-4334-bc2f-3e7254e04bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
