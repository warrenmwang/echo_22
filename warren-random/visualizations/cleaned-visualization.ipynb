{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e8dce0-4a62-4a81-a6fe-13e7bfc1da0e",
   "metadata": {},
   "source": [
    "## Cleaned up version of creating Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e0350-2ac1-48c1-96e8-bd344c178b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Original-Pretrained-R2plus1DMotionSegNet_model.pth\", \"dropout-0_10-R2plus1DMotionSegNet_model.pth\", \"dropout-0_25-R2plus1DMotionSegNet_model.pth\", \"dropout-0_50-R2plus1DMotionSegNet_model.pth\", \"dropout-0_75-R2plus1DMotionSegNet_model.pth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86196c46-497b-4600-ba4f-ee23c424bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change os curr dir to be one below such that src can be imported :/\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0f072-ecb1-46fa-9e1d-8f200d776af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet # original model\n",
    "# new models (small alterations)\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet \n",
    "\n",
    "\n",
    "from src.echonet_dataset import EchoNetDynamicDataset\n",
    "from src.clasfv_losses import deformation_motion_loss, motion_seg_loss, DiceLoss, categorical_dice\n",
    "from src.train_test import train, test\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# for slider visualizations\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import correlate\n",
    "from skimage.filters import *\n",
    "\n",
    "from ipywidgets import VBox, IntSlider, AppLayout\n",
    "# initialize to use dark background ...\n",
    "plt.style.use('dark_background')\n",
    "######\n",
    "# for creating gif animation and viewing them\n",
    "from matplotlib import animation\n",
    "from IPython.display import Image\n",
    "\n",
    "######\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3943b-2661-4eb4-b4e4-dbd5832c0922",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in models\n",
    "\n",
    "We have the luxury of trying inference on multiple models, since I'm trying to create slightly different models.\n",
    "\n",
    "Original pre-trained model by Yida will be `model_name_1`.\n",
    "The one that I will add dropout and k-fold cross validation to will be `model_name_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371d1a9-db2f-4d38-84a6-41c0e89a4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold tuples of (name, model object)\n",
    "loaded_in_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045bbf9-3939-4a8a-bf8f-254ca6bd0b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    model_save_path = f\"save_models/{model_name}\"\n",
    "    \n",
    "    # original model\n",
    "    if model_name == \"Original-Pretrained-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(R2plus1D_18_MotionNet())\n",
    "         model = torch.nn.DataParallel(R2plus1D_18_MotionNet())\n",
    "        \n",
    "    # altered models\n",
    "    if model_name == \"dropout-0_75-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_75_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_75_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_50-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_50_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_50_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_25-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_25_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_25_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_10-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_10_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_10_R2plus1D_18_MotionNet())\n",
    "    \n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee956c3-6700-4976-b618-24b6b0a49a60",
   "metadata": {},
   "source": [
    "## Load in Testing Dataset to do inference on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ea5dc-b752-4af1-9b9a-12b85e2ff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93f01d-17d4-48e0-b09a-973de4c36171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556f950-f275-4165-bbb7-ea16624546f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Grab a video to look at, can be random or manually choose one of the 1276 from test dataset\n",
    "### For sake of comparison, let's look at first sample from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d14b3e-26ee-4e94-93a4-2fc0a4848678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pat_index = np.random.randint(len(test_dataset))\n",
    "test_pat_index = 0 \n",
    "\n",
    "video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045257aa-074b-43a4-989d-5879785f3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "# goes thru a video and annotates where we can start clips given video length, clip length, etc.\n",
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293bde-0bf5-4585-90c3-a5bed153407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "print(len(possible_starts))\n",
    "print(possible_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4256fc-a229-4835-9417-269e6a783f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697bc2f-66d2-45b4-bdee-739d20aaaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7572f-1222-4cff-968a-68ddb9e32f5e",
   "metadata": {},
   "source": [
    "### Segment All 32-Frame Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b0ba9-a0ae-4205-bf2c-be539b2b676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment using all models\n",
    "all_segmentation_outputs = []\n",
    "all_motion_outputs = []\n",
    "\n",
    "# for each model, segment the clips\n",
    "for name, model in loaded_in_models:\n",
    "    \n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "    \n",
    "    # save \n",
    "    all_segmentation_outputs.append(segmentation_outputs)\n",
    "    all_motion_outputs.append(motion_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5bcd0-34ea-4411-883e-debcd91f37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_segmentation_outputs), len(all_motion_outputs))\n",
    "print(len(all_motion_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04eb6a-36f0-45ae-b25b-7cc905723222",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_motion_outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a669d-6d5d-4336-af83-c96fd30b7396",
   "metadata": {},
   "source": [
    "## Define the function to create our Motion Output on ColorMap GIF Animations\n",
    "\n",
    "How to save animation as gif: http://louistiao.me/posts/notebooks/save-matplotlib-animations-as-gifs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec87730-d224-4373-8f09-285152075a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_32_frame_motion_colormap_gif(model_name, motion_output_obj, clip_index, which_direction, out_file_comment=\"\", cmap=\"viridis\"):\n",
    "    # plot all 4, forward x,y and backward x, y\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,5));\n",
    "    \n",
    "    # put a big title with model's file name\n",
    "    plt.suptitle(f'{model_name} | Forwards forward in time | Backwards backward in time');\n",
    "    \n",
    "    start_frame = 0\n",
    "    \n",
    "    # find the max and min value needed for colormapping for all 4 motion outputs across ALL \n",
    "    # 32 frames\n",
    "    all_mins = []\n",
    "    all_maxes = []\n",
    "    for frame_ind in range(32):\n",
    "        for direction_ind in range(4):\n",
    "            all_mins.append(motion_output_obj[clip_index][which_direction[direction_ind]][frame_ind].min())\n",
    "            all_maxes.append(motion_output_obj[clip_index][which_direction[direction_ind]][frame_ind].max())\n",
    "    \n",
    "    color_min = min(all_mins)\n",
    "    color_max = max(all_maxes)\n",
    "    \n",
    "    # for backward motions, we're going to go backwards as it that makes more sense. we will sync up the backward x,y \n",
    "    # they will go from frame 31 -> 0 while forward goes from 0 -> 31\n",
    "    \n",
    "    ax1.set_title(\"Forward X\");\n",
    "    ax1.set_xlabel('Width Pixel Index') # only display axes labels on first fig for clarity\n",
    "    ax1.set_ylabel('Height Pixel Index')\n",
    "    fx = motion_output_obj[clip_index][which_direction[0]][start_frame]\n",
    "    ax1_img = ax1.imshow(fx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "\n",
    "    ax2.set_title(\"Forward Y\")\n",
    "    fy = motion_output_obj[clip_index][which_direction[1]][start_frame]\n",
    "    ax2_img = ax2.imshow(fy, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "\n",
    "    ax3.set_title(\"Backward X\")\n",
    "    bx = motion_output_obj[clip_index][which_direction[2]][31 - start_frame]\n",
    "    ax3_img = ax3.imshow(bx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "\n",
    "    ax4.set_title(\"Backward Y\")\n",
    "    by = motion_output_obj[clip_index][which_direction[3]][31 - start_frame]\n",
    "    ax4_img = ax4.imshow(by, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "    # only need to show colorbar on the last figure, since colorbar is now the same across all figures.\n",
    "    cbar_ax = fig.add_axes([0.92, 0.1, 0.01, 0.75])\n",
    "    fig.colorbar(ax4_img, cax=cbar_ax)\n",
    "    \n",
    "    # funct to update imshow with new frame of motion output\n",
    "    def animate(frame):\n",
    "        fx = motion_output_obj[clip_index][which_direction[0]][frame]\n",
    "        ax1_img = ax1.imshow(fx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax1_img, ax=ax1)\n",
    "        fy = motion_output_obj[clip_index][which_direction[1]][frame]\n",
    "        ax2_img = ax2.imshow(fy, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax2_img, ax=ax2)\n",
    "\n",
    "        bx = motion_output_obj[clip_index][which_direction[2]][31 - frame]\n",
    "        ax3_img = ax3.imshow(bx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax3_img, ax=ax3)\n",
    "        by = motion_output_obj[clip_index][which_direction[3]][31 - frame]\n",
    "        ax4_img = ax4.imshow(by, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax4_img, ax=ax4)\n",
    "\n",
    "        return [ax1, ax2, ax3, ax4]\n",
    "    anim = animation.FuncAnimation(fig, animate, np.arange(0, 32), interval=500, blit=True); # interval is milliseconds between each redraw, adjusts animation speed\n",
    "    anim.save(f'./warren-random/visualization-outputs/{model_name}_motion_colormap_{out_file_comment}.gif', writer='imagemagick', fps=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245a559-c829-4703-87f6-86cbe4f4b838",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make GIFs of motion outputs on a colormap from all the models that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e863bc-7ad8-4eb1-bad9-d1fb83738e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "out_file_comment = \"colorbar-seismic\"\n",
    "color_mapping = \"viridis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29334-c0c2-4362-96ac-d8c9703b27d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(loaded_in_models)):\n",
    "    create_32_frame_motion_colormap_gif(model_name = loaded_in_models[i][0],\n",
    "                                    motion_output_obj = all_motion_outputs[i],\n",
    "                                    clip_index = clip_index,\n",
    "                                    which_direction = which_direction,\n",
    "                                    out_file_comment = out_file_comment,\n",
    "                                    cmap = color_mapping);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32681c7d-cdf1-4444-93cb-ba45cc5daba0",
   "metadata": {},
   "source": [
    "## Look at the Gifs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455f95a-56ec-452b-8ab1-e35cf664bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[0][0]}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3522d-3250-4538-8aca-3cdadd3bbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[1][0]}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cadd98-9f78-4489-83b0-9dc3612fa839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[2][0]}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48f80d-a4f0-48e2-be33-e50dff785b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[3][0]}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e1eef-980d-4e31-b79e-38d8a8fd769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 5\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[4][0]}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31340aba-1867-46ea-8641-78b082d5f250",
   "metadata": {},
   "source": [
    "## Create Vector Fields of our Motion Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a8ab3-d159-4be4-92d6-584fbff02daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_32_frame_motion_vector_field_gifs(model_name, motion_output_obj, clip_index, which_direction, out_file_comment=\"\"):\n",
    "    # 2 subplots, forward and backward motion\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10));\n",
    "    \n",
    "    # starting frame ind\n",
    "    start_frame = 0\n",
    "    \n",
    "    # get initial forward and backward motion\n",
    "    fx = motion_output_obj[clip_index][which_direction[0]][start_frame]\n",
    "    fy = motion_output_obj[clip_index][which_direction[1]][start_frame]\n",
    "    \n",
    "    bx = motion_output_obj[clip_index][which_direction[2]][31 - start_frame]\n",
    "    by = motion_output_obj[clip_index][which_direction[3]][31 - start_frame]\n",
    "    \n",
    "    # create values for the tails of the vectors (these don't need to change)\n",
    "    nrows, ncols = fx.shape\n",
    "    x_tmp = np.linspace(0, 112, ncols)  \n",
    "    y_tmp = np.linspace(0, 112, nrows)\n",
    "    x_tails, y_tails = np.meshgrid(x_tmp, y_tmp, indexing='xy')\n",
    "    \n",
    "    # put titles, axes on subplots\n",
    "    plt.suptitle(f\"Motion Vector Field (x,y) for {model_name} | Forwards forward in time | Backwards backward in time\")\n",
    "    \n",
    "    ax1.set_title('Forward Motion')\n",
    "    ax1.set_xlabel('$\\Delta x$')\n",
    "    ax1.set_ylabel('$\\Delta y$')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    ax2.set_title('Backward Motion')\n",
    "    ax2.set_xlabel('$\\Delta x$')\n",
    "    ax2.set_ylabel('$\\Delta y$')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # put initial magnitudes to the vectors with fixed tails\n",
    "    # forward\n",
    "    ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "    # backward\n",
    "    ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "    \n",
    "    # funct to update imshow with new frame of motion output\n",
    "    def animate(frame):\n",
    "        # update forward xy and backward xy\n",
    "        fx = motion_output_obj[clip_index][which_direction[0]][frame]\n",
    "        fy = motion_output_obj[clip_index][which_direction[1]][frame]\n",
    "\n",
    "        bx = motion_output_obj[clip_index][which_direction[2]][31 - frame]\n",
    "        by = motion_output_obj[clip_index][which_direction[3]][31 - frame]\n",
    "        \n",
    "        # clear forward and backward axes first (we'll need to reapply the labels and inverting the y axis)\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.set_title('Forward Motion')\n",
    "        ax1.set_xlabel('$\\Delta x$')\n",
    "        ax1.set_ylabel('$\\Delta y$')\n",
    "        ax1.invert_yaxis()\n",
    "\n",
    "        ax2.set_title('Backward Motion')\n",
    "        ax2.set_xlabel('$\\Delta x$')\n",
    "        ax2.set_ylabel('$\\Delta y$')\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        # update the magnitudes of the vectors with fixed tails\n",
    "        # forward\n",
    "        ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "        # backward\n",
    "        ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "        \n",
    "    anim = animation.FuncAnimation(fig, animate, np.arange(0, 32), interval=500, blit=True); # interval is milliseconds between each redraw, adjusts animation speed\n",
    "    anim.save(f'./warren-random/visualization-outputs/{model_name}_motion_vector_field_{out_file_comment}.gif', writer='imagemagick', fps=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff1c53-137c-42e1-8fe5-63da7efa9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "out_file_comment = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc23fdc-032a-4902-a7a0-02adbbfd533e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(loaded_in_models)):\n",
    "    create_32_frame_motion_vector_field_gifs(model_name = loaded_in_models[i][0], \n",
    "                                             motion_output_obj = all_motion_outputs[i], \n",
    "                                             clip_index = clip_index, \n",
    "                                             which_direction = which_direction, \n",
    "                                             out_file_comment= out_file_comment);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e38d8-4a6d-4876-8813-66e2dc1904c5",
   "metadata": {},
   "source": [
    "## View Vector Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb53cd-2eaa-449b-b452-a55110d0f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[0][0]}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabf7fe-afee-4d45-8355-b03959dae212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[1][0]}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9651f3e-dda8-4a3e-8100-5404cc2935c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[2][0]}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6935f-996a-41d5-862c-f53275fbfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4\n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[3][0]}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67da0e-b1cc-4db5-b657-5bf7524c0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 5 \n",
    "Image(f'./warren-random/visualization-outputs/{loaded_in_models[4][0]}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae2066-1f54-46f7-b13a-5ce831c08589",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try to zoom in on the Vector Field Visualizations to get a better understanding of whether the vectors are in the right direction\n",
    "## Also, use a slider instead, that way we can slowly compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d57b30-24c0-41e0-a173-0d7381703158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button\n",
    "\n",
    "# grab actual motion output information to try slider animation with.\n",
    "# index represents which model in order of the loaded_in_models\n",
    "ind = 0\n",
    "\n",
    "motion_output_obj = all_motion_outputs[ind] \n",
    "model_name = loaded_in_models[ind][0]\n",
    "\n",
    "clip_index = -1   # last 32 frame clip\n",
    "\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "\n",
    "###########\n",
    "\n",
    "# plot initial vector field at t = 0\n",
    "\n",
    "# 2 subplots, forward and backward motion\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10), sharex=True, sharey=True);\n",
    "\n",
    "\n",
    "# starting frame ind\n",
    "start_frame = 0\n",
    "\n",
    "### this one, since we're sliding, we go forward in time for both forwards and backwards vector fields\n",
    "\n",
    "# get initial forward and backward motion\n",
    "fx = motion_output_obj[clip_index][which_direction[0]][start_frame]\n",
    "fy = motion_output_obj[clip_index][which_direction[1]][start_frame]\n",
    "\n",
    "bx = motion_output_obj[clip_index][which_direction[2]][start_frame]\n",
    "by = motion_output_obj[clip_index][which_direction[3]][start_frame]\n",
    "\n",
    "# create values for the tails of the vectors (these don't need to change)\n",
    "nrows, ncols = fx.shape\n",
    "x_tmp = np.linspace(0, 112, ncols)  \n",
    "y_tmp = np.linspace(0, 112, nrows)\n",
    "x_tails, y_tails = np.meshgrid(x_tmp, y_tmp, indexing='xy')\n",
    "\n",
    "# put titles, axes on subplots\n",
    "plt.suptitle(f\"Motion Vector Field (x,y) for {model_name}\")\n",
    "ax1.set_title('Forward Motion')\n",
    "ax1.set_xlabel('$\\Delta x$')\n",
    "ax1.set_ylabel('$\\Delta y$')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.set_title('Backward Motion')\n",
    "ax2.set_xlabel('$\\Delta x$')\n",
    "ax2.set_ylabel('$\\Delta y$')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# put initial magnitudes to the vectors with fixed tails\n",
    "# forward\n",
    "blah = ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "# backward\n",
    "blah = ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "######################\n",
    "\n",
    "# update function\n",
    "def update(frame):\n",
    "    frame = int(frame)\n",
    "    # update forward xy and backward xy\n",
    "    fx = motion_output_obj[clip_index][which_direction[0]][frame]\n",
    "    fy = motion_output_obj[clip_index][which_direction[1]][frame]\n",
    "\n",
    "    bx = motion_output_obj[clip_index][which_direction[2]][frame]\n",
    "    by = motion_output_obj[clip_index][which_direction[3]][frame]\n",
    "\n",
    "    # clear forward and backward axes first (we'll need to reapply the labels and inverting the y axis)\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax1.set_title('Forward Motion')\n",
    "    ax1.set_xlabel('$\\Delta x$')\n",
    "    ax1.set_ylabel('$\\Delta y$')\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    ax2.set_title('Backward Motion')\n",
    "    ax2.set_xlabel('$\\Delta x$')\n",
    "    ax2.set_ylabel('$\\Delta y$')\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    # update the magnitudes of the vectors with fixed tails\n",
    "    # forward\n",
    "    blah = ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "    # backward\n",
    "    blah = ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "##############\n",
    "# connect slider to the update function\n",
    "\n",
    "# Make a horizontal slider to control the frame we are looking at.\n",
    "axframe = plt.axes([0.10, 0.1, 0.65, 0.03])\n",
    "\n",
    "frame_slider = Slider(\n",
    "    ax=axframe,\n",
    "    label='Frame',\n",
    "    valmin=0,\n",
    "    valmax=31,\n",
    "    valinit=0,\n",
    "    valstep=1.0,\n",
    "    valfmt=f\"%0.f\"\n",
    ")\n",
    "\n",
    "frame_slider.on_changed(update)\n",
    "\n",
    "# Create a `matplotlib.widgets.Button` to reset the sliders to initial values.\n",
    "resetax = plt.axes([0.8, 0.025, 0.1, 0.04])\n",
    "button = Button(resetax, 'Reset', hovercolor='0.975')\n",
    "\n",
    "def reset(event):\n",
    "    frame_slider.reset()\n",
    "    \n",
    "button.on_clicked(reset)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=0.25)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bc8de-5027-4cb3-a305-873b41780ef2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If our visualization doesn't tell us anything, we need to try to quantify how correct our motion outputs are. One of our losses that Yida uses to train the model should be telling us this information of how accurate our warps using the motion tracking information is relative to ground truth real frames. \n",
    "\n",
    "### How do we get the motion tracking information while it is training? We will have to step slowly thru the training cycle. we'll step thru a part of a single train cycle (epoch) on hopefully a single video from the train dataset , a couple of 32 frame clips from this single video to see what kind of motion tracking information we get and see how we can visualize the warping and comparisons that occur during the underlying training cycle (quantified by a loss)\n",
    "\n",
    "### In order to visualizer a warping frame by frame, we need to calculate motion output for a specific frame and then apply it to the previous frame and see what it looks like. Will creating visualizations help at all, this feels like a useless endeavor?\n",
    "\n",
    "1. Pass a single 32 frame clip from a test video to a loaded in model\n",
    "2. Get its motion tracking information\n",
    "3. Incrementally apply the warp using this motion tracking information and see if it matches the ground truth (whatever that's supposed to mean and look like)\n",
    "4. Then we just stare at it, like with my other visualizations, and realize what ? I don't think I've gained any new insight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdce13-8641-48c1-9693-ed1e478dd205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
