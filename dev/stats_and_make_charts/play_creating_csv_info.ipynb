{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca915a4b-a914-439c-8e5a-2cd0f199190b",
   "metadata": {},
   "source": [
    "## Amassing performance of a model in a CSV file in this format:\n",
    "\n",
    "### `Video Index, EF, GLS, ED Dice, ES Dice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60ef663-a4f5-4298-9167-939623dc8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Original_Pretrained_R2plus1DMotionSegNet.pth', 'dropout_v2_0_00_R2plus1DMotionSegNet.pth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c651c3c-069e-4af1-a7fd-2f32c5500658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v1 dropout, not in place dropout\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e1b615-8ec5-4cd9-9572-4bd84aded555",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd769b4b-1b85-42da-b533-904747b21404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.64it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.97it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 12.19it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5ebf29-c808-4a61-aed6-146148ed70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "    \n",
    "\n",
    "# from queue import SimpleQueue as squeue\n",
    "def EDESpairs(diastole, systole):\n",
    "    dframes = np.sort(np.array(diastole))\n",
    "    sframes = np.sort(np.array(systole))\n",
    "    clips = []\n",
    "    \n",
    "    inds = np.searchsorted(dframes, sframes, side='left')\n",
    "    for i, sf in enumerate(sframes):\n",
    "        if inds[i] == 0: # no prior diastolic frames for this sf\n",
    "            continue\n",
    "        best_df = diastole[inds[i]-1] # diastole frame nearest this sf.\n",
    "        if len(clips) == 0 or best_df != clips[-1][0]:\n",
    "            clips.append((best_df, sf))\n",
    "            \n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04cc3e4b-2861-4279-8b2f-7b2595badfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Pretrained_R2plus1DMotionSegNet.pth has 31575731 parameters.\n",
      "dropout_v2_0_00_R2plus1DMotionSegNet.pth has 31575731 parameters.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "loaded_in_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_save_path = f\"save_models/{model_name}\"\n",
    "     \n",
    "    \n",
    "    if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = R2plus1D_18_MotionNet()\n",
    "    elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "        \n",
    "    elif model_name == \"dropout_v3_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "\n",
    "    model = torch.nn.DataParallel(model_template_obj)\n",
    "\n",
    "\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc574327-d874-4c6e-b07f-e23a2cf08da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "def segment_a_video_with_fusion(curr_model, log_file, video, interpolate_last=True, step=1, num_clips=10, \n",
    "                                fuse_method=\"simple\", class_list=[0, 1]):\n",
    "    if video.shape[1] < 32 + num_clips * step:\n",
    "        num_clips = (video.shape[1] - 32) // step\n",
    "    if num_clips < 0:\n",
    "        log_file.write(\"Video is too short\\n\")\n",
    "        num_clips = 1\n",
    "    all_consecutive_clips = []\n",
    "\n",
    "    for shift_dis in range(0, num_clips * step, step):\n",
    "        shifted_video = video[:, shift_dis:]\n",
    "        consecutive_clips = divide_to_consecutive_clips(shifted_video, interpolate_last=interpolate_last)\n",
    "        all_consecutive_clips.append(consecutive_clips)\n",
    "\n",
    "    all_consecutive_clips = np.array(all_consecutive_clips)\n",
    "    all_segmentations = []\n",
    "\n",
    "    for i in range(len(all_consecutive_clips)):\n",
    "        consecutive_clips = all_consecutive_clips[i]\n",
    "        segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "\n",
    "        for i in range(consecutive_clips.shape[0]):\n",
    "            one_clip = np.expand_dims(consecutive_clips[i], 0)\n",
    "            segmentation_output, motion_output = curr_model(torch.Tensor(one_clip))\n",
    "            segmentation_output = F.softmax(segmentation_output, 1)\n",
    "            segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        segmentation_outputs = segmentation_outputs[1:]\n",
    "\n",
    "        all_segmentations.append(segmentation_outputs)\n",
    "\n",
    "    for i in range(len(all_segmentations)):\n",
    "        all_segmentations[i] = all_segmentations[i].transpose([1, 0, 2, 3, 4])\n",
    "        all_segmentations[i] = all_segmentations[i].reshape(2, -1, 112, 112)\n",
    "\n",
    "    all_interpolated_segmentations = []\n",
    "    for i in range(0, len(all_consecutive_clips)):\n",
    "        video_clip = video[:, i * step:]\n",
    "        if interpolate_last and (video_clip.shape[1] % 32 != 0):\n",
    "            interpolated_segmentations = torch.Tensor(all_segmentations[i]).unsqueeze(0)\n",
    "            interpolated_segmentations = F.interpolate(interpolated_segmentations, size=(video_clip.shape[1], 112, 112), \n",
    "                                                       mode=\"trilinear\", align_corners=False)\n",
    "            interpolated_segmentations = interpolated_segmentations.squeeze(0).numpy()\n",
    "            all_interpolated_segmentations.append(np.argmax(interpolated_segmentations, 0))\n",
    "        else:\n",
    "            all_interpolated_segmentations.append(np.argmax(all_segmentations[i], 0))\n",
    "\n",
    "    fused_segmentations = [all_interpolated_segmentations[0][0]]\n",
    "\n",
    "    for i in range(1, video.shape[1]):\n",
    "        if step - 1 < i:\n",
    "            images_to_fuse = []\n",
    "            for index in range(min(i, len(all_interpolated_segmentations))):\n",
    "                if i - index * step < 0:\n",
    "                    break\n",
    "                images_to_fuse.append(itk.GetImageFromArray(all_interpolated_segmentations[index][i - index * step].astype(\"uint8\"),\n",
    "                                                            isVector=False))\n",
    "            if len(images_to_fuse) <= 1:\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(images_to_fuse[0]))\n",
    "            else:\n",
    "                fused_image = fuse_images(images_to_fuse, fuse_method, class_list=class_list)\n",
    "                # If using SIMPLE, the fused image might be in type \"float\"\n",
    "                # So convert it to uint\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(fused_image).astype(\"uint8\"))\n",
    "\n",
    "    fused_segmentations = np.array(fused_segmentations)\n",
    "    \n",
    "    return fused_segmentations\n",
    "\n",
    "\n",
    "def compute_ef_using_putative_clips(fused_segmentations, test_pat_index, log_file):\n",
    "    size = np.sum(fused_segmentations, axis=(1, 2)).ravel()\n",
    "    _05cut, _85cut, _95cut = np.percentile(size, [5, 85, 95]) \n",
    "\n",
    "    trim_min = _05cut\n",
    "    trim_max = _95cut\n",
    "    trim_range = trim_max - trim_min\n",
    "    systole = find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "    diastole = find_peaks(size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "\n",
    "    # keep only real diastoles..\n",
    "    diastole = [x for x in diastole if size[x] >= _85cut]\n",
    "    # Add first frame\n",
    "    if np.mean(size[:3]) >= _85cut:\n",
    "        diastole = [0] + diastole\n",
    "    diastole = np.array(diastole)\n",
    "\n",
    "    clip_pairs = EDESpairs(diastole, systole)\n",
    "\n",
    "    one_array_of_segmentations = fused_segmentations.reshape(-1, 112, 112)\n",
    "\n",
    "    predicted_efs = []\n",
    "\n",
    "    for i in range(len(clip_pairs)):\n",
    "        output_ED = one_array_of_segmentations[clip_pairs[i][0]]\n",
    "        output_ES = one_array_of_segmentations[clip_pairs[i][1]]\n",
    "        \n",
    "        length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "        length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "\n",
    "        edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "        esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "        ef_predicted = (edv - esv) / edv * 100\n",
    "        \n",
    "        if ef_predicted < 0:\n",
    "            log_file.write(\"Negative EF at patient:{:04d}\".format(test_pat_index))\n",
    "            continue\n",
    "\n",
    "        predicted_efs.append(ef_predicted)\n",
    "\n",
    "    return predicted_efs\n",
    "\n",
    "\n",
    "def compute_ef_using_reported_clip(segmentations, ed_index, es_index):\n",
    "    output_ED = segmentations[ed_index]\n",
    "    output_ES = segmentations[es_index]\n",
    "\n",
    "    lv_ed_dice = categorical_dice((output_ED), ed_label, 1)\n",
    "    lv_es_dice = categorical_dice((output_ES), es_label, 1)\n",
    "\n",
    "    length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "    length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "    \n",
    "    edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "    esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "    ef_predicted = (edv - esv) / edv * 100\n",
    "    \n",
    "    return ef_predicted, lv_ed_dice, lv_es_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6bb24f-6d7f-4ee7-abc0-3b59112192a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "log_file_name = \"log.txt\"\n",
    "\n",
    "with open(log_file_name, \"a\") as file:\n",
    "    for j in range(len(loaded_in_models)):\n",
    "        model = loaded_in_models[j][1]\n",
    "        model_name = loaded_in_models[j][0]\n",
    "\n",
    "        ###########\n",
    "        patient_filename = []\n",
    "\n",
    "        EF_list = []\n",
    "        true_EF_list = []\n",
    "        mean_EF_list = []\n",
    "\n",
    "        lv_ed_dice = []\n",
    "        lv_es_dice = []\n",
    "\n",
    "        num_clips = 5\n",
    "        step = 1\n",
    "        interpolate_last = True\n",
    "        fuse_method = \"simple\"\n",
    "        class_list = [0, 1]\n",
    "        # class_list = None\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # choose how many videos you want to calculate EF for \n",
    "\n",
    "        # for i in range(len(test_dataset)):\n",
    "        for i in range(5):\n",
    "            test_pat_index = i\n",
    "            try:\n",
    "                video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "            except:\n",
    "                file.write(\"Get exception when trying to read the video from patient:{:04d}\\n\".format(i))\n",
    "                continue\n",
    "\n",
    "            if test_pat_index == 1053:\n",
    "                video = video[:, :80]\n",
    "\n",
    "            segmentations = segment_a_video_with_fusion(model, file, video, interpolate_last=interpolate_last, \n",
    "                                                        step=step, num_clips=num_clips,\n",
    "                                                        fuse_method=fuse_method, class_list=class_list)\n",
    "\n",
    "            predicted_efs = compute_ef_using_putative_clips(segmentations, test_pat_index=test_pat_index, log_file=file)\n",
    "\n",
    "            _, ed_dice, es_dice = compute_ef_using_reported_clip(segmentations, ed_index, es_index)\n",
    "\n",
    "            lv_ed_dice.append(ed_dice)\n",
    "            lv_es_dice.append(es_dice)\n",
    "\n",
    "            if len(predicted_efs) == 0:\n",
    "                file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "                continue\n",
    "\n",
    "            if np.isnan(np.nanmean(predicted_efs)):\n",
    "                file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "                continue\n",
    "\n",
    "            EF_list.append(predicted_efs)\n",
    "            true_EF_list.append(EF)\n",
    "            mean_EF_list.append(np.nanmean(predicted_efs))\n",
    "            patient_filename.append(filename[:-4])\n",
    "            \n",
    "            # save this video EF to log\n",
    "            file.write(f'video ind: {test_pat_index}\\n')\n",
    "            file.write(f'predicted_efs: {predicted_efs}\\n')\n",
    "            file.write(f'true EF: {EF}\\n')\n",
    "            \n",
    "            # save as csv style\n",
    "            # file.write(f'{test_pat_index},{predicted_efs},{EF}\\n')\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        file.write(f\"Model Name: {model_name}\\n\")\n",
    "        file.write(f\"Used time = {(end - start) // 60:.0f} mins {(end - start) % 60:.0f} secs\\n\")\n",
    "        ## ------------start of --------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "        errors = np.array(np.array(true_EF_list) - np.array(mean_EF_list))\n",
    "        abs_errors = abs(errors)\n",
    "\n",
    "        file.write(\"Mean absolute error (standard deviation):  {:.4f} ({:.4f}) %\\n\".format(np.mean(abs_errors), np.std(abs_errors)))\n",
    "        file.write(\"Median absolute error:  {:.4f} %\\n\".format(np.median(abs_errors)))\n",
    "        file.write(\"Bias +- 1.96 x std:  {:.4f} +- {:.4f}\\n\".format(np.mean(errors), 1.96 * np.std(errors)))\n",
    "        file.write(\"Percentile of mae 50%: {:6.4f}  75%: {:6.4f}  95%: {:6.4f}\\n\".format(np.percentile(abs_errors, 50), np.percentile(abs_errors, 75),\n",
    "                                                                            np.percentile(abs_errors, 95)))\n",
    "\n",
    "\n",
    "        file.write(\"Average ED {:.4f} ({:.4f})\\n\".format(np.mean(lv_ed_dice), np.std(lv_ed_dice)))\n",
    "        file.write(\"Median ED {:.4f}\\n\".format(np.median(lv_ed_dice)))\n",
    "        file.write(\"Average ES {:.4f} ({:.4f})\\n\".format(np.mean(lv_es_dice), np.std(lv_es_dice)))\n",
    "        file.write(\"Median ES {:.4f}\\n\".format(np.median(lv_es_dice)))\n",
    "        ## ---------------end of ----------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "        file.write('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897e9f-948f-4e45-befc-9cc44cbe7121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
