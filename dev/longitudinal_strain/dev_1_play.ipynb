{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c45069-49f7-4589-a2a1-9b06bd312be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.51it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.38it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Pretrained_R2plus1DMotionSegNet.pth has 31575731 parameters.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "# ap = argparse.ArgumentParser(description=\"test\")\n",
    "# ap.add_argument('-l','--list', nargs='+', help='<Required> Set flag', required=True)\n",
    "# args = ap.parse_args()\n",
    "\n",
    "# model_names = args.list # we are going to assume that only one model is used each time for this script\n",
    "\n",
    "model_names = ['Original_Pretrained_R2plus1DMotionSegNet.pth']\n",
    "\n",
    "log_file_name = f\"./warren-random/quantifying-performance/{model_names[0]}log.txt\"\n",
    "\n",
    "USE_CUSTOM_NUM_VIDS_TO_GO_THRU = True\n",
    "CUSTOM_NUM_VIDS = 5\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# %config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v1 dropout, not in place dropout\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "\n",
    "# for finding lv seg borders\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)\n",
    "\n",
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "    \n",
    "\n",
    "# from queue import SimpleQueue as squeue\n",
    "def EDESpairs(diastole, systole):\n",
    "    dframes = np.sort(np.array(diastole))\n",
    "    sframes = np.sort(np.array(systole))\n",
    "    clips = []\n",
    "    \n",
    "    inds = np.searchsorted(dframes, sframes, side='left')\n",
    "    for i, sf in enumerate(sframes):\n",
    "        if inds[i] == 0: # no prior diastolic frames for this sf\n",
    "            continue\n",
    "        best_df = diastole[inds[i]-1] # diastole frame nearest this sf.\n",
    "        if len(clips) == 0 or best_df != clips[-1][0]:\n",
    "            clips.append((best_df, sf))\n",
    "            \n",
    "    return clips\n",
    "\n",
    "loaded_in_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_save_path = f\"save_models/{model_name}\"\n",
    "     \n",
    "    \n",
    "    if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = R2plus1D_18_MotionNet()\n",
    "    elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "        model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "        \n",
    "    elif model_name == \"dropout_v3_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v3_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_00_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_10_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "    elif model_name == \"dropout_v4_0_25_R2plus1D_18_MotionNet.pth\":\n",
    "        model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "\n",
    "    model = torch.nn.DataParallel(model_template_obj)\n",
    "\n",
    "\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18deaa7-7d1d-48d7-8f95-ebe11771b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentations.shape: (248, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "def segment_a_video_with_fusion(curr_model, log_file, video, interpolate_last=True, step=1, num_clips=10, \n",
    "                                fuse_method=\"simple\", class_list=[0, 1]):\n",
    "    if video.shape[1] < 32 + num_clips * step:\n",
    "        num_clips = (video.shape[1] - 32) // step\n",
    "    if num_clips < 0:\n",
    "        log_file.write(\"Video is too short\\n\")\n",
    "        num_clips = 1\n",
    "    all_consecutive_clips = []\n",
    "\n",
    "    for shift_dis in range(0, num_clips * step, step):\n",
    "        shifted_video = video[:, shift_dis:]\n",
    "        consecutive_clips = divide_to_consecutive_clips(shifted_video, interpolate_last=interpolate_last)\n",
    "        all_consecutive_clips.append(consecutive_clips)\n",
    "\n",
    "    all_consecutive_clips = np.array(all_consecutive_clips)\n",
    "    all_segmentations = []\n",
    "\n",
    "    for i in range(len(all_consecutive_clips)):\n",
    "        consecutive_clips = all_consecutive_clips[i]\n",
    "        segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "\n",
    "        for i in range(consecutive_clips.shape[0]):\n",
    "            one_clip = np.expand_dims(consecutive_clips[i], 0)\n",
    "            segmentation_output, motion_output = curr_model(torch.Tensor(one_clip))\n",
    "            segmentation_output = F.softmax(segmentation_output, 1)\n",
    "            segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        segmentation_outputs = segmentation_outputs[1:]\n",
    "\n",
    "        all_segmentations.append(segmentation_outputs)\n",
    "\n",
    "    for i in range(len(all_segmentations)):\n",
    "        all_segmentations[i] = all_segmentations[i].transpose([1, 0, 2, 3, 4])\n",
    "        all_segmentations[i] = all_segmentations[i].reshape(2, -1, 112, 112)\n",
    "\n",
    "    all_interpolated_segmentations = []\n",
    "    for i in range(0, len(all_consecutive_clips)):\n",
    "        video_clip = video[:, i * step:]\n",
    "        if interpolate_last and (video_clip.shape[1] % 32 != 0):\n",
    "            interpolated_segmentations = torch.Tensor(all_segmentations[i]).unsqueeze(0)\n",
    "            interpolated_segmentations = F.interpolate(interpolated_segmentations, size=(video_clip.shape[1], 112, 112), \n",
    "                                                       mode=\"trilinear\", align_corners=False)\n",
    "            interpolated_segmentations = interpolated_segmentations.squeeze(0).numpy()\n",
    "            all_interpolated_segmentations.append(np.argmax(interpolated_segmentations, 0))\n",
    "        else:\n",
    "            all_interpolated_segmentations.append(np.argmax(all_segmentations[i], 0))\n",
    "\n",
    "    fused_segmentations = [all_interpolated_segmentations[0][0]]\n",
    "\n",
    "    for i in range(1, video.shape[1]):\n",
    "        if step - 1 < i:\n",
    "            images_to_fuse = []\n",
    "            for index in range(min(i, len(all_interpolated_segmentations))):\n",
    "                if i - index * step < 0:\n",
    "                    break\n",
    "                images_to_fuse.append(itk.GetImageFromArray(all_interpolated_segmentations[index][i - index * step].astype(\"uint8\"),\n",
    "                                                            isVector=False))\n",
    "            if len(images_to_fuse) <= 1:\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(images_to_fuse[0]))\n",
    "            else:\n",
    "                fused_image = fuse_images(images_to_fuse, fuse_method, class_list=class_list)\n",
    "                # If using SIMPLE, the fused image might be in type \"float\"\n",
    "                # So convert it to uint\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(fused_image).astype(\"uint8\"))\n",
    "\n",
    "    fused_segmentations = np.array(fused_segmentations)\n",
    "    \n",
    "    return fused_segmentations\n",
    "\n",
    "\n",
    "def compute_ef_using_putative_clips(fused_segmentations, test_pat_index, log_file):\n",
    "    size = np.sum(fused_segmentations, axis=(1, 2)).ravel()\n",
    "    _05cut, _85cut, _95cut = np.percentile(size, [5, 85, 95]) \n",
    "\n",
    "    trim_min = _05cut\n",
    "    trim_max = _95cut\n",
    "    trim_range = trim_max - trim_min\n",
    "    systole = find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "    diastole = find_peaks(size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "\n",
    "    # keep only real diastoles..\n",
    "    diastole = [x for x in diastole if size[x] >= _85cut]\n",
    "    # Add first frame\n",
    "    if np.mean(size[:3]) >= _85cut:\n",
    "        diastole = [0] + diastole\n",
    "    diastole = np.array(diastole)\n",
    "\n",
    "    clip_pairs = EDESpairs(diastole, systole)\n",
    "\n",
    "    one_array_of_segmentations = fused_segmentations.reshape(-1, 112, 112)\n",
    "\n",
    "    predicted_efs = []\n",
    "\n",
    "    for i in range(len(clip_pairs)):\n",
    "        output_ED = one_array_of_segmentations[clip_pairs[i][0]]\n",
    "        output_ES = one_array_of_segmentations[clip_pairs[i][1]]\n",
    "        \n",
    "        length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "        length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "\n",
    "        edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "        esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "        ef_predicted = (edv - esv) / edv * 100\n",
    "        \n",
    "        if ef_predicted < 0:\n",
    "            log_file.write(\"Negative EF at patient:{:04d}\".format(test_pat_index))\n",
    "            continue\n",
    "\n",
    "        predicted_efs.append(ef_predicted)\n",
    "\n",
    "    return predicted_efs\n",
    "\n",
    "\n",
    "def compute_ef_using_reported_clip(segmentations, ed_index, es_index):\n",
    "    output_ED = segmentations[ed_index]\n",
    "    output_ES = segmentations[es_index]\n",
    "\n",
    "    lv_ed_dice = categorical_dice((output_ED), ed_label, 1)\n",
    "    lv_es_dice = categorical_dice((output_ES), es_label, 1)\n",
    "\n",
    "    length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "    length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "    \n",
    "    edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "    esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "    ef_predicted = (edv - esv) / edv * 100\n",
    "    \n",
    "    return ef_predicted, lv_ed_dice, lv_es_dice\n",
    "\n",
    "\n",
    "def strain_value(l_0, l_i):\n",
    "    '''\n",
    "    inputs: l_0, l_i -- original length and new length at some time point, respectively\n",
    "    output: e -- strain value (positive for elongation, negative for compressing/shortening) as a percentage (e.g. output 0.155 == 15.5 %)\n",
    "    \n",
    "    examples: \n",
    "        l_i = 10\n",
    "        l_0 = 5\n",
    "        e == (10 - 5) / 5 = 1, factor of lengthening relative to original value\n",
    "        \n",
    "        l_i = 5\n",
    "        l_0 = 5\n",
    "        e == (5 - 5) / 5 = 0, no strain\n",
    "    '''\n",
    "    return (l_i - l_0) / l_0\n",
    "\n",
    "\n",
    "def rmse(x, y):\n",
    "    ''' return root mean square error difference between the two values passed in'''\n",
    "    return np.sqrt((x - y) ** 2)\n",
    "\n",
    "def fromSegOutToEDESLVMasksAndBorders(segmentation_outputs, es_index, ed_index):\n",
    "    '''\n",
    "    input: segmentation_outputs -- the segmentation outputs from the model wrapped in a list of len number of models that we have\n",
    "    output: [[ed lv seg, es lv seg], [ed lv seg border, es lv seg border]] all will be 112 x 112 images which pixels of value 0 or 255\n",
    "    '''\n",
    "    \n",
    "    # how many frames ahead is the ES frame relative to the ED frame?\n",
    "    # delta_ed_es_frames = es_index - ed_index\n",
    "    \n",
    "    # clip_to_grab_index = -1   # grab the last clip where ED frame is the first frame\n",
    "    \n",
    "    # clip_with_ed_frame_index_0 = segmentation_outputs[clip_to_grab_index] \n",
    "    \n",
    "    # move from gpu mem to cpu mem if not\n",
    "    # clip_with_ed_frame_index_0 = clip_with_ed_frame_index_0.cpu().numpy()\n",
    "\n",
    "    foo = np.argmax(segmentation_outputs, axis=0)\n",
    "    foo_times_255 = 255 * foo\n",
    "\n",
    "    ed_lv_seg = foo_times_255[ed_index]\n",
    "    es_lv_seg = foo_times_255[es_index]\n",
    "    \n",
    "    \n",
    "    image_8bit = np.uint8(foo_times_255)\n",
    "    ed_im = image_8bit[0]\n",
    "    es_im = image_8bit[delta_ed_es_frames]\n",
    "\n",
    "#     print(f'image_8bit: {image_8bit}')\n",
    "    \n",
    "#     print(f'ed_im: {ed_im}')\n",
    "#     print(f'ed_im.shape: {ed_im.shape}')\n",
    "    \n",
    "    ret, thresh = cv.threshold(ed_im, 127, 255, 0)\n",
    "    ed_contours, ed_hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    ret, thresh = cv.threshold(es_im, 127, 255, 0)\n",
    "    es_contours, es_hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    \n",
    "    ed_blank = np.zeros((112,112))\n",
    "    cv.drawContours(ed_blank, ed_contours, -1, (255,255,255), 1)\n",
    "\n",
    "    es_blank = np.zeros((112,112))\n",
    "    cv.drawContours(es_blank, es_contours, -1, (255,255,255), 1)\n",
    "    \n",
    "    return [[ed_lv_seg, es_lv_seg], [ed_blank, es_blank]]\n",
    "    \n",
    "def groundTruthBoundaries(ed_label, es_label):\n",
    "    '''\n",
    "    input: ed_label, es_label -- 112 x 112 manual segmented ed/es lv answers (input values are either 0 or 1)\n",
    "    output: [[ed label, es label], [ed label border, es label border]]\n",
    "    '''\n",
    "    ed_im = np.uint8(ed_label * 255)\n",
    "    es_im = np.uint8(es_label * 255)\n",
    "\n",
    "    ret, thresh = cv.threshold(ed_im, 127, 255, 0)\n",
    "    ed_contours, ed_hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    ret, thresh = cv.threshold(es_im, 127, 255, 0)\n",
    "    es_contours, es_hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "    ed_blank = np.zeros((112,112))\n",
    "    cv.drawContours(ed_blank, ed_contours, -1, (255,255,255), 1)\n",
    "\n",
    "    es_blank = np.zeros((112,112))\n",
    "    cv.drawContours(es_blank, es_contours, -1, (255,255,255), 1)\n",
    "\n",
    "    return [[ed_im, es_im], [ed_blank, es_blank]]\n",
    "\n",
    "\n",
    "\n",
    "with open(log_file_name, \"a\") as file:\n",
    "    file.write(\"Video Index, Predicted EF, True EF, Predicted GLS, True GLS, ED Dice, ES Dice\\n\")\n",
    "    for j in range(len(loaded_in_models)):\n",
    "        model = loaded_in_models[j][1]\n",
    "        model_name = loaded_in_models[j][0]\n",
    "\n",
    "        ###########\n",
    "        patient_filename = []\n",
    "\n",
    "        # EF_list = []\n",
    "        # true_EF_list = []\n",
    "        # mean_EF_list = []\n",
    "\n",
    "        # lv_ed_dice = []\n",
    "        # lv_es_dice = []\n",
    "\n",
    "        num_clips = 5\n",
    "        step = 1\n",
    "        interpolate_last = True\n",
    "        fuse_method = \"simple\"\n",
    "        class_list = [0, 1]\n",
    "        # class_list = None\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # choose how many videos you want to calculate EF for \n",
    "        if USE_CUSTOM_NUM_VIDS_TO_GO_THRU:\n",
    "            NUM_VIDS_TO_GO_THRU = CUSTOM_NUM_VIDS\n",
    "        else:\n",
    "            NUM_VIDS_TO_GO_THRU = len(test_dataset)\n",
    "\n",
    "        # for i in range(len(test_dataset)):\n",
    "        # for i in range(5):\n",
    "        for i in range(NUM_VIDS_TO_GO_THRU):\n",
    "            test_pat_index = i\n",
    "            try:\n",
    "                video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "            except:\n",
    "                file.write(\"Get exception when trying to read the video from patient:{:04d}\\n\".format(i))\n",
    "                continue\n",
    "\n",
    "            if test_pat_index == 1053:\n",
    "                video = video[:, :80]\n",
    "\n",
    "            segmentations = segment_a_video_with_fusion(model, file, video, interpolate_last=interpolate_last, \n",
    "                                                        step=step, num_clips=num_clips,\n",
    "                                                        fuse_method=fuse_method, class_list=class_list)\n",
    "            \n",
    "            print(f'segmentations.shape: {segmentations.shape}')\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "#             # EFs\n",
    "#             predicted_efs = compute_ef_using_putative_clips(segmentations, test_pat_index=test_pat_index, log_file=file)\n",
    "\n",
    "#             # Dice\n",
    "#             _, ed_dice, es_dice = compute_ef_using_reported_clip(segmentations, ed_index, es_index)\n",
    "            \n",
    "#             # GLS\n",
    "#             y = groundTruthBoundaries(ed_label, es_label)\n",
    "#             ground_truth_strain = strain_value(np.count_nonzero(y[1][0] == 255), np.count_nonzero(y[1][1] == 255))\n",
    "\n",
    "#             x = fromSegOutToEDESLVMasksAndBorders(segmentations, es_index, ed_index)\n",
    "#             seg_out_strain = strain_value(np.count_nonzero(x[1][0] == 255), np.count_nonzero(x[1][1] == 255))\n",
    "\n",
    "\n",
    "#             # lv_ed_dice.append(ed_dice)\n",
    "#             # lv_es_dice.append(es_dice)\n",
    "\n",
    "#             if len(predicted_efs) == 0:\n",
    "#                 file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "#                 continue\n",
    "\n",
    "#             if np.isnan(np.nanmean(predicted_efs)):\n",
    "#                 file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "#                 continue\n",
    "\n",
    "#             # EF_list.append(predicted_efs)\n",
    "#             # true_EF_list.append(EF)\n",
    "#             # mean_EF_list.append(np.nanmean(predicted_efs))\n",
    "#             # patient_filename.append(filename[:-4])\n",
    "            \n",
    "#             # save this video EF to log\n",
    "#             # file.write(f'video ind: {test_pat_index}\\n')\n",
    "#             # file.write(f'predicted_efs: {predicted_efs}\\n')\n",
    "#             # file.write(f'true EF: {EF}\\n')\n",
    "            \n",
    "#             # save as csv style\n",
    "#             file.write(f'{test_pat_index},{np.nanmean(predicted_efs)},{EF},{seg_out_strain},{ground_truth_strain},{ed_dice},{es_dice}\\n')\n",
    "\n",
    "#         end = time.time()\n",
    "\n",
    "\n",
    "#         file.write(f\"Model Name: {model_name}\\n\")\n",
    "#         file.write(f\"Used time = {(end - start) // 60:.0f} mins {(end - start) % 60:.0f} secs\\n\")\n",
    "#         ## ------------start of --------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "#         errors = np.array(np.array(true_EF_list) - np.array(mean_EF_list))\n",
    "#         abs_errors = abs(errors)\n",
    "\n",
    "#         file.write(\"Mean absolute error (standard deviation):  {:.4f} ({:.4f}) %\\n\".format(np.mean(abs_errors), np.std(abs_errors)))\n",
    "#         file.write(\"Median absolute error:  {:.4f} %\\n\".format(np.median(abs_errors)))\n",
    "#         file.write(\"Bias +- 1.96 x std:  {:.4f} +- {:.4f}\\n\".format(np.mean(errors), 1.96 * np.std(errors)))\n",
    "#         file.write(\"Percentile of mae 50%: {:6.4f}  75%: {:6.4f}  95%: {:6.4f}\\n\".format(np.percentile(abs_errors, 50), np.percentile(abs_errors, 75),\n",
    "#                                                                             np.percentile(abs_errors, 95)))\n",
    "\n",
    "\n",
    "#         file.write(\"Average ED {:.4f} ({:.4f})\\n\".format(np.mean(lv_ed_dice), np.std(lv_ed_dice)))\n",
    "#         file.write(\"Median ED {:.4f}\\n\".format(np.median(lv_ed_dice)))\n",
    "#         file.write(\"Average ES {:.4f} ({:.4f})\\n\".format(np.mean(lv_es_dice), np.std(lv_es_dice)))\n",
    "#         file.write(\"Median ES {:.4f}\\n\".format(np.median(lv_es_dice)))\n",
    "#         ## ---------------end of ----------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "#         file.write('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df45c0-66ec-4446-b924-3437356fbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49870dea-353b-402d-8baa-a5b09180fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0b2ff5b690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAT0ElEQVR4nO3dfYxcV33G8e+DnRCSQG1jYhnbbUy1JRjUNKmbF6BtinlZpxFOpUZ1aMCNjKxICQSEBE6RGlX9hwqKADWJtQompqBYUWLhbeRiLANNK5pgm6TBjjHeOq29yRJnSRuqIAiOf/3jnknHm92dmZ07e++cfT7SaPbemTn3J3v22XPPuS+KCMzMcvCqqgswMyuLA83MsuFAM7NsONDMLBsONDPLhgPNzLLhQDOzSkjaKumkpINTvC5JX5I0IulxSZe2arNngSZpUNKRVMzmXm3HzPrWPcDgNK+vBQbSYxNwV6sGexJokuYBd6SCVgHXS1rVi22ZWX+KiIeA56Z5yzrgq1F4GFggael0bc4vs8AmlwEjEXEMQNL2VNwTk71Zkk9XMOu98Yh4QzcNDA4Oxvj4eMv3HThw4BDwi6ZVQxEx1OHmlgEnmpZH07qxqT7Qq0CbrJDLm98gaRNFN9LMZsd/ddvA+Pg4+/fvb/k+Sb+IiNVdbk6TrJu289OrQGtZSErrIXAPzayfzOL536PAiqbl5cDT032gV5MCHRdiZv3h9OnTLR8lGQY+lGY7rwCej4gpdzehdz20fcCApJXAU8B64AM92paZzZKIKK2HJule4CpgsaRR4HbgrLSdLcAu4GpgBPg5cGOrNnsSaBFxStItwG5gHrA1Ig71YltmNrvKCrSIuL7F6wHc3EmbveqhERG7KBLWzDJS52so9izQzCxPDjQzy4YDzcyyEBFlzmKWzoFmZh1xD83MsuFAM7NsONDMLAtlHljbCw40M+uIJwXMLBvuoZlZFrzLaWZZcaCZWTYcaGaWDQeamWXBpz6ZWVbcQzOzbDjQzCwbDjQzy4YDzcyy4EkBM8uKe2hmlg0Hmpllw4FmZlnwyelmlhUHmpllw7OcZpYN99DMLAseQzOzrDjQzCwbDjQzy0adA+1VM/2gpBWSviPpsKRDkm5N6xdJ2iPpaHpeWF65ZlalxrmcrR5VmXGgAaeAT0TEW4ArgJslrQI2A3sjYgDYm5bNLBONiYHpHlWZcaBFxFhE/CD9/L/AYWAZsA7Ylt62Dbi22yLNrD7qHGiljKFJuhC4BHgEWBIRY1CEnqQLpvjMJmBTGds3s9mT5Rhag6TzgQeAj0XEz9r9XEQMRcTqiFjdbQ1mNnvK6qFJGpR0RNKIpFcMTUn6NUn/KOnf0zj9ja3a7CrQJJ1FEWZfj4gdafUzkpam15cCJ7vZhpnVR1mTApLmAXcAa4FVwPVpDL7ZzcATEXExcBXwd5LOnq7dbmY5BXwZOBwRn296aRjYkH7eAOyc6TbMrH5K6qFdBoxExLGIeBHYTjH+fsamgNemrDkfeI5iMnJK3YyhvQP4IPBDSY+ldX8JfAa4T9JG4DhwXRfbMLOaaTOwFkva37Q8FBFDTcvLgBNNy6PA5RPa+HuKDtLTwGuBP4uIabt/Mw60iPhXQFO8vGam7drsmOnAbvHH0uayNr874y3Gxyf7Ik1s+H3AY8C7gN8E9kj6l+nG6rueFDCzuaOd3c02A28UWNG0vJyiJ9bsRmBHFEaAJ4GLpmvUgWYdqcvxRladkgJtHzAgaWUa6F9PsXvZ7Dhpb0/SEuDNwLHpGvW5nGbWkTJObYqIU5JuAXYD84CtEXFI0k3p9S3A3wD3SPohxS7qpyJifLp2HWhzTNm9qonteYwtf2V9hyJiF7BrwrotTT8/Dby3kzYdaGbWtroPNTjQrFSNL7t7avlyoJlZNhxoNue4p5YvB5rNWZ40yEvjXM66cqCZWUfcQzNLvCva/xxoZpYNB5rZBO6p9S8HmpllwZMCZpYV99DMLBsONLMpRITH0fqMA83MsuCT081a8Ixnf3GgmVk2PMtp1gb31PqDe2hmlgWPoZlZVhxoZpYNB5pZBzyWVm8ONDPLgs/lNJsh99TqyT00M8uGA83MsuFAM7Ns1DnQXtVtA5LmSXpU0oNpeZGkPZKOpueF3ZdpZnXQmBRo9ahK14EG3AocblreDOyNiAFgb1o2s0w0zhaY7lGVrgJN0nLgj4G7m1avA7aln7cB13azDSuXpL6bNaz6l8TOVOdA63YM7QvAJ4HXNq1bEhFjABExJumCyT4oaROwqcvtm9ksq/Mflxn30CRdA5yMiAMz+XxEDEXE6ohYPdMazGx2tdM769ce2juA90u6GjgHeJ2krwHPSFqaemdLgZNlFGpm9ZBlDy0ibouI5RFxIbAe+HZE3AAMAxvS2zYAO7uu0gyPpdVFnWc5e3Ec2meA+yRtBI4D1/VgG2ZWgbr/USnjsA0i4rsRcU36+acRsSYiBtLzc2Vsw8zqoawxNEmDko5IGpE06eFdkq6S9JikQ5L+uVWbPlPAzDpSRg9N0jzgDuA9wCiwT9JwRDzR9J4FwJ3AYEQcn+qIiWal9NDMbO4oqYd2GTASEcci4kVgO8UxrM0+AOyIiONpuy0nGB1oZta2Dk59Wixpf9Nj4jGny4ATTcujaV2z3wIWSvqupAOSPtSqPu9ymllH2uyBjbc4xnSy01UmNjwf+F1gDfAa4N8kPRwRP56qUQeamXWkpFnOUWBF0/Jy4OlJ3jMeES8AL0h6CLgYmDLQvMs5R/XjOZ0NdT90IHcljaHtAwYkrZR0NsWxrMMT3rMT+H1J8yWdC1zOmRfCeAX30MysI2X8MYmIU5JuAXYD84CtEXFI0k3p9S0RcVjSN4HHgdPA3RFxcLp2HWhm1rYye8cRsQvYNWHdlgnLnwU+226bDjQz64jv+mRm2ajz+KUDzcw64kAzsyzUfYbZgWZ9yzciroYDzcyy4UAzs2x4ltPMsuAxNDPLigPNzLLhQDPrIc92zi4HmplloXGBx7pyoJlZR9xDM7NsONDMLBsONDPLhgPNzLLgA2vNLCue5TSzbLiHZmbZcKBZ7dT5S2n15TE0M8tKnQOtqxsNS1og6X5JP5J0WNKVkhZJ2iPpaHpeWFaxZla9km403BPd3jn9i8A3I+Iiilu0HwY2A3sjYgDYm5bNLBOnT59u+ajKjANN0uuAPwC+DBARL0bE/wDrgG3pbduAa7st0qwdVfcO5oJ2emf92kN7E/As8BVJj0q6W9J5wJKIGANIzxdM9mFJmyTtl7S/ixrMbJblGmjzgUuBuyLiEuAFOti9jIihiFgdEau7qMHMZlmugTYKjEbEI2n5foqAe0bSUoD0fLK7Es2sTrIMtIj4CXBC0pvTqjXAE8AwsCGt2wDs7KpCM6uNxgUe6zop0O1xaB8Bvi7pbOAYcCNFSN4naSNwHLiuy22YWY3UeeKlq0CLiMeAycbA1nTTrvVOnb+M1h/q/B3ymQJm1hEHmpllw4FmZlmoehazFQeamXWkzhd47PZcTjObY8o6Dk3SoKQjkkYkTXlQvqTfk/SSpD9t1aYDzcw6UkagSZoH3AGsBVYB10taNcX7/hbY3U5tDjQza1uJJ6dfBoxExLGIeBHYTnFhi4k+AjxAm2ccOdDMrCNtBtrixsUn0mPThGaWASealkfTupdJWgb8CbCl3do8KWBmHWmzBzbe4sITmqzpCctfAD4VES9Jk739lRxoZtaRkmY5R4EVTcvLgacnvGc1sD2F2WLgakmnIuIbUzXqQDOztpV4HNo+YEDSSuApYD3wgQnbWtn4WdI9wIPThRk40MysQ2UEWkScknQLxezlPGBrRBySdFN6ve1xs2YOtDmmMRZR56O9rd7K+u5ExC5g14R1kwZZRPxFO2060MysI3X+Y+hAs2y0OxNmM9e4wGNdOdDMrCPuoZlZNhxoZpYNB5pZD3nsbHY50MwsC77Ao9WSj0ezmfIsp5llo85/BB1o1nc8ZlYtB5qZZcFjaFZr/TCW5h5ZvdT5u+JAM7OOeFLAaq/Knpp7YP3Du5xmlhUHmvWNsnpq7nXlq86B1tVdnyR9XNIhSQcl3SvpHEmLJO2RdDQ9LyyrWDOrXlk3Gu6FGQdausXUR4HVEfE2isvorgc2A3sjYgDYm5atz0g649Hq9Vbvt3xkGWjJfOA1kuYD51LctWUdsC29vg24tsttmFlNNC7w2OpRlRkHWkQ8BXwOOA6MAc9HxLeAJRExlt4zBlxQRqFWLffArCHLHloaG1sHrATeCJwn6YYOPr+pcVflmdZgZrOvzoHWzSznu4EnI+JZAEk7gLcDz0haGhFjkpYCJyf7cEQMAUPps/WdNjGzM+Q6y3kcuELSuSr2QdYAh4FhYEN6zwZgZ3clmlldtNM768seWkQ8Iul+4AfAKeBRih7X+cB9kjZShN51ZRRqZvVQ5x5aVwfWRsTtwO0TVv+SordmZhnyuZxmlo1se2hmNrdUPUbWigPNzDriQDOzbDjQzCwbnhQwsyx4DM3MsuJAM7NsONDMLBt1DrRur4dmZnNMWedyShqUdETSiKRXXAhW0p9Lejw9vifp4lZtuodmZm1rXOCxW5LmAXcA7wFGgX2ShiPiiaa3PQn8YUT8t6S1FOeKXz5duw40M+tISbuclwEjEXEMQNJ2iusrvhxoEfG9pvc/DCxv1agDzcw60magLZ5w8dahdA3EhmXAiablUabvfW0E/qnVRh1oZtaRNgNtPCJWT/P6ZNdxn7RhSX9EEWjvbLVRB5qZta3EA2tHgRVNy8spbrJ0Bkm/DdwNrI2In7Zq1LOcZtaRkmY59wEDklZKOpviFpjDzW+Q9OvADuCDEfHjdhp1D83MOlLGLGdEnJJ0C7Cb4p6+WyPikKSb0utbgL8CXg/cme40dqrFbqwDzcw6U9aBtRGxC9g1Yd2Wpp8/DHy4kzYdaGbWNp+cbmZZcaCZWTYcaGaWDV/g0cyy4DE0M8uKA83MsuFAM7NsONDMLBsONDPLQlkXeOwVB5qZdcQ9NDPLRp0DreXlgyRtlXRS0sGmdYsk7ZF0ND0vbHrttnTTgyOS3terws2sGmXdJKUX2rke2j3A4IR1m4G9ETEA7E3LSFpFcV2jt6bP3JluhmBmGWgnzGodaBHxEPDchNXrgG3p523AtU3rt0fELyPiSWCE4mYIZpaJOgfaTMfQlkTEGEBEjEm6IK1fRnF3lobRtO4VJG0CNs1w+2ZWkbk0y9n2jQ/SHWCGACTVd5TRzM7Q15MCU3hG0lKA9HwyrW/rxgdm1p/6fgxtCsPAhvTzBmBn0/r1kl4taSUwAHy/uxLNrE7qHGgtdzkl3QtcRXHj0FHgduAzwH2SNgLHgesA0k0O7qO4+/Ep4OaIeKlHtZtZBeq8y6k6FOcxNLNZcaDVXZNaOeuss2LBggUt3zc+Pt71tmbCZwqYWduq3qVsxYFmZh1xoJlZNhxoZpYNB5qZZcOBZmZZ8AUezSwr7qGZWTYcaGaWDQeamWXBB9aaWVYcaGaWDc9ymlk23EMzsyzUfQxtphd4NLM5qqwLPEoaTLe7HJG0eZLXJelL6fXHJV3aqk0Hmpl1pIxAS7e3vANYC6wCrk+3wWy2luKq1wMUN1S6q1W7DjQz68jp06dbPtpwGTASEcci4kVgO8VtMJutA74ahYeBBY17mUylLmNo48AL6bmOFuPaZsK1da6Xdf1GCW3spqixlXMk7W9aHkp3emtYBpxoWh4FLp/QxmTvWQaMTbXRWgRaRLxB0v4qLtnbDtc2M66tc3WtqyEiBktqqp1bXrZ9W8wG73KaWRXaueVlx7fFdKCZWRX2AQOSVko6G1hPcRvMZsPAh9Js5xXA8xEx5e4m1GSXMxlq/ZbKuLaZcW2dq2tdpYqIU5JuoRiTmwdsTbfBvCm9vgXYBVwNjAA/B25s1W4tbmNnZlYG73KaWTYcaGaWjVoEWqtTIGaxjhWSviPpsKRDkm5N6xdJ2iPpaHpeWGGN8yQ9KunBOtUmaYGk+yX9KP37XVmj2j6e/j8PSrpX0jlV1SZpq6STkg42rZuyFkm3pd+LI5LeNxs19rPKA63NUyBmyyngExHxFuAK4OZUy2Zgb0QMAHvTclVuBQ43Ldelti8C34yIi4CLKWqsvDZJy4CPAqsj4m0UA9DrK6ztHmDisVyT1pK+e+uBt6bP3Jl+X2wq7ZyX1csHcCWwu2n5NuC2qutKtewE3gMcAZamdUuBIxXVs5ziC/8u4MG0rvLagNcBT5ImmZrW16G2xtHmiyhm9R8E3ltlbcCFwMFW/04TfxcoZgSvrOK71y+PyntoTH16Q6UkXQhcAjwCLIl0/Et6vqCisr4AfBJoPlmuDrW9CXgW+EraHb5b0nl1qC0ingI+BxynOGXm+Yj4Vh1qazJVLbX83aizOgRax6c39Jqk84EHgI9FxM+qrKVB0jXAyYg4UHUtk5gPXArcFRGXUJyXW+Vu+cvSeNQ6YCXwRuA8STdUW1Xbave7UXd1CLSOT2/oJUlnUYTZ1yNiR1r9TOMs//R8soLS3gG8X9J/UlyZ4F2SvlaT2kaB0Yh4JC3fTxFwdajt3cCTEfFsRPwK2AG8vSa1NUxVS61+N/pBHQKtnVMgZoUkAV8GDkfE55teGgY2pJ83UIytzaqIuC0ilkfEhRT/Rt+OiBtqUttPgBOS3pxWrQGeqENtFLuaV0g6N/3/rqGYsKhDbQ1T1TIMrJf0akkrKa4L9v0K6usfVQ/iRTHYeTXwY+A/gE9XWMc7Kbr0jwOPpcfVwOspBuOPpudFFf97XcX/TwrUojbgd4D96d/uG8DCGtX218CPgIPAPwCvrqo24F6KsbxfUfTANk5XC/Dp9HtxBFhb5feuHx4+9cnMslGHXU4zs1I40MwsGw40M8uGA83MsuFAM7NsONDMLBsONDPLxv8BwXPdPXUvSLkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(segmentations[ed_index], cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6f1cf7-ad58-4594-8ae5-b96140405406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0b2c012550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(segmentations[es_index], cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed3e70-b0c6-440c-9701-b512a076264c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
