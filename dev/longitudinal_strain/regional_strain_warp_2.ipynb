{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f0bc6f-8a47-4960-84b7-11321a7c630a",
   "metadata": {},
   "source": [
    "## Also, I realize I was indeed doing it wrong before. I was splitting the ED and ES frames into their own respective horizontal cuts than computing the strain on the arbitrary cuts I am making on the ED and ES frames. I need to\n",
    "\n",
    "## 1. Slice ED Label<br>2. Warp the individual slices<br>3. Calculate Strain on $Slice_i$ and $Slice_{i+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e232bdf-a519-4b24-86d0-13da94d346c7",
   "metadata": {},
   "source": [
    "## We test out if we can get more accurately slicing of the LV into `N` slices to get local strain analyses by using `get2dpucks` function to do a little more complicated slicing of the LV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8637b66a-62ba-472e-bc62-a215ce2b9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Original_Pretrained_R2plus1DMotionSegNet.pth\"\n",
    "\n",
    "# model_name = \"dropout_v2_0_10_R2plus1DMotionSegNet.pth\"\n",
    "# model_name = \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e2610a-5445-4bdb-ba14-4748a3059335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\")\n",
    "print(os.getcwd())\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads, forgot to define in forward pass function, but still saw diff, weird.)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_50_R2plus1D_18_MotionNet import dropout_v2_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_75_R2plus1D_18_MotionNet import dropout_v2_0_75_R2plus1D_18_MotionNet\n",
    "# v3 dropout (one dropout layer defined in forward pass func, this should've been the correct way to do it.)\n",
    "from src.model.dropout_v3_0_00_R2plus1D_18_MotionNet import dropout_v3_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_10_R2plus1D_18_MotionNet import dropout_v3_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_25_R2plus1D_18_MotionNet import dropout_v3_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_50_R2plus1D_18_MotionNet import dropout_v3_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_75_R2plus1D_18_MotionNet import dropout_v3_0_75_R2plus1D_18_MotionNet\n",
    "# v4 dropout (4 dropout layers in different places in the forward func, I'm going to guess more \"generalizable\")\n",
    "from src.model.dropout_v4_0_00_R2plus1D_18_MotionNet import dropout_v4_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_10_R2plus1D_18_MotionNet import dropout_v4_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_25_R2plus1D_18_MotionNet import dropout_v4_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_50_R2plus1D_18_MotionNet import dropout_v4_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_75_R2plus1D_18_MotionNet import dropout_v4_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# for finding lv seg borders\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "# random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d880f3e-4514-4c33-a87a-ee73de1ce3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Pretrained_R2plus1DMotionSegNet.pth has 31575731 parameters.\n"
     ]
    }
   ],
   "source": [
    "model_save_path = f\"save_models/{model_name}\"\n",
    "    \n",
    "if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_10_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_10_R2plus1D_18_MotionNet()\n",
    "\n",
    "elif model_name == \"dropout_v3_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "\n",
    "model = torch.nn.DataParallel(model_template_obj)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a710c176-2ea7-4305-8418-57f730c3f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    ''' return root mean square error difference between the two values passed in'''\n",
    "    return np.sqrt((x - y) ** 2)\n",
    "\n",
    "def strain_value(l_0, l_i):\n",
    "    '''\n",
    "    inputs: l_0, l_i -- original length and new length at some time point, respectively\n",
    "    output: e -- strain value (positive for elongation, negative for compressing/shortening) as a percentage (e.g. output 0.155 == 15.5 %)\n",
    "    \n",
    "    examples: \n",
    "        l_i = 10\n",
    "        l_0 = 5\n",
    "        e == (10 - 5) / 5 = 1, factor of lengthening relative to original value\n",
    "        \n",
    "        l_i = 5\n",
    "        l_0 = 5\n",
    "        e == (5 - 5) / 5 = 0, no strain\n",
    "    '''\n",
    "    return (l_i - l_0) / l_0\n",
    "\n",
    "def give_boundary(x):\n",
    "    '''\n",
    "    input: \n",
    "        x (112, 112) one-hot encoded lv mask/segmentation\n",
    "        unique values (0,1)\n",
    "        has to be numpy ndarray on cpu mem\n",
    "    output: \n",
    "        y (112, 112) black and white picture of boundary of lv\n",
    "        unique vals (0,1)\n",
    "    '''\n",
    "    foo = np.uint8(x * 255)\n",
    "    ret, thresh = cv.threshold(foo, 127, 255, 0)\n",
    "    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    blank = np.zeros((112,112))\n",
    "    cv.drawContours(blank, contours, -1, (255,255,255), 1)\n",
    "    \n",
    "    # return boundary image with unique values of (0,1)\n",
    "    blank = blank / 255\n",
    "    \n",
    "    return blank\n",
    "\n",
    "def boundaries_to_strain(before, after):\n",
    "    '''\n",
    "    input:\n",
    "        before (112, 112) boundary of lv\n",
    "        after (112, 112) boundary of lv\n",
    "        expect unique values of (0,1)\n",
    "        \n",
    "    output: \n",
    "        y - floating point number representing strain value, left as a decimal, NOT multiplied by 100\n",
    "    '''\n",
    "    # # make sure values in image are only 0 and 1\n",
    "    # check_unique_vals = np.unique(before)\n",
    "    # if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "    #     before = before / 255\n",
    "    # check_unique_vals = np.unique(after)\n",
    "    # if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "    #     after = after / 255\n",
    "        \n",
    "    # cut basal plane out\n",
    "    before = cut_basal_plane_out(before)\n",
    "    after = cut_basal_plane_out(after)\n",
    "    \n",
    "    # count lv pixels\n",
    "    l_0 = np.count_nonzero(before == 1)\n",
    "    l_i = np.count_nonzero(after == 1)\n",
    "    \n",
    "    return strain_value(l_0, l_i)\n",
    "    \n",
    "def images_to_strain(ed_frame, es_frame):\n",
    "    '''\n",
    "    input:\n",
    "        ed_frame (112, 112)\n",
    "        es_frame (112, 112)\n",
    "        \n",
    "        expect vals to be one-hot encoded (1's for lv, 0's for not lv)\n",
    "    output:\n",
    "        x - floating point number\n",
    "        strain value (some decimal, should be negative)\n",
    "    '''\n",
    "    # get boundaries, then cut basal plane out\n",
    "    ed_bound = cut_basal_plane_out(give_boundary(ed_frame))\n",
    "    es_bound = cut_basal_plane_out(give_boundary(es_frame))\n",
    "    # compute strain and return\n",
    "    return boundaries_to_strain(ed_bound, es_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5694d41b-0b82-4a08-870e-71dcbaec43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blot_out_given_rect(rectangle_corners, I, replace_val = 0):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        I - shape (112, 112) \n",
    "        \n",
    "    output:\n",
    "        I_copy -- deepcopy of original image I with the defined rectangle section given zeroed out in the image\n",
    "    '''\n",
    "    \n",
    "     # i = row, j = col, we're thinking in ij/row col instead of xy\n",
    "    if rectangle_corners[0][0] < rectangle_corners[1][0]:\n",
    "        i_start = int(rectangle_corners[0][0])\n",
    "        i_end = int(rectangle_corners[1][0])\n",
    "    else:\n",
    "        i_end = int(rectangle_corners[0][0])\n",
    "        i_start = int(rectangle_corners[1][0])\n",
    "        \n",
    "    if rectangle_corners[1][1] < rectangle_corners[0][1]:\n",
    "        j_start = int(rectangle_corners[1][1])\n",
    "        j_end = int(rectangle_corners[0][1])\n",
    "    else:\n",
    "        j_end = int(rectangle_corners[1][1])\n",
    "        j_start = int(rectangle_corners[0][1])\n",
    "        \n",
    "    # make copy and alter then return\n",
    "    import copy \n",
    "    I_copy = copy.deepcopy(I)\n",
    "    \n",
    "    # zero out everything at and below the highest of the two index points (i_start)\n",
    "    I_copy[i_start : I.shape[0], 0 : I.shape[1]] = replace_val\n",
    "    \n",
    "    return I_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1220fa-dd0a-4753-b937-f52e31da96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_with_rect(rectangle_corners, I, replace_val=127):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2, 2)\n",
    "    output:\n",
    "        empty image with the rectangle as defined blotted out with values of 127 -- shape (112, 112)\n",
    "    '''\n",
    "    I_rect = np.zeros(I.shape)\n",
    "    I_rect = blot_out_given_rect(rectangle_corners, I_rect, replace_val=replace_val)\n",
    "    return I_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf97f40-2f0d-4087-96f8-de3a9856228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(rectangle_corners, I=None, show_applied=False):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        Well, technically rectangle corners could even have shape of (4, 2)\n",
    "    \n",
    "    plot a rectangle based on the coords given (in format of the skimage functions that get the corner pixels coords)\n",
    "    also if an image is given, plot the rectangle on top of it\n",
    "    '''\n",
    "   \n",
    "    # plot the rectangle\n",
    "    I_rect = blank_with_rect(rectangle_corners, I)\n",
    "    \n",
    "    # plot the original image if given\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if I is not None:\n",
    "        # assuming I has unique values (0,1)\n",
    "        plt.imshow(I * 255, cmap='gray', zorder=1)\n",
    "        plt.imshow(I_rect, cmap='gray', zorder=2, alpha=0.5)\n",
    "        # plt.colorbar()\n",
    "    else:\n",
    "        plt.imshow(I_rect, cmap='gray')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # show another figure of using the shown rectangle to zero out original image if asked\n",
    "    if show_applied:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(blot_out_given_rect(rectangle_corners, I), cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7716313d-ac5e-4a03-a995-41d626b12da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_corner_pixels_detected(I, show_plot=True, return_vars=False):\n",
    "    '''\n",
    "    input: I (112, 112), unique vals of 0,1\n",
    "    output (if requested):\n",
    "        coords - numpy ndarray\n",
    "        coords_subpix - nump ndarray\n",
    "            shapes of 2 objects above may vary...\n",
    "    \n",
    "    Prints the matplotlib figure with the corner pixels detected attached to the image\n",
    "    \n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)\n",
    "    coords_subpix = corner_subpix(I, coords, window_size=13)\n",
    "\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        ax.imshow(I, cmap=plt.cm.gray)\n",
    "        ax.plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "                linestyle='None', markersize=6)\n",
    "        ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)\n",
    "        plt.show()\n",
    "    \n",
    "    # return quantities if requested\n",
    "    if return_vars:\n",
    "        return coords, coords_subpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e8f0a0-fa18-4ee8-90aa-8cafea024594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangle_corners(I):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (112, 112)\n",
    "    output:\n",
    "        rectange_corners - shape (2,2)\n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # get corner pixels to form rectangle from\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)    \n",
    "    \n",
    "    # descending order sort all possible values along axis=0 for sorting by i / row index\n",
    "    coords[::-1].sort(axis=0)\n",
    "    \n",
    "    rectangle_corners = coords[0:2]\n",
    "\n",
    "    return rectangle_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fb1c48-7d87-42e1-84e0-6dc3eeb0f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_basal_plane_out(I):\n",
    "    '''\n",
    "    input: I (112, 112) expected to have unique values of (0,1)...1 for lv, 0 for not lv\n",
    "    output: I_new (112, 112) with unique values of (0,1)... 1 for lv, 0 for not lv...this has the basal plane removed\n",
    "    \n",
    "    https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py\n",
    "    \n",
    "    When choosing which 2 points to create a rough rectangle to zero out the bottom mitral valve / basal plane boundary\n",
    "    we will go with this ordering of points to use:\n",
    "    1. Both points from corner_subpix\n",
    "    2. One point from corner_subpix, one point from corner_peaks\n",
    "    3. Both points from corner_peaks\n",
    "    \n",
    "    If we can't get two points at the area of the basal plane from any of the three situations described above, we are out of luck. \n",
    "    We will have to do something else, but this won't work then. \n",
    "    '''  \n",
    "    # # make sure values in image are only 0 and 1\n",
    "    # check_unique_vals = np.unique(I)\n",
    "    # if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "    #     I = I / 255\n",
    "    # elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "    #     print('incorrect values in Image')\n",
    "    #     return\n",
    "    \n",
    "    rectangle_corners = get_rectangle_corners(I)\n",
    "    return blot_out_given_rect(rectangle_corners, I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f4a9df-3ef5-4223-922a-2c098264c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# taken from stough's dip vis_utils.py\n",
    "def vis_pair(I, J, figsize = (8,3), shared = True, \n",
    "             first_title = 'Original', second_title = 'New',\n",
    "             show_ticks = True, **kwargs):\n",
    "    '''\n",
    "    vis_pair(I, J, figsize = (8,3), shared = True, first_title = 'Original', second_title = 'New'):\n",
    "    produce a plot of images I and J together. By default takes care of sharing axes to provide\n",
    "    a little 1x2 plot without all the coding.\n",
    "    '''\n",
    "    f, ax = plt.subplots(1,2, figsize=figsize, sharex = shared, sharey = shared)\n",
    "    ax[0].imshow(I, **kwargs)\n",
    "    ax[0].set_title(first_title)\n",
    "    ax[1].imshow(J, **kwargs)\n",
    "    ax[1].set_title(second_title)\n",
    "    \n",
    "    if not show_ticks:\n",
    "        [a.axes.get_xaxis().set_visible(False) for a in ax];\n",
    "        [a.axes.get_yaxis().set_visible(False) for a in ax];\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460a20e-ee31-40c1-91f5-7b90ad9c16e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05e7833-42e7-4956-80b5-6035695aa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_and_warp_data(model, test_dataset, test_pat_index):\n",
    "    '''\n",
    "    input: \n",
    "         model, dataset, and video index in dataset\n",
    "    output:\n",
    "        segmentation and motion tracking info on a specified video\n",
    "        delta_ed_es = index difference between the ed and es clip\n",
    "        returned seg/mot information is for a 32 frame clip where ed is at index 0\n",
    "        clip_index - index of clip \n",
    "    '''\n",
    "    ########################### Helper functions ###########################\n",
    "    # goes thru a video and annotates where we can start clips given video length, clip length, etc.\n",
    "    def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "        assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "        possible_shift = clip_length - (es_index - ed_index)\n",
    "        allowed_right = video_length - es_index\n",
    "        if allowed_right < possible_shift:\n",
    "            return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "        if possible_shift < 0:\n",
    "            return np.array([ed_index])\n",
    "        elif ed_index < possible_shift:\n",
    "            return np.arange(ed_index + 1)\n",
    "        else:\n",
    "            return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "        \n",
    "    def generate_2dmotion_field_PLAY(x, offset):\n",
    "        # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "        # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "        x_shape = x.shape\n",
    "        # print(f'x_shape: {x_shape}')\n",
    "\n",
    "        grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "\n",
    "        # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "        # floating point precision\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_h = grid_h.cuda().float()\n",
    "\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "\n",
    "        # OLD \n",
    "        # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "        # NEW , TESTING\n",
    "        offset_h, offset_w = torch.split(offset, 1)\n",
    "\n",
    "        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "\n",
    "        offset_w = grid_w + offset_w\n",
    "        offset_h = grid_h + offset_h\n",
    "\n",
    "        offsets = torch.stack((offset_h, offset_w), 3)\n",
    "        return offsets\n",
    "\n",
    "    def categorical_dice(prediction, truth, k, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "            Compute the dice overlap between the predicted labels and truth\n",
    "            Not a loss\n",
    "        \"\"\"\n",
    "        # Dice overlap metric for label value k\n",
    "        A = (prediction == k)\n",
    "        B = (truth == k)\n",
    "        return 2 * np.sum(A * B) / (np.sum(A) + np.sum(B) + epsilon)\n",
    "    \n",
    "    ########################################################################\n",
    "    # initial grabbing of the data that we'll use\n",
    "    video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "    \n",
    "    # get all possible start indices for 32 frame clip with ed/es in right order\n",
    "    possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "    \n",
    "    # for now, let's use the last clip from our set of all possible clips to use\n",
    "    clip_index = len(possible_starts) - 1\n",
    "    # print(f'clip_index: {clip_index}')\n",
    "\n",
    "    # get the diff in frame len from ed to es\n",
    "    delta_ed_es = es_index - ed_index\n",
    "    \n",
    "    # use model to segment frames\n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "\n",
    "    # grab whatever clip we want\n",
    "    curr_clip_segmentations = segmentation_outputs[clip_index]\n",
    "    curr_clip_motions = motion_outputs[clip_index]\n",
    "    \n",
    "    return curr_clip_segmentations, curr_clip_motions, delta_ed_es, clip_index, ed_label, es_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "545258c3-f611-49eb-8fd5-9f0880509982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(I):\n",
    "    '''\n",
    "    input: I - shape (2, 112, 112)\n",
    "    convert a raw segmentation output into a one_hot_encoded image\n",
    "    '''\n",
    "    import copy\n",
    "    I_copy = copy.deepcopy(I)\n",
    "    return np.argmax(I_copy, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e43b0e2-c641-4c49-adf8-546104c548aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dice(prediction, truth):\n",
    "    '''\n",
    "    assume both images are one_hot_encoded\n",
    "    '''\n",
    "    k = 1\n",
    "    return categorical_dice(prediction, truth, k, epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8453145-f2c7-4993-8f42-f3f03ae11271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_forward(I, motions, delta_ed_es, clip_index):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (1, 2, 112, 112), not one-hot encoded, must be the raw model segmentation output\n",
    "        motions - shape ()\n",
    "        delta_ed_es - integer defining how many forward iterations to take (max 31)\n",
    "                        we are only interested in warping to/from ED/ES\n",
    "    output:\n",
    "        I_1 - shape (1, 2, 112, 112), not one-hot encoded, raw ES image, if want one-hot encoded need to apply np.argmax\n",
    "        \n",
    "    for now, try to do things all on the cpu\n",
    "    '''\n",
    "    def generate_2dmotion_field_custom(x, offset):\n",
    "        # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "        # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "        x_shape = x.shape\n",
    "        # print(f'x_shape: {x_shape}')\n",
    "\n",
    "        grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "\n",
    "        # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "        # floating point precision\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_h = grid_h.cuda().float()\n",
    "\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "\n",
    "        # OLD \n",
    "        # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "        # NEW , TESTING\n",
    "        offset_h, offset_w = torch.split(offset, 1)\n",
    "\n",
    "        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "\n",
    "        offset_w = grid_w + offset_w\n",
    "        offset_h = grid_h + offset_h\n",
    "\n",
    "        offsets = torch.stack((offset_h, offset_w), 3)\n",
    "        return offsets\n",
    "    \n",
    "    # convert numpy ndarrays into tensor objects moved onto device\n",
    "    flow_source = torch.from_numpy(I).to(device).float()\n",
    "    motions = torch.from_numpy(motions).to(device).float()\n",
    "\n",
    "    # warping FORWARD from ED -> ES\n",
    "    # python range is [x, y), inclusive start and exclusive end\n",
    "    for frame_index in range(31 - delta_ed_es - clip_index, 31 - clip_index + 1, 1):\n",
    "        # grab forward motion\n",
    "        forward_motion = motions[:2, frame_index,...]\n",
    "\n",
    "        # # grab the ED seg out frame to warp\n",
    "        # if frame_index == 0:\n",
    "        #     # flow_source = np.array([curr_clip_segmentations[:, frame_index, ...]])\n",
    "        #     flow_source = I\n",
    "        #     print(f'type(flow_source): {type(flow_source)}')\n",
    "        #     # flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "        # else:\n",
    "        #     pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "\n",
    "        # convert to tensors and move to gpu with float dtype\n",
    "        # forward_motion = torch.from_numpy(forward_motion).to(device).float()\n",
    "        # generate motion field for forward motion\n",
    "        motion_field = generate_2dmotion_field_custom(flow_source, forward_motion)\n",
    "        # create frame i+1 relative to curr frame i \n",
    "        next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "        # use i+1 frame as next loop iter's i frame\n",
    "        flow_source = next_label\n",
    "\n",
    "    flow_source = flow_source.cpu().detach().numpy()\n",
    "    \n",
    "    return flow_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed031a52-2c7b-482f-a85b-48a60490c6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191dbe4f-e212-4a64-9bea-7b43a6d6c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf609060-017b-409d-94ae-a68f42966df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_clip_segmentations, curr_clip_motions, delta_ed_es, clip_index, ed_label, es_label = get_seg_and_warp_data(model = model,\n",
    "                                                                                            test_dataset = test_dataset,\n",
    "                                                                                            test_pat_index = test_pat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ed78d46-be82-4068-9b0b-f5e6b7fd4dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32, 112, 112)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2768c986-253b-450d-b449-6d5a079f61d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 112, 112)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_segmentations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641dc22b-1a60-43f4-beee-431def36919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_raw_seg_out = curr_clip_segmentations[:,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f13cc8a-f8a8-4390-96e1-029031d9acb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 112, 112)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_raw_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918ad97e-97a8-4893-b2ec-31c39c3601ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_, height, width = ed_raw_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba5e159e-ea6a-485e-84ab-eebd6ae14db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.86601543, -9.85149574, -9.79540539, ..., 10.27981377,\n",
       "       10.39611626, 10.40559196])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ed_raw_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66116236-5562-4fdc-b3ae-42a2ffd3e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_one_hot_seg_out = one_hot(ed_raw_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a43b8ca-43cf-4a32-a89d-8294ff840d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_one_hot_seg_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e326e50a-f60e-4420-832a-3e9d4992eccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ed_one_hot_seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7c1f199-fefa-4e8a-bcbe-8c226fd5b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9661)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(ed_one_hot_seg_out, ed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10a0a370-c564-4d49-8527-1c3b26e3b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ed_raw_seg_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "380935c4-7d30-443f-b78b-76c592d4afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 112, 112)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5246b76-8c4c-4e89-a837-8e0cd4b83af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_warped = warp_forward(foo, curr_clip_motions, delta_ed_es, clip_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dac92b2-6410-413e-bf85-77a0c41f8d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 112, 112)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_warped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b6a9314-4a26-4645-9544-ca444ca3fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_warped_vis = one_hot(I_warped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e25eb65f-a905-4ddb-9ad2-b4772d79aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_warped_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e3aea2a-6004-49e4-8a39-2590573700ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9277)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(I_warped_vis, es_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9ec25f9-583a-4af7-8ba6-4732f72ada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_bottom_index(I):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (112, 112), unique vals (0,1)\n",
    "    output:\n",
    "        top, bot -- index vals of highest pixel with val 1 and lowest pixel with val 1\n",
    "                    i.e. range of the heart\n",
    "                    \n",
    "                    top inclusive\n",
    "                    bot exclusive\n",
    "                        Dijkstra would like me decision ;)\n",
    "    '''\n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "    \n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    delta = 0\n",
    "    \n",
    "    found_top = False\n",
    "    found_bottom = False\n",
    "    \n",
    "    for row in I:\n",
    "        # find the top\n",
    "        if not found_top:\n",
    "            if 1 in row:\n",
    "                found_top = True\n",
    "                top = delta\n",
    "            \n",
    "        # find bottom\n",
    "        if not found_bottom and found_top:\n",
    "            if 1 not in row:\n",
    "                found_bottom = True\n",
    "                bottom = delta\n",
    "                \n",
    "        # leave when found both\n",
    "        if found_top and found_bottom:\n",
    "            break\n",
    "            \n",
    "        # increment row index\n",
    "        delta += 1\n",
    "            \n",
    "    return top, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5a5d0ae-ee56-47fd-939a-a21b5c067882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_points(I, N, inds):\n",
    "    '''\n",
    "    input: \n",
    "        I - shape (112, 112), unique vals (0,1)\n",
    "        N - int\n",
    "        inds - list/iterable containing top and bottom index of heart (top, bot)\n",
    "    output:\n",
    "        list of N points where we divide the image\n",
    "    '''\n",
    "    top = inds[0]\n",
    "    bot = inds[1]\n",
    "    \n",
    "    lv_vert_len = bot - top\n",
    "    deltas = lv_vert_len // N\n",
    "    divide_points = [top + (deltas * i) for i in range(N)]\n",
    "    divide_points.append(bot)\n",
    "    \n",
    "    return divide_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acf072c5-d0dc-4e7f-8d4f-57726b489ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_strain_single_video(test_pat_index, test_dataset, model, N, vis=False):\n",
    "    '''\n",
    "    input:\n",
    "        test_pat_index = index of video to get data of\n",
    "        test_dataset = test dataset object\n",
    "        model = object loaded onto device\n",
    "        N = int\n",
    "    output:\n",
    "        list of N regions (from top to bottom) of local strain \n",
    "        NOTE: this is just completely horizontal cuttings!!!\n",
    "        \n",
    "        TODO: use 2d pucks with more accurate slicings\n",
    "    '''\n",
    "    curr_clip_segmentations, curr_clip_motions, delta_ed_es, clip_index, ed_label, es_label = get_seg_and_warp_data(model = model,\n",
    "                                                                                            test_dataset = test_dataset,\n",
    "                                                                                            test_pat_index = test_pat_index)\n",
    "    \n",
    "    ed_raw_seg_out = curr_clip_segmentations[:,0,...]\n",
    "    \n",
    "    ed_one_hot_seg_out = one_hot(ed_raw_seg_out)\n",
    "    \n",
    "    split_points = get_split_points(ed_one_hot_seg_out, N, top_bottom_index(ed_one_hot_seg_out))\n",
    "    \n",
    "    all_strains = [0 for i in range(N)]\n",
    "    \n",
    "    I = ed_one_hot_seg_out\n",
    "    \n",
    "    # convert to boundary\n",
    "    I = cut_basal_plane_out(give_boundary(I))\n",
    "    \n",
    "    import copy\n",
    "    for i in range(N):\n",
    "        I_copy = copy.deepcopy(I)\n",
    "        \n",
    "        start = split_points[i]\n",
    "        end = split_points[i+1]\n",
    "        \n",
    "        I_copy[0:start, 0:I_copy.shape[1]] = 0\n",
    "        I_copy[end:I_copy.shape[0], 0:I_copy.shape[1]] = 0\n",
    "        \n",
    "        # need to use the raw data, but need to also be boundary, no basal plate, and be the same\n",
    "        # pixels described in the I_copy\n",
    "        \n",
    "        print(f'I_copy unique: {np.unique(I_copy)}')\n",
    "        print(f'I_copy num pixels 0: {np.count_nonzero(I_copy == 0)}')\n",
    "        \n",
    "        \n",
    "        pixels_changed = 0\n",
    "        \n",
    "        z_, height, width = ed_raw_seg_out.shape\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if I_copy[i][j] == 0:\n",
    "                    # the functions don't like me zeroing things out, apparently.\n",
    "                    # I need to alter the values in a different way (alter the LV weight to be small, to make model think it is not lv\n",
    "                    # so that we discard this pixel data point)\n",
    "                    \n",
    "                    # ed_raw_seg_out[:, i, j] = 1\n",
    "                    \n",
    "                    # i want this to be zeroed out, I want the first dim val to be larger than the second dim val on this axis\n",
    "                    \n",
    "                    ed_raw_seg_out[0][i][j] = 9.99\n",
    "                    ed_raw_seg_out[1][i][j] = -9.99\n",
    "                    pixels_changed += 1\n",
    "                    \n",
    "#         ed_raw_seg_out_one_hot = one_hot(ed_raw_seg_out)\n",
    "        \n",
    "#         vis_pair(I_copy, ed_raw_seg_out_one_hot)\n",
    "        \n",
    "#         return\n",
    "    \n",
    "        '''ok till this point '''\n",
    "\n",
    "        # ed_raw_seg_out[0][0:112][0:112] = 10\n",
    "    \n",
    "        # print(f'ed_raw_seg_out[0][0][0]: {ed_raw_seg_out[0][0][0]}')\n",
    "        # print(f'ed_raw_seg_out[1][0][0]: {ed_raw_seg_out[1][0][0]}')\n",
    "        \n",
    "        \n",
    "        print(f'pixels_changed: {pixels_changed}')\n",
    "        # print(f'percentage of picture zeroed out: {(pixels_changed / (height * width)) * 100}%')\n",
    "        print(f'ed_raw_seg_out unique: {np.unique(ed_raw_seg_out)}')\n",
    "        \n",
    "        \n",
    "        ''' this is where problems happen '''\n",
    "        tmp = np.array([ed_raw_seg_out]) # make it shape of (1, 2, 112, 112)\n",
    "        \n",
    "        I_copy_warped = warp_forward(tmp, curr_clip_motions, delta_ed_es, clip_index)\n",
    "        \n",
    "        I_copy_warped_one_hot = one_hot(I_copy_warped[0])\n",
    "        \n",
    "        if vis:\n",
    "            vis_pair(I_copy, I_copy_warped_one_hot, first_title = 'ED frame', second_title = 'Warped to ES frame')\n",
    "\n",
    "        ''' the below will be ok if the above warping is ok '''\n",
    "        \n",
    "        print(f'I_copy.shape: {I_copy.shape}')\n",
    "        print(f'I_copy_warped_one_hot.shape: {I_copy_warped_one_hot.shape}')\n",
    "        print(f'I_copy unique: {np.unique(I_copy)}')\n",
    "        \n",
    "        print(f'I_copy_warped_one_hot unique: {np.unique(I_copy_warped_one_hot)}')\n",
    "        print(f'I_copy_warped unique: {np.unique(I_copy_warped)}')\n",
    "        \n",
    "        print(f'I_copy_warped num 0 val: {np.count_nonzero(I_copy_warped == 0.0)}')\n",
    "        print(f'ed_raw_seg_out num 0 val: {np.count_nonzero(ed_raw_seg_out == 0.0)}')\n",
    "\n",
    "        \n",
    "        return\n",
    "        \n",
    "        curr_sec_strain = boundaries_to_strain(I_copy, I_copy_warped_one_hot)\n",
    "        all_strains[i] = curr_sec_strain\n",
    "        \n",
    "        \n",
    "    return all_strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "434f3b1e-490e-4190-b661-98c539f8ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_copy unique: [0. 1.]\n",
      "I_copy num pixels 0: 12499\n",
      "pixels_changed: 12499\n",
      "ed_raw_seg_out unique: [-9.99       -3.12754202 -2.76959348 -2.54523849 -2.43617105 -2.37414813\n",
      " -2.21390605 -2.11287451 -2.06149054 -1.9575901  -1.9334358  -1.88733399\n",
      " -1.84932184 -1.74173284 -1.73475623 -1.67011571 -1.60161185 -1.58858895\n",
      " -1.58683014 -1.58186853 -1.57867444 -1.50172973 -1.4711473  -1.39731765\n",
      " -1.30360687 -1.2982012  -1.25389254 -1.24554121 -1.21684885 -1.18388033\n",
      " -1.17881215 -1.13096249 -1.11057115 -0.94567835 -0.92177999 -0.81043351\n",
      " -0.74247271 -0.64227253 -0.58567429 -0.56073481 -0.4513486  -0.43028235\n",
      " -0.40762624 -0.27291748 -0.19071108 -0.08442135  0.02336665  0.03015753\n",
      "  0.10149201  0.40462983  0.41351503  0.47514153  0.52568591  0.53895038\n",
      "  0.60555559  0.63200939  0.6812191   0.91243201  0.9130494   1.03357649\n",
      "  1.063936    1.08583117  1.08745134  1.12608027  1.17585969  1.1950295\n",
      "  1.21885252  1.2376771   1.24860311  1.42303157  1.43634725  1.43803728\n",
      "  1.45107508  1.53271043  1.54005039  1.56585193  1.61043417  1.66097128\n",
      "  1.69552517  1.77239537  1.82003856  1.85374773  1.93857729  1.96561921\n",
      "  2.08474922  2.12798023  2.30182862  2.39325237  2.48222518  2.76033664\n",
      "  3.11926079  9.99      ]\n",
      "I_copy.shape: (112, 112)\n",
      "I_copy_warped_one_hot.shape: (112, 112)\n",
      "I_copy unique: [0. 1.]\n",
      "I_copy_warped_one_hot unique: [0]\n",
      "I_copy_warped unique: [-9.990003 -9.990002 -9.990001 -9.99     -9.989999 -9.989998 -9.989997\n",
      "  9.989997  9.989998  9.989999  9.99      9.990001  9.990002  9.990003]\n",
      "I_copy_warped num 0 val: 0\n",
      "ed_raw_seg_out num 0 val: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ff44fdafa04de18ee3b85820043a05",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=800.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_strains = get_local_strain_single_video(test_pat_index = 0, \n",
    "                                            test_dataset = test_dataset, \n",
    "                                            model = model, \n",
    "                                            N = 3, \n",
    "                                            vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "175ebb94-e1b3-4998-8988-250ef67f9501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 112, 112)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_segmentations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "040157c6-c9ad-4f68-b692-df480436e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.422019958496094, -9.938758850097656)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_segmentations.max(), curr_clip_segmentations.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "069a6e04-caac-45b4-a4cf-5e48b0f9375d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32, 112, 112)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6421ef1d-9dcc-4de7-aac7-f85e095a4c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8517086505889893, -0.7713068127632141)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_motions.max(), curr_clip_motions.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b916712-a22e-4b60-ac12-5da0235231a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9654039740562439, -1.0369848012924194)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_segmentations[0][0][23][47], curr_clip_segmentations[1][0][23][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67119519-4d6b-40b3-a6a1-d6f0fa0e3dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05518908053636551"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_clip_motions[0][0][23][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8166e6a-75c1-4cb6-89e8-38da12a68e17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-4cb4dfda6616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mI_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m47\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'I_copy' is not defined"
     ]
    }
   ],
   "source": [
    "I_copy[23][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98c9ccb8-e852-4bb7-82c3-fd39f4c4e9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9654039740562439, -1.0369848012924194)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_raw_seg_out[0][23][47], ed_raw_seg_out[1][23][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00290463-2d9a-4952-8fb0-b322bb163f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-a2158cc6696c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mI_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'I_copy' is not defined"
     ]
    }
   ],
   "source": [
    "I_copy[23][48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18a3a6-9d78-44c6-9c79-049fe5352be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_raw_seg_out[0][23][48], ed_raw_seg_out[1][23][48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b0252-7fc2-4970-9ffc-22db413a8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(I_copy), I_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09eebbb-bd3e-452a-ae01-e44f1d2a8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ed_raw_seg_out), ed_raw_seg_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0179e9f-72a2-44d0-8c61-0b2f7e1abff7",
   "metadata": {},
   "source": [
    "## This proves that the logic to cookie cutter our desired part of the lv boundary in the ed raw seg out is correct. However the motion tracking / motion field warping is having some problem with the values that we are adjusting, I believe.\n",
    "## This appears to be ok because after one-hot encoding we can visualize it pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a20aa-5eb4-47d9-80b0-4e336eccacfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09230cac-d64e-4e00-9656-486619362a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-181ea9bfdc44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mI_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0med_raw_seg_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'I_copy' is not defined"
     ]
    }
   ],
   "source": [
    "z_, height, width = ed_raw_seg_out.shape\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        if I_copy[i][j] == 0:\n",
    "            ed_raw_seg_out[:, i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cdaaa4f-87df-4625-867e-0e5f84032efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-40920fac79bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0med_raw_seg_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'I_copy' is not defined"
     ]
    }
   ],
   "source": [
    "vis_pair(I_copy, one_hot(ed_raw_seg_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01ed0c-a575-48a9-be65-1cc8b6fe7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strains = get_local_strain_single_video(test_pat_index = 0, \n",
    "                                            test_dataset = test_dataset, \n",
    "                                            model = model, \n",
    "                                            N = 3, \n",
    "                                            vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6b950-69d0-4ab7-8937-a358aa12def7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4415e0-ee44-49a8-b3d8-4afbf073bee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07762a-8d95-47e2-a4a6-be2b79de7f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1531138-6921-4c6b-b6d3-c9d1bb942d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
