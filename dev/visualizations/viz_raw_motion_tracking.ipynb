{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49815440-3d73-4f73-8ad5-223e2cb9acf8",
   "metadata": {},
   "source": [
    "# Try to visualize how the warping of the input image occurs via the motion field output of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d15a2-09be-4f05-ae5f-8a97d35cd5c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First, let's just do some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd9ced6-d32f-4041-bf76-f5f1ba412e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\"Original-Pretrained-R2plus1DMotionSegNet_model.pth\", \"dropout-0_10-R2plus1DMotionSegNet_model.pth\", \"dropout-0_25-R2plus1DMotionSegNet_model.pth\", \"dropout-0_50-R2plus1DMotionSegNet_model.pth\", \"dropout-0_75-R2plus1DMotionSegNet_model.pth\"]\n",
    "model_names = [\"Original_Pretrained_R2plus1DMotionSegNet_model.pth\", \"dropout_v3_0_10_R2plus1D_18_MotionNet.pth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de29739-5339-4657-a841-0e5d06d7ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet # original model\n",
    "# new models (small alterations)\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet \n",
    "\n",
    "\n",
    "from src.echonet_dataset import EchoNetDynamicDataset\n",
    "from src.clasfv_losses import deformation_motion_loss, motion_seg_loss, DiceLoss, categorical_dice\n",
    "from src.train_test import train, test\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# for slider visualizations\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import correlate\n",
    "from skimage.filters import *\n",
    "\n",
    "from ipywidgets import VBox, IntSlider, AppLayout\n",
    "# initialize to use dark background ...\n",
    "plt.style.use('dark_background')\n",
    "######\n",
    "# for creating gif animation and viewing them\n",
    "from matplotlib import animation\n",
    "from IPython.display import Image\n",
    "\n",
    "######\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be8146-68fe-4c89-bb8f-750665870698",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in models\n",
    "\n",
    "We have the luxury of trying inference on multiple models, since I'm trying to create slightly different models.\n",
    "\n",
    "Original pre-trained model by Yida will be `model_name_1`.\n",
    "The one that I will add dropout and k-fold cross validation to will be `model_name_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03ef121-c2f8-445a-9322-d6bd3c40c116",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20392/2385903995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# hold tuples of (name, model object)\n",
    "loaded_in_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_save_path = f\"save_models/{model_name}\"\n",
    "    \n",
    "    # original model\n",
    "    if model_name == \"Original-Pretrained-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(R2plus1D_18_MotionNet())\n",
    "         model = torch.nn.DataParallel(R2plus1D_18_MotionNet())\n",
    "        \n",
    "    # altered models\n",
    "    if model_name == \"dropout-0_75-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_75_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_75_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_50-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_50_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_50_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_25-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_25_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_25_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_10-R2plus1DMotionSegNet_model.pth\":\n",
    "        # model = DDP(dropout_0_10_R2plus1D_18_MotionNet())\n",
    "        model = torch.nn.DataParallel(dropout_0_10_R2plus1D_18_MotionNet())\n",
    "    \n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8fa20-af9b-4aba-a191-84bb7ca05216",
   "metadata": {},
   "source": [
    "## Load in Testing Dataset to do inference on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ade12-30c8-4059-ad1c-293534105673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# batch_size = 4\n",
    "# num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "# def worker_init_fn_valid(worker_id):                                                          \n",
    "#     np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "# test_dataloader = DataLoader(valid_dataset, \n",
    "#                               batch_size=batch_size, \n",
    "#                               num_workers=num_workers,\n",
    "#                               shuffle=False, \n",
    "#                               pin_memory=(\"cuda\"),\n",
    "#                               worker_init_fn=worker_init_fn_valid\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a566e5-4e99-4628-aa34-7267678cb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10143be6-2fa5-4968-be70-c43ccc89540e",
   "metadata": {},
   "source": [
    "### Grab a video to look at, can be random or manually choose one of the 1276 from test dataset\n",
    "### For sake of comparison, let's look at first sample from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecf0a3-b5b8-4e85-b506-113662d11919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pat_index = np.random.randint(len(test_dataset))\n",
    "test_pat_index = 0 \n",
    "\n",
    "video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2c9e9-8e75-4fe2-9b31-7d2522d87123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(video))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbb8bd-e2f9-407b-90ad-1d995cb4afa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get all possible 32-Frame Clips that covers ED-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3628c-8abb-4fae-bd6a-d0827a5af4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "# goes thru a video and annotates where we can start clips given video length, cli length, etc.\n",
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb4a75-331e-49f2-a708-16352aa893ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "print(len(possible_starts))\n",
    "print(possible_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcd4b3-ad2d-4e48-9e12-480dd2342ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f53e7-a47f-411b-8981-ab2056800f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff2f04-22d9-4e9f-a5b9-aef7c5b2a2a5",
   "metadata": {},
   "source": [
    "### Segment All 32-Frame Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d725dc3-9677-4836-bfa5-d9a342aebbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment using all models\n",
    "all_segmentation_outputs = []\n",
    "all_motion_outputs = []\n",
    "\n",
    "# for each model, segment the clips\n",
    "for name, model in loaded_in_models:\n",
    "    \n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "    \n",
    "    # save \n",
    "    all_segmentation_outputs.append(segmentation_outputs)\n",
    "    all_motion_outputs.append(motion_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82bf75-823a-46e6-96d6-bdc33ce32398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_segmentation_outputs), len(all_motion_outputs))\n",
    "print(len(all_segmentation_outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fb72c-2e79-41de-9a67-bd0156b2cad2",
   "metadata": {},
   "source": [
    "## Know that our shapes are: [Forward x, y, backward x, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8c36e-6445-4f8e-b392-6256a7d3a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_outputs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721728b2-9978-4e0c-bdce-411497dbec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_outputs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86b383-a06c-4c6b-8707-7ec4142d1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 32 frame segmented clip\n",
    "(motion_outputs_1[-1].shape, motion_outputs_1[-1].min(), motion_outputs_1[-1].max(), motion_outputs_1[-1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b79bd-8286-4bfa-92a0-f0eb88b9ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_outputs_1[-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778c94d-9c72-441b-b320-17c74c322a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_outputs_1[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4695ad9-df2e-42f0-b8ab-0c114501b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = motion_outputs_1[-1][0][0]\n",
    "tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9cd09-671b-4a0c-ada3-3760426f52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.min(), tmp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea1796-2134-4ecf-9453-c3ef358ba0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_outputs_1[-1][0][0].min(), motion_outputs_1[-1][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbd56e-836a-4629-9b68-3a5abc672757",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = (motion_outputs_1[-1][0][0].max() + motion_outputs_1[-1][0][0].min()) / 2\n",
    "print(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4c4aa-70de-4c6e-869b-7dfc3512b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_mid(x):\n",
    "#     '''Expects a single image, calculates the mid point value of this single image\n",
    "#     '''\n",
    "#     return (x.min() + x.max()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a7ca0-bc40-4e53-a171-9289523b6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(x):\n",
    "#     ''' normalizes the thing passed in, assumes input is a ndarray numpy object'''\n",
    "#     return (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba30119-195f-43c0-8447-64ac867636a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "frame_index = 0   # first frame\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf43a6-d9d4-4155-8ac8-9fa7dd16137d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot frame 0 of all 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66af906-060a-4fa3-ba4f-86ca0ca22921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot all 4, forward x,y and backward x, y\n",
    "# make sure to normalize.\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(10,5));\n",
    "plt.suptitle(model_name_1)\n",
    "ax1.set_title(\"forward x\");\n",
    "fx = motion_outputs_1[clip_index][which_direction[0]][frame_index]\n",
    "ax1_img = ax1.imshow(fx, cmap=\"viridis\");\n",
    "fig.colorbar(ax1_img, ax=ax1)\n",
    "\n",
    "ax2.set_title(\"forward y\")\n",
    "fy = motion_outputs_1[clip_index][which_direction[1]][frame_index]\n",
    "ax2_img = ax2.imshow(fy, cmap=\"viridis\");\n",
    "fig.colorbar(ax2_img, ax=ax2)\n",
    "\n",
    "ax3.set_title(\"backward x\")\n",
    "bx = motion_outputs_1[clip_index][which_direction[2]][frame_index]\n",
    "ax3_img = ax3.imshow(bx, cmap=\"viridis\");\n",
    "fig.colorbar(ax3_img, ax=ax3)\n",
    "\n",
    "ax4.set_title(\"backward y\")\n",
    "by = motion_outputs_1[clip_index][which_direction[3]][frame_index]\n",
    "ax4_img = ax4.imshow(by, cmap=\"viridis\");\n",
    "fig.colorbar(ax4_img, ax=ax4)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce16020-3347-45c7-b207-95a4a9eb6484",
   "metadata": {},
   "source": [
    "## That's not good, as seen by the colorbars that the same colors across images do not mean the same number. Let's fix that using `vmin` and `vmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53497e-49ea-4415-931e-bb2c7ad8b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "frame_index = 0   # first frame\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "# find the absolute max and min values of the 4 motion fields for this singular frame and use that for min and max\n",
    "all_mins = [all_motion_outputs[0][clip_index][which_direction[i]][frame_index].min() for i in range(4)]\n",
    "all_maxes = [all_motion_outputs[0][clip_index][which_direction[i]][frame_index].max() for i in range(4)]\n",
    "color_min = min(all_mins)\n",
    "color_max = max(all_maxes)\n",
    "print(color_min, color_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d082f09-6090-4083-9139-7446a8a52bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all 4, forward x,y and backward x, y\n",
    "# make sure to normalize.\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,5));\n",
    "\n",
    "ax1.set_title(\"forward x\");\n",
    "fx = all_motion_outputs[0][clip_index][which_direction[0]][frame_index]\n",
    "ax1_img = ax1.imshow(fx, cmap=\"viridis\", vmin=color_min, vmax=color_max);\n",
    "# fig.colorbar(ax1_img, ax=ax1)\n",
    "\n",
    "ax2.set_title(\"forward y\")\n",
    "fy = all_motion_outputs[0][clip_index][which_direction[1]][frame_index]\n",
    "ax2_img = ax2.imshow(fy, cmap=\"viridis\", vmin=color_min, vmax=color_max);\n",
    "# fig.colorbar(ax2_img, ax=ax2)\n",
    "\n",
    "ax3.set_title(\"backward x\")\n",
    "bx = all_motion_outputs[0][clip_index][which_direction[2]][frame_index]\n",
    "ax3_img = ax3.imshow(bx, cmap=\"viridis\", vmin=color_min, vmax=color_max);\n",
    "# fig.colorbar(ax3_img, ax=ax3)\n",
    "\n",
    "ax4.set_title(\"backward y\")\n",
    "by = all_motion_outputs[0][clip_index][which_direction[3]][frame_index]\n",
    "ax4_img = ax4.imshow(by, cmap=\"viridis\", vmin=color_min, vmax=color_max);\n",
    "\n",
    "cbar_ax = fig.add_axes([0.92, 0.1, 0.01, 0.75])\n",
    "fig.colorbar(ax4_img, cax=cbar_ax)\n",
    "\n",
    "# fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ced89-8095-4d6f-8e29-20a6e4cc37e3",
   "metadata": {},
   "source": [
    "## Look at all frames in our specific clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49cebf-8821-43f7-9ac7-77bcbacbab4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Create a slide object\n",
    "\n",
    "slider = IntSlider(\n",
    "    value=0, # start value\n",
    "    min=0,\n",
    "    max=31,\n",
    "    step=1,\n",
    "    description='Frame:',\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "\n",
    "slider.layout.margin = '0px 00% 0px 00%'\n",
    "slider.layout.width = '80%'\n",
    "\n",
    "#######################\n",
    "\n",
    "# calculate midpoints of all \n",
    "\n",
    "# plot all 4, forward x,y and backward x, y\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,5))\n",
    "ax1.set_title(\"forward x\");\n",
    "fx = motion_outputs_1[clip_index][which_direction[0]][frame_index]\n",
    "ax1.imshow(fx, cmap=\"viridis\");\n",
    "\n",
    "ax2.set_title(\"forward y\")\n",
    "fy = motion_outputs_1[clip_index][which_direction[1]][frame_index]\n",
    "ax2.imshow(fy, cmap=\"viridis\");\n",
    "\n",
    "ax3.set_title(\"backward x\")\n",
    "bx = motion_outputs_1[clip_index][which_direction[2]][frame_index]\n",
    "ax3.imshow(bx, cmap=\"viridis\");\n",
    "\n",
    "ax4.set_title(\"backward y\")\n",
    "by = motion_outputs_1[clip_index][which_direction[3]][frame_index]\n",
    "ax4.imshow(by, cmap=\"viridis\");\n",
    "\n",
    "######################\n",
    "\n",
    "# A function that will be called whenever the slider changes.\n",
    "def update_lines(change):\n",
    "    fx = motion_outputs_1[clip_index][which_direction[0]][slider.value]\n",
    "    ax1.imshow(fx, cmap=\"viridis\")\n",
    "    fy = motion_outputs_1[clip_index][which_direction[1]][slider.value]\n",
    "    ax2.imshow(fy, cmap=\"viridis\")\n",
    "    \n",
    "    bx = motion_outputs_1[clip_index][which_direction[2]][slider.value]\n",
    "    ax3.imshow(bx, cmap=\"viridis\")\n",
    "    by = motion_outputs_1[clip_index][which_direction[3]][slider.value]\n",
    "    ax4.imshow(by, cmap=\"viridis\")\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "# Connecting the slider object to the update function above.\n",
    "# This is event-handling.\n",
    "slider.observe(update_lines, names='value')\n",
    "\n",
    "# Creates an application interface with the various \n",
    "# pieces we already instantiated inside of it. \n",
    "AppLayout(\n",
    "    center=fig.canvas,\n",
    "    footer=slider,\n",
    "    pane_heights=[0, 6, 1]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb588723-3a34-44c9-abda-1c90ff9926c7",
   "metadata": {},
   "source": [
    "## Define the function to create our GIF Animations\n",
    "\n",
    "How to save animation as gif: http://louistiao.me/posts/notebooks/save-matplotlib-animations-as-gifs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d1140-f745-4b07-bcb8-ded16eefaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_32_frame_motion_colormap_gif(model_name, motion_output_obj, clip_index, which_direction, out_file_comment=\"\", cmap=\"viridis\"):\n",
    "    # plot all 4, forward x,y and backward x, y\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,5));\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    start_frame = 0\n",
    "    \n",
    "    # find the max and min value needed for colormapping for all 4 motion outputs across ALL \n",
    "    # 32 frames\n",
    "    all_mins = []\n",
    "    all_maxes = []\n",
    "    for frame_ind in range(32):\n",
    "        for direction_ind in range(4):\n",
    "            all_mins.append(motion_output_obj[clip_index][which_direction[direction_ind]][frame_ind].min())\n",
    "            all_maxes.append(motion_output_obj[clip_index][which_direction[direction_ind]][frame_ind].max())\n",
    "    \n",
    "    color_min = min(all_mins)\n",
    "    color_max = max(all_maxes)\n",
    "    \n",
    "    ax1.set_title(\"forward x\");\n",
    "    fx = motion_output_obj[clip_index][which_direction[0]][start_frame]\n",
    "    ax1_img = ax1.imshow(fx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "    fig.colorbar(ax1_img, ax=ax1) # show colorbar\n",
    "\n",
    "    ax2.set_title(\"forward y\")\n",
    "    fy = motion_output_obj[clip_index][which_direction[1]][start_frame]\n",
    "    ax2_img = ax2.imshow(fy, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "    fig.colorbar(ax2_img, ax=ax2)\n",
    "\n",
    "    ax3.set_title(\"backward x\")\n",
    "    bx = motion_output_obj[clip_index][which_direction[2]][start_frame]\n",
    "    ax3_img = ax3.imshow(bx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "    fig.colorbar(ax3_img, ax=ax3)\n",
    "\n",
    "    ax4.set_title(\"backward y\")\n",
    "    by = motion_output_obj[clip_index][which_direction[3]][start_frame]\n",
    "    ax4_img = ax4.imshow(by, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "    fig.colorbar(ax4_img, ax=ax4)\n",
    "    \n",
    "    # funct to update imshow with new frame of motion output\n",
    "    def animate(frame):\n",
    "        fx = motion_output_obj[clip_index][which_direction[0]][frame]\n",
    "        ax1_img = ax1.imshow(fx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax1_img, ax=ax1)\n",
    "        fy = motion_output_obj[clip_index][which_direction[1]][frame]\n",
    "        ax2_img = ax2.imshow(fy, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax2_img, ax=ax2)\n",
    "\n",
    "        bx = motion_output_obj[clip_index][which_direction[2]][frame]\n",
    "        ax3_img = ax3.imshow(bx, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax3_img, ax=ax3)\n",
    "        by = motion_output_obj[clip_index][which_direction[3]][frame]\n",
    "        ax4_img = ax4.imshow(by, cmap=cmap, vmin=color_min, vmax=color_max);\n",
    "        # fig.colorbar(ax4_img, ax=ax4)\n",
    "\n",
    "        return [ax1, ax2, ax3, ax4]\n",
    "    anim = animation.FuncAnimation(fig, animate, np.arange(0, 32), interval=500, blit=True); # interval is milliseconds between each redraw, adjusts animation speed\n",
    "    anim.save(f'./warren-random/visualization-outputs/{model_name}_motion_colormap_{out_file_comment}.gif', writer='imagemagick', fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bbd39-1ae2-4077-9b53-967880ee3e0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 1: Video animation of forward and backward x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815bc0a-950c-46f1-91f1-eed0874747ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "out_file_comment = \"3-colorbar-seismic\"\n",
    "color_mapping = \"viridis\"\n",
    "\n",
    "create_32_frame_motion_colormap_gif(model_name_1, motion_outputs_1, clip_index, which_direction, out_file_comment=out_file_comment, cmap=color_mapping);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b8946-cce0-4a0d-9bed-cb07cd2f82c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(f'./warren-random/visualization-outputs/{model_name_1}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd1bfb-84e4-43e2-b2aa-8c9bea9d05cc",
   "metadata": {},
   "source": [
    "## Model 2: Video animation of forward x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f9573-956d-4d1f-a6a0-2eb89265e0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "out_file_comment = \"1-colorbar-seismic\"\n",
    "color_mapping = \"seismic\"\n",
    "\n",
    "create_32_frame_motion_colormap_gif(model_name_2, motion_outputs_2, clip_index, which_direction, out_file_comment=out_file_comment, cmap=color_mapping);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855190ad-7bdd-4c27-bcf8-13375d2d9b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(f'./warren-random/visualization-outputs/{model_name_2}_motion_colormap_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e83714-6276-4e79-9fa3-3650ea2c6673",
   "metadata": {},
   "source": [
    "## Try to print motion output as vector fields\n",
    "\n",
    "### One field for Forward (x,y), and another one for Backward (x,y). \n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50537f1-fd47-45d6-b9a5-a99ce563f376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "frame_index = 0   # first frame\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76043487-4a6d-44e5-ac66-15556496c068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fx.shape, fy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64199512-ffa2-4dbe-aef3-1e11f7f34af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fx), len(fx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2f989-8e7e-46f0-b110-1bab1154be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e95140-1838-4440-9d4a-7c74110c5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to plot a single instance of the vector field from this \n",
    "# significant help from: https://stackoverflow.com/a/40370633\n",
    "\n",
    "# plt.style.use('seaborn-white')\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "## TODO: figure out how to display 4 of these: \n",
    "''' [ m1fxy, m1bxy ]\n",
    "    [ m2fxy, m1bxy ] \n",
    "'''\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "\n",
    "##### Model 1, Forward\n",
    "\n",
    "fx = motion_outputs_1[clip_index][which_direction[0]][frame_index]\n",
    "fy = motion_outputs_1[clip_index][which_direction[1]][frame_index]\n",
    "\n",
    "nrows, ncols = fx.shape\n",
    "x_tmp = np.linspace(0, 112, ncols)  \n",
    "y_tmp = np.linspace(0, 112, nrows)\n",
    "x_tails, y_tails = np.meshgrid(x_tmp, y_tmp, indexing='xy') ## Todo: what is a numpy meshgrid ?\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"Forward x,y for {model_name_1}\")\n",
    "plt.xlabel('$\\Delta x$')\n",
    "plt.ylabel('$\\Delta y$')\n",
    "ax = plt.gca(); ax.invert_yaxis()\n",
    "# plt.quiver(x_tails, y_tails, fx, fy, alpha=0.5)\n",
    "plt.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### TODO\n",
    "##### Model 1, Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57214b0c-8a21-42b0-a1cb-44849ff6ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_tails), len(y_tails))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5c139-6bd7-4be3-8ba0-5ea1930f6d13",
   "metadata": {},
   "source": [
    "## Now let's try to make an animation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cdf8a-8af5-4de7-9278-e2b6a818a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_32_frame_motion_vector_field_gifs(model_name, motion_output_obj, clip_index, which_direction, out_file_comment=\"\"):\n",
    "    # 2 subplots, forward and backward motion\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10));\n",
    "    \n",
    "    # starting frame ind\n",
    "    start_frame = 0\n",
    "    \n",
    "    # get initial forward and backward motion\n",
    "    fx = motion_output_obj[clip_index][which_direction[0]][start_frame]\n",
    "    fy = motion_output_obj[clip_index][which_direction[1]][start_frame]\n",
    "    \n",
    "    bx = motion_output_obj[clip_index][which_direction[2]][start_frame]\n",
    "    by = motion_output_obj[clip_index][which_direction[3]][start_frame]\n",
    "    \n",
    "    # create values for the tails of the vectors (these don't need to change)\n",
    "    nrows, ncols = fx.shape\n",
    "    x_tmp = np.linspace(0, 112, ncols)  \n",
    "    y_tmp = np.linspace(0, 112, nrows)\n",
    "    x_tails, y_tails = np.meshgrid(x_tmp, y_tmp, indexing='xy')\n",
    "    \n",
    "    # put titles, axes on subplots\n",
    "    plt.suptitle(f\"Motion Vector Field (x,y) for {model_name}\")\n",
    "    ax1.set_title('Forward Motion')\n",
    "    ax1.set_xlabel('$\\Delta x$')\n",
    "    ax1.set_ylabel('$\\Delta y$')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    ax2.set_title('Backward Motion')\n",
    "    ax2.set_xlabel('$\\Delta x$')\n",
    "    ax2.set_ylabel('$\\Delta y$')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # put initial magnitudes to the vectors with fixed tails\n",
    "    # forward\n",
    "    ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "    # backward\n",
    "    ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "    \n",
    "    # funct to update imshow with new frame of motion output\n",
    "    def animate(frame):\n",
    "        # update forward xy and backward xy\n",
    "        fx = motion_output_obj[clip_index][which_direction[0]][frame]\n",
    "        fy = motion_output_obj[clip_index][which_direction[1]][frame]\n",
    "\n",
    "        bx = motion_output_obj[clip_index][which_direction[2]][frame]\n",
    "        by = motion_output_obj[clip_index][which_direction[3]][frame]\n",
    "        \n",
    "        # clear forward and backward axes first (we'll need to reapply the labels and inverting the y axis)\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.set_title('Forward Motion')\n",
    "        ax1.set_xlabel('$\\Delta x$')\n",
    "        ax1.set_ylabel('$\\Delta y$')\n",
    "        ax1.invert_yaxis()\n",
    "\n",
    "        ax2.set_title('Backward Motion')\n",
    "        ax2.set_xlabel('$\\Delta x$')\n",
    "        ax2.set_ylabel('$\\Delta y$')\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        # update the magnitudes of the vectors with fixed tails\n",
    "        # forward\n",
    "        ax1.quiver(x_tails, y_tails, fx, fy, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "        # backward\n",
    "        ax2.quiver(x_tails, y_tails, bx, by, edgecolor='b', facecolor='none', linewidth=0.7);\n",
    "\n",
    "        \n",
    "    anim = animation.FuncAnimation(fig, animate, np.arange(0, 32), interval=500, blit=True); # interval is milliseconds between each redraw, adjusts animation speed\n",
    "    anim.save(f'./warren-random/visualization-outputs/{model_name}_motion_vector_field_{out_file_comment}.gif', writer='imagemagick', fps=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feacea3a-a97e-46c2-9794-c42b8cd9da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_index = -1   # last 32 frame clip\n",
    "which_direction = [0,1,2,3] # in order of: forward x,y, backward x,y \n",
    "\n",
    "out_file_comment = \"attempt3\"\n",
    "\n",
    "create_32_frame_motion_vector_field_gifs(model_name = model_name_1, \n",
    "                                         motion_output_obj = motion_outputs_1, \n",
    "                                         clip_index = clip_index, \n",
    "                                         which_direction = which_direction, \n",
    "                                         out_file_comment=out_file_comment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae3104-cb81-43e1-b2a7-7bf0288a7f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(f'./outputs/{model_name_1}_motion_vector_field_{out_file_comment}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee284f62-55d0-45e9-bc9b-abde21992bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
