{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f24b24-25ce-42f4-ba31-f49c598b90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer):\n",
    "    \"\"\" Training function for the network \"\"\"\n",
    "    # set the model to be in training mode, allow weights and biases of internal \n",
    "    # layers with tunable parmeters to be changed by the optimizer\n",
    "    model.train()\n",
    "    # list to store losses from this current epoch of training\n",
    "    epoch_loss = []\n",
    "    # dice is the percentage of overlap between two 2d sets, our images, comparing the segmentation out\n",
    "    # ED and ES frame to the labeled ED and ES frame from the dataset\n",
    "    # every batch has 4 videos, each video has a pair of ED/ES frames\n",
    "    # in every epoch we will loop thru the entire train split of the dataset, wrapped in the dataloader \n",
    "    # iterable. this outer variable must be declared here to store the mean of the individual ED/ES frame dice scores\n",
    "    # of all videos in the train split of the dataset during this epoch (epoch being the individual iteration over\n",
    "    # all of the data units in the train split of the dataset)\n",
    "    ed_lv_dice = 0\n",
    "    es_lv_dice = 0\n",
    "    \n",
    "    # seed the numpy random number generator.\n",
    "    # however, I don't see it being seeded with a specific value to allow for reproducibility.\n",
    "    # it should be.\n",
    "    np.random.seed()\n",
    "    \n",
    "    # loop thru training dataloader, \n",
    "    # batchsize is 4\n",
    "    # so shape will be (4, 3, 32, 112, 112)\n",
    "    # 3 channels, 32 frames in each clip, 112 x 112 frame size\n",
    "    for batch_idx, batch in enumerate(train_loader, 1):\n",
    "        \n",
    "        # this is the raw frames from the video in the dataset to be passed into the model\n",
    "        # he's just transforming them into the right tensor data types to pass thru to the modle\n",
    "        \n",
    "        # is this a single video, or is this 4 videos of clip size 32 frames that are being passed to the model?\n",
    "        video_clips = torch.Tensor(batch[0])\n",
    "        video_clips = video_clips.type(Tensor)\n",
    "        \n",
    "        # grab other data from current batch that we will use later on\n",
    "        filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label = batch[1]\n",
    "\n",
    "        # reset the optimizer's params' gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get the motion tracking output from the motion tracking head using the feature map\n",
    "        # get the segmentation output too (guess of lv segmentation)\n",
    "        segmentation_output, motion_output = model(video_clips)\n",
    "        \n",
    "        # compute loss using the actual video clip information with the motion tracking information\n",
    "        # what is the difference btwn the two loss functions here?\n",
    "        # deformation_motion_loss and motion_seg_loss ?\n",
    "        loss = 0\n",
    "        deform_loss = deformation_motion_loss(video_clips, motion_output)\n",
    "        loss += deform_loss\n",
    "        \n",
    "        # these vars are to store the mean seg and motion losses for the 32 frames clips that we have.\n",
    "        segmentation_loss = 0\n",
    "        motion_loss = 0\n",
    "        \n",
    "        # iterate for the number of 32 frames clips that we have for this particular video in this current\n",
    "        # batch of data. so we should be doing this 4 times if the batch_size is 4, but I don't see that \n",
    "        # happening\n",
    "        for i in range(video_clips.shape[0]):\n",
    "            # transform the ed/es labeled frames info into whatever we need\n",
    "            # them to be to be used for our loss functions\n",
    "            label_ed = np.expand_dims(ed_label.numpy(), 1).astype(\"int\")\n",
    "            label_es = np.expand_dims(es_label.numpy(), 1).astype(\"int\")\n",
    "\n",
    "            label_ed = label_ed[i]\n",
    "            label_es = label_es[i]\n",
    "\n",
    "            label_ed = np.expand_dims(label_ed, 0)\n",
    "            label_es = np.expand_dims(label_es, 0)\n",
    "\n",
    "            motion_one_output = motion_output[i].unsqueeze(0)\n",
    "            segmentation_one_output = segmentation_output[i].unsqueeze(0)\n",
    "\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "\n",
    "            segmentation_one_loss, motion_one_loss = motion_seg_loss(label_ed, label_es, \n",
    "                                                                     ed_one_index, es_one_index, \n",
    "                                                                     motion_one_output, segmentation_one_output, \n",
    "                                                                     0, video_clips.shape[2], \n",
    "                                                                     F.binary_cross_entropy_with_logits)\n",
    "            segmentation_loss += segmentation_one_loss\n",
    "            motion_loss += motion_one_loss\n",
    "        loss += (segmentation_loss / video_clips.shape[0])\n",
    "        loss += (motion_loss / video_clips.shape[0])              \n",
    "        \n",
    "        ed_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        es_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        for i in range(len(ed_clip_index)):\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "            \n",
    "            ed_seg = segmentation_output[i, :, ed_one_index].unsqueeze(0)\n",
    "            ed_segmentations = torch.cat([ed_segmentations, ed_seg])\n",
    "            \n",
    "            es_seg = segmentation_output[i, :, es_one_index].unsqueeze(0)\n",
    "            es_segmentations = torch.cat([es_segmentations, es_seg])\n",
    "            \n",
    "            \n",
    "        ed_es_seg_loss = 0\n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(ed_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(ed_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\") \n",
    "        \n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(es_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(es_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\") \n",
    "        ed_es_seg_loss /= 2\n",
    "        \n",
    "        loss += ed_es_seg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        ed_segmentation_argmax = torch.argmax(ed_segmentations, 1).cpu().detach().numpy()\n",
    "        es_segmentation_argmax = torch.argmax(es_segmentations, 1).cpu().detach().numpy()\n",
    "            \n",
    "        ed_lv_dice += categorical_dice(ed_segmentation_argmax, ed_label.numpy(), 1)\n",
    "        es_lv_dice += categorical_dice(es_segmentation_argmax, es_label.numpy(), 1)\n",
    "        \n",
    "        # Printing the intermediate training statistics\n",
    "        if batch_idx % 280 == 0:\n",
    "            print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(video_clips), len(train_loader) * len(video_clips),\n",
    "                100. * batch_idx / len(train_loader), np.mean(epoch_loss)))\n",
    "\n",
    "            print(\"ED LV: {:.3f}\".format(ed_lv_dice / batch_idx))\n",
    "            print(\"ES LV: {:.3f}\".format(es_lv_dice / batch_idx))\n",
    "            \n",
    "            print(\"On a particular batch:\")\n",
    "            print(\"Deform loss: \", deform_loss)\n",
    "            print(\"Segmentation loss: \", ed_es_seg_loss)\n",
    "            print(\"Seg Motion loss: \", segmentation_loss / video_clips.shape[0], motion_loss / video_clips.shape[0])\n",
    "    \n",
    "    return epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
