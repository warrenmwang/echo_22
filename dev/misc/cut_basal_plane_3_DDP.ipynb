{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41bfd50-7fd3-4ab7-97a1-c4df057e235b",
   "metadata": {},
   "source": [
    "## Try to use [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) because regular `DataParallel` does not seem to be working (infinite hang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b93c4c-1676-49f1-9a5e-5add9a8157d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Original_Pretrained_R2plus1DMotionSegNet.pth\"\n",
    "\n",
    "# model_name = \"dropout_v2_0_10_R2plus1DMotionSegNet.pth\"\n",
    "# model_name = \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f03d8eb-f6dd-490e-9af9-f098620282a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wmw015/echo_22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads, forgot to define in forward pass function, but still saw diff, weird.)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_50_R2plus1D_18_MotionNet import dropout_v2_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_75_R2plus1D_18_MotionNet import dropout_v2_0_75_R2plus1D_18_MotionNet\n",
    "# v3 dropout (one dropout layer defined in forward pass func, this should've been the correct way to do it.)\n",
    "from src.model.dropout_v3_0_00_R2plus1D_18_MotionNet import dropout_v3_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_10_R2plus1D_18_MotionNet import dropout_v3_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_25_R2plus1D_18_MotionNet import dropout_v3_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_50_R2plus1D_18_MotionNet import dropout_v3_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v3_0_75_R2plus1D_18_MotionNet import dropout_v3_0_75_R2plus1D_18_MotionNet\n",
    "# v4 dropout (4 dropout layers in different places in the forward func, I'm going to guess more \"generalizable\")\n",
    "from src.model.dropout_v4_0_00_R2plus1D_18_MotionNet import dropout_v4_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_10_R2plus1D_18_MotionNet import dropout_v4_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_25_R2plus1D_18_MotionNet import dropout_v4_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_50_R2plus1D_18_MotionNet import dropout_v4_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v4_0_75_R2plus1D_18_MotionNet import dropout_v4_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# for finding lv seg borders\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    print(f'worker_seed: {worker_seed}')\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "# random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0de3bd-e49a-49aa-906d-9c6f8e0d419b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2db86c3-9ac4-48f5-ad13-6e0a26481deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573f2a6-a74c-4ef5-87d1-af481423cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model_save_path = f\"save_models/{model_name}\"\n",
    "    \n",
    "if model_name == 'Original_Pretrained_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_00_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == 'dropout_v2_0_10_R2plus1DMotionSegNet.pth':\n",
    "    model_template_obj = dropout_v2_0_10_R2plus1D_18_MotionNet()\n",
    "\n",
    "elif model_name == \"dropout_v3_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v3_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v3_0_25_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_00_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_00_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_10_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_10_R2plus1D_18_MotionNet()\n",
    "elif model_name == \"dropout_v4_0_25_R2plus1DMotionSegNet.pth\":\n",
    "    model_template_obj = dropout_v4_0_25_R2plus1D_18_MotionNet()\n",
    "\n",
    "print('1')\n",
    "model = torch.nn.DataParallel(model_template_obj)\n",
    "print('2')\n",
    "\n",
    "model.to(device)\n",
    "print('3')\n",
    "torch.cuda.empty_cache()\n",
    "print('4')\n",
    "model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "print('5')\n",
    "print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "print('6')\n",
    "model.eval();\n",
    "print('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2236ba-5658-421e-8bb3-6f7e0660f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warp_data(test_pat_index, test_dataset, model):\n",
    "    '''\n",
    "    INPUT: \n",
    "    test_pat_index and test_dataset to be used like the following:\n",
    "        video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "    model\n",
    "        model to be used, one model at a time\n",
    "    clip_index\n",
    "        the clip we should use, determines the position of the ED and ES frame in the 32 frame clip\n",
    "        the difference in frames from ED to ES is always the same\n",
    "\n",
    "    OUTPUT:\n",
    "    es_created_from_warping_ed, ed_created_from_warping_es\n",
    "        both with shape: (1, 2, 112, 112)\n",
    "        you should double check the shape.\n",
    "    and other vars \n",
    "    '''\n",
    "    ########################### Helper functions ###########################\n",
    "    # goes thru a video and annotates where we can start clips given video length, clip length, etc.\n",
    "    def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "        assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "        possible_shift = clip_length - (es_index - ed_index)\n",
    "        allowed_right = video_length - es_index\n",
    "        if allowed_right < possible_shift:\n",
    "            return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "        if possible_shift < 0:\n",
    "            return np.array([ed_index])\n",
    "        elif ed_index < possible_shift:\n",
    "            return np.arange(ed_index + 1)\n",
    "        else:\n",
    "            return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "        \n",
    "    def generate_2dmotion_field_PLAY(x, offset):\n",
    "        # Qin's code for joint_motion_seg learning works fine on our purpose too\n",
    "        # Same idea https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/5\n",
    "        x_shape = x.shape\n",
    "        # print(f'x_shape: {x_shape}')\n",
    "\n",
    "        grid_w, grid_h = torch.meshgrid([torch.linspace(-1, 1, x_shape[2]), torch.linspace(-1, 1, x_shape[3])])  # (h, w)\n",
    "\n",
    "        # this should just be moving the vars to gpu mem and doing some data type conversion to some\n",
    "        # floating point precision\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_h = grid_h.cuda().float()\n",
    "\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "\n",
    "        # OLD \n",
    "        # offset_h, offset_w = torch.split(offset, 1, 1)\n",
    "        # NEW , TESTING\n",
    "        offset_h, offset_w = torch.split(offset, 1)\n",
    "\n",
    "        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n",
    "\n",
    "        offset_w = grid_w + offset_w\n",
    "        offset_h = grid_h + offset_h\n",
    "\n",
    "        offsets = torch.stack((offset_h, offset_w), 3)\n",
    "        return offsets\n",
    "\n",
    "    def categorical_dice(prediction, truth, k, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "            Compute the dice overlap between the predicted labels and truth\n",
    "            Not a loss\n",
    "        \"\"\"\n",
    "        # Dice overlap metric for label value k\n",
    "        A = (prediction == k)\n",
    "        B = (truth == k)\n",
    "        return 2 * np.sum(A * B) / (np.sum(A) + np.sum(B) + epsilon)\n",
    "    \n",
    "    ########################################################################\n",
    "    # initial grabbing of the data that we'll use\n",
    "    video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "    \n",
    "    # get all possible start indices for 32 frame clip with ed/es in right order\n",
    "    possible_starts = get_all_possible_start_points(ed_index, es_index, video.shape[1], clip_length=32)\n",
    "    \n",
    "    # for now, let's use the last clip from our set of all possible clips to use\n",
    "    clip_index = len(possible_starts) - 1\n",
    "    # print(f'clip_index: {clip_index}')\n",
    "\n",
    "    # get the diff in frame len from ed to es\n",
    "    delta_ed_es = es_index - ed_index\n",
    "    \n",
    "    # use model to segment frames\n",
    "    segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "    motion_outputs = np.empty(shape=(1, 4, 32, 112, 112))\n",
    "    for start in possible_starts:\n",
    "        one_clip = np.expand_dims(video[:, start: start + 32], 0)\n",
    "        segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "        segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        motion_outputs = np.concatenate([motion_outputs, motion_output.cpu().detach().numpy()])\n",
    "    segmentation_outputs = segmentation_outputs[1:]\n",
    "    motion_outputs = motion_outputs[1:]\n",
    "\n",
    "    # grab whatever clip we want\n",
    "    curr_clip_segmentations = segmentation_outputs[clip_index]\n",
    "    curr_clip_motions = motion_outputs[clip_index]\n",
    "    \n",
    "    # print(f'curr_clip_segmentations: {curr_clip_segmentations.shape}')\n",
    "    # print(f'curr_clip_motions: {curr_clip_motions.shape}')\n",
    "    \n",
    "    ######################## Warp from ED -> ES and ED <- ES ########################\n",
    "    \n",
    "    # remember, we will want to continuously warp the previous frame that has been warped forward in time\n",
    "    # only the first frame that we start with will be the actual seg out frame\n",
    "    flow_source = None\n",
    "\n",
    "    # warping FORWARD from ED -> ES\n",
    "    # python range is [x, y), inclusive start and exclusive end\n",
    "    for frame_index in range(31 - delta_ed_es - clip_index, 31 - clip_index + 1, 1):\n",
    "        # grab forward motion\n",
    "        forward_motion = curr_clip_motions[:2, frame_index,...]\n",
    "\n",
    "        # grab the ED seg out frame to warp\n",
    "        if frame_index == 0:\n",
    "            flow_source = np.array([curr_clip_segmentations[:, frame_index, ...]])\n",
    "            flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "        else:\n",
    "            pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "\n",
    "        # convert to tensors and move to gpu with float dtype\n",
    "        forward_motion = torch.from_numpy(forward_motion).to(device).float()\n",
    "        # generate motion field for forward motion\n",
    "        motion_field = generate_2dmotion_field_PLAY(flow_source, forward_motion)\n",
    "        # create frame i+1 relative to curr frame i \n",
    "        next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "        # use i+1 frame as next loop iter's i frame\n",
    "        flow_source = next_label\n",
    "\n",
    "    es_created_from_warping_ed = flow_source.cpu().detach().numpy()\n",
    "\n",
    "    # warping BACKWARD from ED <- ES\n",
    "    for frame_index in range(31 - clip_index, 31 - delta_ed_es - clip_index - 1, -1):\n",
    "        # grab backward motion\n",
    "        backward_motion = curr_clip_motions[2:, frame_index,...]\n",
    "\n",
    "        # grab the ES seg out frame to start\n",
    "        if frame_index == delta_ed_es:\n",
    "            flow_source = np.array([curr_clip_segmentations[:, frame_index, ...]])\n",
    "            flow_source = torch.from_numpy(flow_source).to(device).float()\n",
    "        else:\n",
    "            pass # use previous next_label as flow_source, should be redefined at end of previous loop iter\n",
    "\n",
    "        # convert to tensors and move to gpu with float dtype\n",
    "        backward_motion = torch.from_numpy(backward_motion).to(device).float()\n",
    "        # generate motion field for backward motion\n",
    "        motion_field = generate_2dmotion_field_PLAY(flow_source, backward_motion)\n",
    "        # create frame i-1 relative to curr frame i \n",
    "        next_label = F.grid_sample(flow_source, motion_field, align_corners=False, mode=\"bilinear\", padding_mode='border')\n",
    "        # use i-1 frame as next loop iter's i frame\n",
    "        flow_source = next_label\n",
    "\n",
    "    ed_created_from_warping_es = flow_source.cpu().detach().numpy()\n",
    "    \n",
    "    ######################## ######################## ########################\n",
    "    \n",
    "    ed_created_from_warping_es = np.argmax(ed_created_from_warping_es, 1)[0]\n",
    "\n",
    "    es_created_from_warping_ed = np.argmax(es_created_from_warping_ed, 1)[0]\n",
    "\n",
    "    \n",
    "    return es_created_from_warping_ed, ed_created_from_warping_es, curr_clip_segmentations, ed_label, es_label, delta_ed_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e094138-5a4a-44ec-aa46-dc9861228b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    ''' return root mean square error difference between the two values passed in'''\n",
    "    return np.sqrt((x - y) ** 2)\n",
    "\n",
    "def strain_value(l_0, l_i):\n",
    "    '''\n",
    "    inputs: l_0, l_i -- original length and new length at some time point, respectively\n",
    "    output: e -- strain value (positive for elongation, negative for compressing/shortening) as a percentage (e.g. output 0.155 == 15.5 %)\n",
    "    \n",
    "    examples: \n",
    "        l_i = 10\n",
    "        l_0 = 5\n",
    "        e == (10 - 5) / 5 = 1, factor of lengthening relative to original value\n",
    "        \n",
    "        l_i = 5\n",
    "        l_0 = 5\n",
    "        e == (5 - 5) / 5 = 0, no strain\n",
    "    '''\n",
    "    return (l_i - l_0) / l_0\n",
    "\n",
    "def give_boundary(x):\n",
    "    '''\n",
    "    input: \n",
    "        x (112, 112) one-hot encoded lv mask/segmentation\n",
    "        has to be numpy ndarray on cpu mem\n",
    "    output: \n",
    "        y (112, 112) black and white picture of boundary of lv\n",
    "        lv is denoted by white pixels (255 val)\n",
    "        not lv is denoted by black pixels (0 val)\n",
    "    '''\n",
    "    foo = np.uint8(x * 255)\n",
    "    ret, thresh = cv.threshold(foo, 127, 255, 0)\n",
    "    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    blank = np.zeros((112,112))\n",
    "    cv.drawContours(blank, contours, -1, (255,255,255), 1)\n",
    "    return blank\n",
    "\n",
    "def boundaries_to_strain(before, after):\n",
    "    '''\n",
    "    input:\n",
    "        before (112, 112) boundary of lv\n",
    "        after (112, 112) boundary of lv\n",
    "        expect black and white pixels, white pixels for lv seg\n",
    "        counting white (val = 255) pixels of the boundary as length as one perimeter\n",
    "        \n",
    "    output: \n",
    "        y - floating point number representing strain value, left as a decimal, NOT multiplied by 100\n",
    "    '''    \n",
    "    l_0 = np.count_nonzero(before == 255)\n",
    "    l_i = np.count_nonzero(after == 255)\n",
    "    return strain_value(l_0, l_i)\n",
    "    \n",
    "def images_to_strain(ed_frame, es_frame):\n",
    "    '''\n",
    "    input:\n",
    "        ed_frame (112, 112)\n",
    "        es_frame (112, 112)\n",
    "        \n",
    "        expect vals to be one-hot encoded (1's for lv, 0's for not lv)\n",
    "    output:\n",
    "        x - floating point number\n",
    "        strain value (some decimal, should be negative)\n",
    "    '''\n",
    "    # get boundaries\n",
    "    ed_bound = give_boundary(ed_frame)\n",
    "    es_bound = give_boundary(es_frame)\n",
    "    # compute strain and return\n",
    "    return boundaries_to_strain(ed_bound, es_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fd893-2c4e-4074-af9a-ce8153988003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blot_out_given_rect(rectangle_corners, I, replace_val = 0):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        I - shape (112, 112) \n",
    "        \n",
    "    output:\n",
    "        I_copy -- deepcopy of original image I with the defined rectangle section given zeroed out in the image\n",
    "    '''\n",
    "    \n",
    "     # i = row, j = col, we're thinking in ij/row col instead of xy\n",
    "    if rectangle_corners[0][0] < rectangle_corners[1][0]:\n",
    "        i_start = int(rectangle_corners[0][0])\n",
    "        i_end = int(rectangle_corners[1][0])\n",
    "    else:\n",
    "        i_end = int(rectangle_corners[0][0])\n",
    "        i_start = int(rectangle_corners[1][0])\n",
    "        \n",
    "    if rectangle_corners[1][1] < rectangle_corners[0][1]:\n",
    "        j_start = int(rectangle_corners[1][1])\n",
    "        j_end = int(rectangle_corners[0][1])\n",
    "    else:\n",
    "        j_end = int(rectangle_corners[1][1])\n",
    "        j_start = int(rectangle_corners[0][1])\n",
    "        \n",
    "    # make copy and alter then return\n",
    "    import copy \n",
    "    I_copy = copy.deepcopy(I)\n",
    "    \n",
    "    # zero out everything at and below the highest of the two index points (i_start)\n",
    "    I_copy[i_start : I.shape[0], 0 : I.shape[1]] = replace_val\n",
    "    \n",
    "    return I_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79737662-7484-4f29-89ba-71344a53681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_with_rect(rectangle_corners, I, replace_val=127):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2, 2)\n",
    "    output:\n",
    "        empty image with the rectangle as defined blotted out with values of 127 -- shape (112, 112)\n",
    "    '''\n",
    "    I_rect = np.zeros(I.shape)\n",
    "    I_rect = blot_out_given_rect(rectangle_corners, I_rect, replace_val=replace_val)\n",
    "    return I_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ff2e2-419d-4d2f-ab00-f5e5820cd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(rectangle_corners, I=None, show_applied=False):\n",
    "    '''\n",
    "    input:\n",
    "        rectangle_corners - shape (2,2)\n",
    "        Well, technically rectangle corners could even have shape of (4, 2\n",
    "    \n",
    "    plot a rectangle based on the coords given (in format of the skimage functions that get the corner pixels coords)\n",
    "    also if an image is given, plot the rectangle on top of it\n",
    "    '''\n",
    "   \n",
    "    # plot the rectangle\n",
    "    I_rect = blank_with_rect(rectangle_corners, I)\n",
    "    \n",
    "    # plot the original image if given\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if I is not None:\n",
    "        # assuming I has unique values (0,1)\n",
    "        plt.imshow(I * 255, cmap='gray', zorder=1)\n",
    "        plt.imshow(I_rect, cmap='gray', zorder=2, alpha=0.5)\n",
    "        # plt.colorbar()\n",
    "    else:\n",
    "        plt.imshow(I_rect, cmap='gray')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # show another figure of using the shown rectangle to zero out original image if asked\n",
    "    if show_applied:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(blot_out_given_rect(rectangle_corners, I), cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f2684-43e4-44da-bed9-377020a703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_corner_pixels_detected(I, show_plot=True, return_vars=False):\n",
    "    '''\n",
    "    input: I (112, 112), unique vals of 0,1\n",
    "    output (if requested):\n",
    "        coords - numpy ndarray\n",
    "        coords_subpix - nump ndarray\n",
    "            shapes of 2 objects above may vary...\n",
    "    \n",
    "    Prints the matplotlib figure with the corner pixels detected attached to the image\n",
    "    \n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)\n",
    "    coords_subpix = corner_subpix(I, coords, window_size=13)\n",
    "\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        ax.imshow(I, cmap=plt.cm.gray)\n",
    "        ax.plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "                linestyle='None', markersize=6)\n",
    "        ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)\n",
    "        plt.show()\n",
    "    \n",
    "    # return quantities if requested\n",
    "    if return_vars:\n",
    "        return coords, coords_subpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96057bd-ecd1-416b-964a-488ec55829cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangle_corners(I):\n",
    "    '''\n",
    "    input:\n",
    "        I - shape (112, 112)\n",
    "    output:\n",
    "        rectange_corners - shape (2,2)\n",
    "    '''\n",
    "    from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "\n",
    "    # get corner pixels to form rectangle from\n",
    "    coords = corner_peaks(corner_harris(I), min_distance=5, threshold_rel=0.02)    \n",
    "    \n",
    "    # descending order sort all possible values along axis=0 for sorting by i / row index\n",
    "    coords[::-1].sort(axis=0)\n",
    "    \n",
    "    rectangle_corners = coords[0:2]\n",
    "\n",
    "    return rectangle_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc906e4f-95a2-478d-9d6e-3b607f21f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_basal_plane_out(I):\n",
    "    '''\n",
    "    input: I (112, 112) expected to have unique values of (0,1)...1 for lv, 0 for not lv\n",
    "    output: I_new (112, 112) with unique values of (0,1)... 1 for lv, 0 for not lv...this has the basal plane removed\n",
    "    \n",
    "    https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py\n",
    "    \n",
    "    When choosing which 2 points to create a rough rectangle to zero out the bottom mitral valve / basal plane boundary\n",
    "    we will go with this ordering of points to use:\n",
    "    1. Both points from corner_subpix\n",
    "    2. One point from corner_subpix, one point from corner_peaks\n",
    "    3. Both points from corner_peaks\n",
    "    \n",
    "    If we can't get two points at the area of the basal plane from any of the three situations described above, we are out of luck. \n",
    "    We will have to do something else, but this won't work then. \n",
    "    '''  \n",
    "    # make sure values in image are only 0 and 1\n",
    "    check_unique_vals = np.unique(I)\n",
    "    if check_unique_vals[0] == 0 and check_unique_vals[1] == 255:\n",
    "        I = I / 255\n",
    "    elif check_unique_vals[0] == 0 and check_unique_vals[1] != 1:\n",
    "        print('incorrect values in Image')\n",
    "        return\n",
    "    \n",
    "    rectangle_corners = get_rectangle_corners(I)\n",
    "    return blot_out_given_rect(rectangle_corners, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09db5ce-4dc3-4cbb-95c9-35df9ebb0112",
   "metadata": {},
   "source": [
    "## Let's start doing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493ac2a-3ab0-4cf7-90e9-b5c26b0403d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat_index = np.random.randint(0, len(test_dataset))\n",
    "print(test_pat_index)\n",
    "es_warp, ed_warp, curr_clip_segmentations, ed_label, es_label, delta_ed_es = get_warp_data(test_pat_index = test_pat_index, test_dataset = test_dataset, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282f8b-7561-4aaa-a053-a10992680357",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_seg_out = np.argmax(curr_clip_segmentations, 0)[0]\n",
    "es_seg_out = np.argmax(curr_clip_segmentations, 0)[delta_ed_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39827a-18b0-4b40-bc8b-9b7c0c77bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [give_boundary(x) for x in [ed_label, es_label, ed_seg_out, es_seg_out, ed_warp, es_warp]]\n",
    "name_map = ['ed_label', 'es_label', 'ed_seg_out', 'es_seg_out', 'ed_warp', 'es_warp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7ecbc-4371-47d3-817a-d3c1f34e55fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ind, boundary in enumerate(boundaries):\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(25,5))\n",
    "    I = boundary\n",
    "    fig.suptitle(f'test_pat_index: {test_pat_index} | {name_map[ind]}')\n",
    "    ax1.set_title('Image')\n",
    "    ax1.imshow(I, cmap='gray')\n",
    "    \n",
    "    ax2.set_title('All points found')\n",
    "    coords, coords_subpix = vizualize_corner_pixels_detected(I, show_plot = False, return_vars=True)\n",
    "    ax2.imshow(I, cmap='gray')\n",
    "    ax2.plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "            linestyle='None', markersize=6)\n",
    "    ax2.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)  \n",
    "    \n",
    "    rectangle_corners = get_rectangle_corners(I)\n",
    "    ax3.set_title('Translucent Zero Range over Image')\n",
    "    ax3.imshow(I, cmap='gray', zorder=1)                                                # image at bottom, completely opaque\n",
    "    ax3.imshow(blank_with_rect(rectangle_corners, I), cmap='gray', zorder=2, alpha=0.5) # rectangle on top, 50% see thru\n",
    "    \n",
    "    ax4.set_title('Remove Basal Plane')\n",
    "    ax4.imshow(cut_basal_plane_out(I), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bb013-17e3-4f66-9289-0171e7d67a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a95f82-2f1c-4a54-ba44-b8c916fcc29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
