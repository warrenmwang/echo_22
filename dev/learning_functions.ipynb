{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93995e3-0828-49aa-a0a1-ddc8e855dce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding the Training Function\n",
    "\n",
    "Go thru and comment each line <br>\n",
    "\n",
    "Where are the loss functions called ? \n",
    "\n",
    "Train function only ever calls "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1ab15-eb32-4833-9883-c981be4f25b6",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "```python\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assume the input shape (d, h, w)\n",
    "        \n",
    "        # pass thru r2plus1d model layers\n",
    "        \n",
    "        # Features output from stem channels == 64, shape = (32, 56, 56)\n",
    "        output_stem = self.r2plus1d_model.stem(x)\n",
    "        # Features output from block 1 channels == 64, shape = (32, 56, 56)\n",
    "        output_layer_1 = self.r2plus1d_model.layer1(output_stem)\n",
    "        # Features output from block 2 channels == 128, shape = (16, 28, 28)\n",
    "        output_layer_2 = self.r2plus1d_model.layer2(output_layer_1)\n",
    "        # Features output from block 3 channels == 256, shape = (8, 14, 14)\n",
    "        output_layer_3 = self.r2plus1d_model.layer3(output_layer_2)\n",
    "        # Features output from block 4 channels == 512, shape = (4, 7, 7)\n",
    "        output_layer_4 = self.r2plus1d_model.layer4(output_layer_3)\n",
    "        \n",
    "        # pass thru additional convolutional layers that we had defined in the model\n",
    "        \n",
    "        # Upsampling 5 features output to shape of original input (32, 112, 112)\n",
    "        # Stem (32, 56, 56) -> (32, 112, 112)\n",
    "        up_stem = F.interpolate(output_stem, scale_factor=[1, 2, 2], mode='trilinear', align_corners=True)\n",
    "        # block 1 (32, 56, 56) -> (32, 112, 112)\n",
    "        up_layer_1 = F.interpolate(output_layer_1, scale_factor=[1, 2, 2], mode='trilinear', align_corners=True)\n",
    "        # block 2 (16, 28, 28) -> (32, 112, 112)\n",
    "        up_layer_2 = F.interpolate(output_layer_2, scale_factor=[2, 4, 4], mode='trilinear', align_corners=True)\n",
    "        # block 3 (8, 14, 14) -> (32, 112, 112)\n",
    "        up_layer_3 = F.interpolate(output_layer_3, scale_factor=[4, 8, 8], mode='trilinear', align_corners=True)\n",
    "        # block 4 (4, 7, 7) -> (32, 112, 112)\n",
    "        up_layer_4 = F.interpolate(output_layer_4, scale_factor=[8, 16, 16], mode='trilinear', align_corners=True)\n",
    "        \n",
    "        # Concatenate the upsampled output: 64 + 64 + 128 + 256 + 512 = 1024\n",
    "        cat_output = torch.cat([up_stem, up_layer_1, up_layer_2, up_layer_3, up_layer_4], 1)\n",
    "            \n",
    "        # 1024 -> 64\n",
    "        x = self.comb_1_layer(cat_output)\n",
    "        x = self.comb_batch_norm_1(x)\n",
    "        x = self.comb_relu_1(x)\n",
    "        \n",
    "        # 64 -> 64\n",
    "        x = self.comb_2_layer(x)\n",
    "        x = self.comb_batch_norm_2(x)\n",
    "        x = self.comb_relu_2(x)\n",
    "        \n",
    "        # Segmentation output: 64 -> 2 [Background, LV]\n",
    "        segmentation_output = self.segmentation_head(x)\n",
    "        \n",
    "        # Motion output: 64 -> 4 [Forward x, y, backward x, y]\n",
    "        motion_output = self.motion_head(x)\n",
    "        motion_output = torch.tanh(motion_output)\n",
    "        \n",
    "        return segmentation_output, motion_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927535d3-e9f0-4151-a747-b42c5fb55863",
   "metadata": {},
   "source": [
    "### The training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98efdd-d92e-4511-80fd-e60b4cf3fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "model_save_path = \"save_models/R2plus1DMotionSegNet_model.pth\"   \n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "n_epoch = 10\n",
    "min_loss = 1e5\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print(\"-\" * 32 + 'Epoch {}'.format(epoch) + \"-\" * 32)\n",
    "    start = time.time()\n",
    "    \n",
    "    # actual training functoin call\n",
    "    train_loss = train(epoch, train_loader=train_dataloader, model=model, optimizer=optimizer)\n",
    "    \n",
    "    train_loss_list.append(np.mean(train_loss))\n",
    "    end = time.time()\n",
    "    print(\"training took {:.8f} seconds\".format(end-start))\n",
    "    \n",
    "    # actual validation functoin call\n",
    "    valid_loss = test(epoch, test_loader=valid_dataloader, model=model, optimizer=optimizer)\n",
    "    \n",
    "    valid_loss_list.append(np.mean(valid_loss))\n",
    "    \n",
    "    # only save models after a training cycle only if validation loss is lower than previous\n",
    "    # min validation loss (we use avg of validation loss)\n",
    "    if (np.mean(valid_loss) < min_loss) and (epoch > 0):\n",
    "        # save new min loss, average of all losses from the validation losses\n",
    "        # all the losses are of the multiple segmentations per 32 clip frame from all\n",
    "        # videos from the test_dataloader (1276 videos)\n",
    "        min_loss = np.mean(valid_loss) \n",
    "        \n",
    "        # save model\n",
    "        torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, model_save_path)\n",
    "    \n",
    "    # change optimizer learning rate after some number of epochs to be smaller to take\n",
    "    # smaller steps, do not overjump\n",
    "    if epoch == 3:\n",
    "        lr_T = 1e-5\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efc6eb-e6ec-44f7-98e4-555bdd62334c",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b722f-d566-42cf-a024-0a21d331fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer):\n",
    "    \"\"\" Training function for the network \"\"\"\n",
    "    model.train()   # set model to training mode\n",
    "    epoch_loss = [] # holder of epoch losses\n",
    "    ed_lv_dice = 0\n",
    "    es_lv_dice = 0\n",
    "    \n",
    "    np.random.seed() # seed RNG\n",
    "    for batch_idx, batch in enumerate(train_loader, 1):\n",
    "        # from the batch of data from the train_loader, convert it into a usable video_clips var to be passed into the model\n",
    "        # where the internal feature map will parse thru it, and then spit out the seg and motion outputs\n",
    "        video_clips = torch.Tensor(batch[0])\n",
    "        video_clips = video_clips.type(Tensor) # cast to Tensor type and return, if video clip already tensor (\n",
    "                                               # then don't, it looks to me like another try just in case transform to tensor doesn't work?\n",
    "        filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label = batch[1]\n",
    "\n",
    "        # clear optimizer gradients, sets them all to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the motion tracking output from the motion tracking head using the feature map\n",
    "        # model just eats up a video_clip and spits out 2 ndarrays of values of the segmentation and motion outputs\n",
    "        # the function that gets called should be the forward pass.\n",
    "        # for reference, here is that forward pass that the video_clips goes thru:\n",
    "        # video_clips becomes the x that goes thru the layers of the model\n",
    "\n",
    "        segmentation_output, motion_output = model(video_clips)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        # clasfv loss func\n",
    "        # see how much warp based on motion_output is off by.\n",
    "        # compare frame n+warp to actual n+1 frame to see how much of a difference there exists btwn the two ?\n",
    "        # returned loss is an average of smooth/huber loss and the mse loss\n",
    "        deform_loss = deformation_motion_loss(video_clips, motion_output)\n",
    "        \n",
    "        # add deform loss to total loss count\n",
    "        loss += deform_loss\n",
    "\n",
    "        segmentation_loss = 0\n",
    "        motion_loss = 0\n",
    "        \n",
    "        # loop over number of 32 frame video clips (since that is individual data unit to be passed to the model)\n",
    "        for i in range(video_clips.shape[0]):\n",
    "            # adjust the dimensions, expand by axis=1\n",
    "            label_ed = np.expand_dims(ed_label.numpy(), 1).astype(\"int\")\n",
    "            label_es = np.expand_dims(es_label.numpy(), 1).astype(\"int\")\n",
    "\n",
    "            # grab at index i\n",
    "            label_ed = label_ed[i]\n",
    "            label_es = label_es[i]\n",
    "\n",
    "            # adjust dimensions, expand by axis=0\n",
    "            label_ed = np.expand_dims(label_ed, 0)\n",
    "            label_es = np.expand_dims(label_es, 0)\n",
    "\n",
    "            # pytorch unsqueeze: \"returns a new tensor with a dimension of size one inserted at the specified position\"\n",
    "            motion_one_output = motion_output[i].unsqueeze(0)\n",
    "            segmentation_one_output = segmentation_output[i].unsqueeze(0)\n",
    "\n",
    "            # grab at one index (i)\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "\n",
    "            # clasfv loss func\n",
    "            # warps/transforms ED -> ES and ED <- ES (forward, backward) using the motion output and compare to \n",
    "            # actual ED/ES frames to compute losses (returns flow_loss, OTS_Loss)\n",
    "            # we get:\n",
    "            # segmentation_loss, motion_loss = flow_loss, OTS_Loss\n",
    "            segmentation_one_loss, motion_one_loss = motion_seg_loss(label_ed, label_es, \n",
    "                                                                     ed_one_index, es_one_index, \n",
    "                                                                     motion_one_output, segmentation_one_output, \n",
    "                                                                     0, video_clips.shape[2], \n",
    "                                                                     F.binary_cross_entropy_with_logits)\n",
    "            # concatenate losses to our existing losses for this video\n",
    "            segmentation_loss += segmentation_one_loss\n",
    "            motion_loss += motion_one_loss\n",
    "        \n",
    "        # average our losses from the individual losses computed from the individual frames over number of frames in video clip\n",
    "        loss += (segmentation_loss / video_clips.shape[0])\n",
    "        loss += (motion_loss / video_clips.shape[0])              \n",
    "        \n",
    "        # initialize tensors\n",
    "        ed_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        es_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        \n",
    "        # iterate over number of ed clips we have (ed_clip_index holds the indeces of all ed clip/frames ?)\n",
    "        for i in range(len(ed_clip_index)):\n",
    "            # grab one index of an ed and es frame\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "            \n",
    "            # grab segmentatoin output for this specific ed and es frame\n",
    "            ed_seg = segmentation_output[i, :, ed_one_index].unsqueeze(0)\n",
    "            # concatenate segmentation results to our storage vars\n",
    "            ed_segmentations = torch.cat([ed_segmentations, ed_seg])\n",
    "            \n",
    "            es_seg = segmentation_output[i, :, es_one_index].unsqueeze(0)\n",
    "            es_segmentations = torch.cat([es_segmentations, es_seg])\n",
    "            \n",
    "           \n",
    "        # compute loss from the segmentation results of ed and es frames (remember, we have the labeled answers for ed and es frames)\n",
    "        ed_es_seg_loss = 0\n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(ed_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(ed_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\") \n",
    "        \n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(es_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(es_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\")\n",
    "        # average ed / es segmentation loss\n",
    "        ed_es_seg_loss /= 2\n",
    "        \n",
    "        # add this avg to overall loss\n",
    "        loss += ed_es_seg_loss\n",
    "\n",
    "        loss.backward()   # compute gradient of the objective function\n",
    "        \n",
    "        optimizer.step()  # update the weights of network using the computed gradients \n",
    "        \n",
    "        epoch_loss.append(loss.item()) # convert loss tensor to a python number, then append curr loss to our epoch loss\n",
    "        \n",
    "        # lets look at this carefully. (just makes our ed and es segmentaion results usable for other computation)\n",
    "        # torch.argmax() returns index of the max val in the input tensor (1st param), then reduce to a single dimension (2nd param)\n",
    "        # .detach() return a new tensor detached from the graph, storage/mem loc still same as original\n",
    "        # .cpu() returns a copy of tensor to cpu memory (RAM) from (presumably) the gpu, if already on cpu mem / ram just return obj\n",
    "        # .numpy() convert pytorch tensor to numpy ndarray obj\n",
    "        ed_segmentation_argmax = torch.argmax(ed_segmentations, 1).cpu().detach().numpy()\n",
    "        es_segmentation_argmax = torch.argmax(es_segmentations, 1).cpu().detach().numpy()\n",
    "            \n",
    "        # compute dice overlap from the ed and es (predicted) segmentations to actual labeled ed and es\n",
    "        # clasfv loss funcs\n",
    "        ed_lv_dice += categorical_dice(ed_segmentation_argmax, ed_label.numpy(), 1)\n",
    "        es_lv_dice += categorical_dice(es_segmentation_argmax, es_label.numpy(), 1)\n",
    "        \n",
    "        # Printing the intermediate training statistics\n",
    "        if batch_idx % 280 == 0:\n",
    "            print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(video_clips), len(train_loader) * len(video_clips),\n",
    "                100. * batch_idx / len(train_loader), np.mean(epoch_loss)))\n",
    "\n",
    "            print(\"ED LV: {:.3f}\".format(ed_lv_dice / batch_idx))\n",
    "            print(\"ES LV: {:.3f}\".format(es_lv_dice / batch_idx))\n",
    "            \n",
    "            print(\"On a particular batch:\")\n",
    "            print(\"Deform loss: \", deform_loss)\n",
    "            print(\"Segmentation loss: \", ed_es_seg_loss)\n",
    "            print(\"Seg Motion loss: \", segmentation_loss / video_clips.shape[0], motion_loss / video_clips.shape[0])\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429fa3c-1012-420a-84f2-9df99ab4a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_loader, model, optimizer):\n",
    "    model.eval()\n",
    "    epoch_loss = []\n",
    "    ed_lv_dice = 0\n",
    "    es_lv_dice = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(test_loader, 1):\n",
    "        filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label = batch[1]\n",
    "        # tell torch we dont need gradients, since we're testing not training when creating video clips\n",
    "        with torch.no_grad():\n",
    "            video_clips = torch.Tensor(batch[0])\n",
    "            video_clips = video_clips.type(Tensor)\n",
    "\n",
    "        # Get the motion tracking output from the motion tracking head using the feature map\n",
    "        segmentation_output, motion_output = model(video_clips)\n",
    "        \n",
    "        # use warp to get predicted i-1, i+1 frames from ith frame, compare to actual frames i-1, i+1 to compute\n",
    "        # deformation motion loss\n",
    "        loss = 0\n",
    "        deform_loss = deformation_motion_loss(video_clips, motion_output)\n",
    "        loss += deform_loss\n",
    "\n",
    "    \n",
    "        segmentation_loss = 0\n",
    "        motion_loss = 0\n",
    "        \n",
    "        # loop thru 32 frame video clips \n",
    "        for i in range(video_clips.shape[0]):\n",
    "            # adjust ed and es labels, expand along dimension 1 after converting to numpy ndarray objs from tensor\n",
    "            label_ed = np.expand_dims(ed_label.numpy(), 1).astype(\"int\")\n",
    "            label_es = np.expand_dims(es_label.numpy(), 1).astype(\"int\")\n",
    "\n",
    "            # get current ed and es label\n",
    "            label_ed = label_ed[i]\n",
    "            label_es = label_es[i]\n",
    "\n",
    "            # adjust ed and es labels again, expand along dimension 0\n",
    "            label_ed = np.expand_dims(label_ed, 0)\n",
    "            label_es = np.expand_dims(label_es, 0)\n",
    "\n",
    "            # pytorch unsqueeze: \"returns a new tensor with a dimension of size one inserted at the specified position\"\n",
    "            # initialize storage vars basically \n",
    "            motion_one_output = motion_output[i].unsqueeze(0)\n",
    "            segmentation_one_output = segmentation_output[i].unsqueeze(0)\n",
    "\n",
    "            # grab ed and es frame indeces\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "\n",
    "            # compute segmentation and motion losses for curr clip\n",
    "            segmentation_one_loss, motion_one_loss = motion_seg_loss(label_ed, label_es, \n",
    "                                                                     ed_one_index, es_one_index, \n",
    "                                                                     motion_one_output, segmentation_one_output, \n",
    "                                                                     0, video_clips.shape[2], \n",
    "                                                                     F.binary_cross_entropy_with_logits)\n",
    "            # using previous unsqueezed tensors to store our curr seg and mot losses\n",
    "            segmentation_loss += segmentation_one_loss\n",
    "            motion_loss += motion_one_loss\n",
    "        \n",
    "        # after all clips have been iterated thru, avg their losses\n",
    "        # total loss looks like a sum of the averages of the seg and motion losses\n",
    "        loss += (segmentation_loss / video_clips.shape[0])\n",
    "        loss += (motion_loss / video_clips.shape[0])\n",
    "        \n",
    "        # create ed and es seg storage tensors\n",
    "        ed_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        es_segmentations = torch.Tensor([]).type(Tensor)\n",
    "        \n",
    "        # iterate over number of ed clips we have (ed_clip_index holds the indeces of all ed clip/frames ?)\n",
    "        for i in range(len(ed_clip_index)):\n",
    "            # grab curr ed and es labeled frame index\n",
    "            ed_one_index = ed_clip_index[i]\n",
    "            es_one_index = es_clip_index[i]\n",
    "            \n",
    "            # transform segmentation outputs of the ed and es predicted labels from the model\n",
    "            ed_seg = segmentation_output[i, :, ed_one_index].unsqueeze(0)\n",
    "            ed_segmentations = torch.cat([ed_segmentations, ed_seg])\n",
    "            \n",
    "            es_seg = segmentation_output[i, :, es_one_index].unsqueeze(0)\n",
    "            es_segmentations = torch.cat([es_segmentations, es_seg])\n",
    "            \n",
    "            \n",
    "        # compare ed and es predicted segmentation frames from the model with the ground truth from our datasets\n",
    "        # compute Binary Cross Entropy between target and input logits \n",
    "        # input = our segmentations\n",
    "        # target = ground truth labels, after some preprocessing (numpy, expand dims, convertTo1Hot)\n",
    "        ed_es_seg_loss = 0\n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(ed_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(ed_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\") \n",
    "        \n",
    "        ed_es_seg_loss += F.binary_cross_entropy_with_logits(es_segmentations, \n",
    "                                                             convert_to_1hot(np.expand_dims(es_label.numpy().astype(\"int\"), 1), 2), \n",
    "                                                             reduction=\"mean\") \n",
    "        # average ed and es segmentation losses\n",
    "        ed_es_seg_loss /= 2\n",
    "        \n",
    "        # add averaged ed/es segmentation losses with the previously computed segmentation and motion losses \n",
    "        loss += ed_es_seg_loss\n",
    "        \n",
    "        epoch_loss.append(loss.item())  # save final loss for this epoch (this function call)\n",
    "        \n",
    "        \n",
    "        # compute dice loss of ed and es segmentations with ground truth\n",
    "        ed_segmentation_argmax = torch.argmax(ed_segmentations, 1).cpu().detach().numpy()\n",
    "        es_segmentation_argmax = torch.argmax(es_segmentations, 1).cpu().detach().numpy()\n",
    "        \n",
    "        ed_lv_dice += categorical_dice(ed_segmentation_argmax, ed_label.numpy(), 1)\n",
    "        es_lv_dice += categorical_dice(es_segmentation_argmax, es_label.numpy(), 1)\n",
    "    \n",
    "    # print info\n",
    "    print(\"-\" * 30 + \"Validation\" + \"-\" * 30)\n",
    "    print(\"\\nED LV: {:.3f}\".format(ed_lv_dice / batch_idx))\n",
    "    print(\"ES LV: {:.3f}\".format(es_lv_dice / batch_idx))\n",
    "        \n",
    "    # Printing the intermediate training statistics\n",
    "        \n",
    "    print('\\nValid set: Average loss: {:.4f}\\n'.format(np.mean(epoch_loss)))\n",
    "    \n",
    "    return epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
