{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accurate-front",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Here I try to modify the train function so we get dropout and something using the motion field? \n",
    "\n",
    "Original functions were taken from `src/train_test.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "married-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet  # ORIGINAL MODEL\n",
    "\n",
    "# updated models\n",
    "# dropout not in place, and also might have been in a weird location.\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet \n",
    "# dropout? (didn't have forward pass defined, but still saw different outputs??)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v2_0_50_R2plus1D_18_MotionNet import dropout_v2_0_50_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v2_0_75_R2plus1D_18_MotionNet import dropout_v2_0_75_R2plus1D_18_MotionNet \n",
    "# dropout with what I think is properly defined behavior in the models.\n",
    "from src.model.dropout_v3_0_00_R2plus1D_18_MotionNet import dropout_v3_0_00_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v3_0_10_R2plus1D_18_MotionNet import dropout_v3_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v3_0_25_R2plus1D_18_MotionNet import dropout_v3_0_25_R2plus1D_18_MotionNet \n",
    "# multiple dropout layers (4)\n",
    "from src.model.dropout_v4_0_00_R2plus1D_18_MotionNet import dropout_v4_0_00_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v4_0_10_R2plus1D_18_MotionNet import dropout_v4_0_10_R2plus1D_18_MotionNet \n",
    "from src.model.dropout_v4_0_25_R2plus1D_18_MotionNet import dropout_v4_0_25_R2plus1D_18_MotionNet \n",
    "\n",
    "\n",
    "\n",
    "from src.echonet_dataset import EchoNetDynamicDataset\n",
    "from src.clasfv_losses import deformation_motion_loss, motion_seg_loss, DiceLoss, categorical_dice\n",
    "from src.train_test import train, test, train_with_log, test_with_log\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-lecture",
   "metadata": {},
   "source": [
    "## Now we need to load in data to train a new model\n",
    "### Copy and paste cells below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-netscape",
   "metadata": {},
   "source": [
    "### Load the indices of the subset data used for training and validating\n",
    "\n",
    "Subset out our Train and Validation Dataset. We exclude the EchoNet videos with no clinically denoted systolic clip or ED-ES duration > 30 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fold_indexes/stanford_train_sampled_indices\", \"rb\") as infile:\n",
    "    train_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-ministry",
   "metadata": {},
   "source": [
    "### Set up the training and validating dataset\n",
    "work initialization function is required for generating **random** 32-frame video clip in each training epoch  \n",
    "Fail to initialize the worker will cause the random 32-frame window to be the **same** for a video during every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "educated-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-african",
   "metadata": {},
   "source": [
    "### Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "editorial-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.49it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EchoNetDynamicDataset(split='train', subset_indices=train_mask, period=1)\n",
    "valid_dataset = EchoNetDynamicDataset(split='val', subset_indices=valid_mask, period=1)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              num_workers=num_workers, \n",
    "                              shuffle=True, pin_memory=(\"cuda\"), \n",
    "                              worker_init_fn=worker_init_fn,\n",
    "                              drop_last=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=False, pin_memory=(\"cuda\"),\n",
    "                              worker_init_fn=worker_init_fn_valid\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-match",
   "metadata": {},
   "source": [
    "### Instantiate the CLAS-FV Model that we want to train up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "939dac14-efa4-4e34-a836-a925f289f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models_to_train = [[\"dropout_v2-0_75-R2plus1DMotionSegNet_model.pth\", dropout_v2_0_75_R2plus1D_18_MotionNet()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-puzzle",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and Test the CLAS-FV model for ten epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "divine-fashion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_new_model(foo, log_file):\n",
    "    \n",
    "    new_model_name = foo[0]\n",
    "    model_template_obj = foo[1]\n",
    "    \n",
    "    #### Start of load new model in\n",
    "    model = torch.nn.DataParallel(model_template_obj)\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    log_file.write(f'{new_model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.\\n')\n",
    "\n",
    "    lr_T = 1e-4 \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_T)\n",
    "    \n",
    "    \n",
    "    ##### End of load new model in #####\n",
    "    \n",
    "    ######## Start of Train #####\n",
    "    \n",
    "    \n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "    model_save_path = f\"tmp_save_models/{new_model_name}\"\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    total_train_time = 0\n",
    "\n",
    "    n_epoch = 10\n",
    "    min_loss = 1e5\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        log_file.write(f\"{'-' * 32} Epoch {epoch} {'-' * 32}\\n\")\n",
    "        start = time.time()\n",
    "        train_loss = train_with_log(epoch, train_loader=train_dataloader, model=model, optimizer=optimizer, log_file = log_file)\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        end = time.time()\n",
    "        log_file.write(\"training took {:.8f} seconds\\n\".format(end-start))\n",
    "        total_train_time += (end - start)\n",
    "        valid_loss = test_with_log(epoch, test_loader=valid_dataloader, model=model, optimizer=optimizer, log_file = log_file)\n",
    "        valid_loss_list.append(np.mean(valid_loss))\n",
    "\n",
    "        if (np.mean(valid_loss) < min_loss) and (epoch > 0):\n",
    "            min_loss = np.mean(valid_loss)\n",
    "            torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, model_save_path)\n",
    "\n",
    "        if epoch == 3:\n",
    "            lr_T = 1e-5\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr_T)\n",
    "    ######### End of Train #####\n",
    "    log_file.write(f'total training took: {total_train_time // 60} m {total_train_time % 60}s\\n')\n",
    "    \n",
    "    #### cleanup\n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_loss_list\n",
    "    del valid_loss_list\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-conviction",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking/src/model/dropout_v2_0_75_R2plus1D_18_MotionNet.py\", line 62, in forward\n    cat_output = torch.cat([up_stem, up_layer_1, up_layer_2, up_layer_3, up_layer_4], 1)\nRuntimeError: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 1; 23.65 GiB total capacity; 10.89 GiB already allocated; 2.08 GiB free; 12.06 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2fb10951fa1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblah\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_models_to_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model name: {blah[0]}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_new_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblah\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3a86696bc0a7>\u001b[0m in \u001b[0;36mtrain_new_model\u001b[0;34m(foo, log_file)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training took {:.8f} seconds\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtotal_train_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_with_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking/src/train_test.py\u001b[0m in \u001b[0;36mtest_with_log\u001b[0;34m(epoch, test_loader, model, optimizer, log_file)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Get the motion tracking output from the motion tracking head using the feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msegmentation_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda3/envs/dynamic37-wang/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/wang/workspace/JupyterNoteBooksAll/fully-automated-multi-heartbeat-echocardiography-video-segmentation-and-motion-tracking/src/model/dropout_v2_0_75_R2plus1D_18_MotionNet.py\", line 62, in forward\n    cat_output = torch.cat([up_stem, up_layer_1, up_layer_2, up_layer_3, up_layer_4], 1)\nRuntimeError: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 1; 23.65 GiB total capacity; 10.89 GiB already allocated; 2.08 GiB free; 12.06 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tmp_save_models/training_log.txt\", \"a\") as log_file:\n",
    "    for blah in new_models_to_train:\n",
    "        log_file.write(f'Model name: {blah[0]}\\n\\n')\n",
    "        train_new_model(blah, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2bdae5-c170-423d-b81e-7d6680be1d82",
   "metadata": {},
   "source": [
    "## Might as well compute ED/ES LV Dice and EF's on our new models right after.\n",
    "\n",
    "### Below will calculate ED/ES LV Dice and EF for all testdataset on models in `tmp_save_models` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2876c-2e4c-447f-89b1-43fa8d6dbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "\n",
    "# v1 dropout, not in place dropout\n",
    "from src.model.dropout_0_10_R2plus1D_18_MotionNet import dropout_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_25_R2plus1D_18_MotionNet import dropout_0_25_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_50_R2plus1D_18_MotionNet import dropout_0_50_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_0_75_R2plus1D_18_MotionNet import dropout_0_75_R2plus1D_18_MotionNet\n",
    "\n",
    "# v2 dropout (in place before motion heads)\n",
    "from src.model.dropout_v2_0_00_R2plus1D_18_MotionNet import dropout_v2_0_00_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_10_R2plus1D_18_MotionNet import dropout_v2_0_10_R2plus1D_18_MotionNet\n",
    "from src.model.dropout_v2_0_25_R2plus1D_18_MotionNet import dropout_v2_0_25_R2plus1D_18_MotionNet\n",
    "\n",
    "\n",
    "\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}\n",
    "\n",
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)\n",
    "\n",
    "\n",
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "    \n",
    "\n",
    "# from queue import SimpleQueue as squeue\n",
    "def EDESpairs(diastole, systole):\n",
    "    dframes = np.sort(np.array(diastole))\n",
    "    sframes = np.sort(np.array(systole))\n",
    "    clips = []\n",
    "    \n",
    "    inds = np.searchsorted(dframes, sframes, side='left')\n",
    "    for i, sf in enumerate(sframes):\n",
    "        if inds[i] == 0: # no prior diastolic frames for this sf\n",
    "            continue\n",
    "        best_df = diastole[inds[i]-1] # diastole frame nearest this sf.\n",
    "        if len(clips) == 0 or best_df != clips[-1][0]:\n",
    "            clips.append((best_df, sf))\n",
    "            \n",
    "    return clips\n",
    "\n",
    "\n",
    "model_names = ['dropout_v2-0_50-R2plus1DMotionSegNet_model.pth', 'dropout_v2-0_75-R2plus1DMotionSegNet_model.pth', 'retrain-original-R2plus1DMotionSegNet_model.pth']\n",
    "\n",
    "\n",
    "\n",
    "loaded_in_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_save_path = f\"tmp_save_models/{model_name}\"\n",
    "    \n",
    "    # original model\n",
    "    if model_name == \"Original-Pretrained-R2plus1DMotionSegNet_model.pth\":\n",
    "         model = torch.nn.DataParallel(R2plus1D_18_MotionNet())\n",
    "    # original model just retrained\n",
    "    if model_name == 'retrain-original-R2plus1DMotionSegNet_model.pth':\n",
    "        model = torch.nn.DataParallel(R2plus1D_18_MotionNet())\n",
    "        \n",
    "    # altered models\n",
    "    if model_name == \"dropout-0_75-R2plus1DMotionSegNet_model.pth\":\n",
    "        model = torch.nn.DataParallel(dropout_0_75_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_50-R2plus1DMotionSegNet_model.pth\":\n",
    "        model = torch.nn.DataParallel(dropout_0_50_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_25-R2plus1DMotionSegNet_model.pth\":\n",
    "        model = torch.nn.DataParallel(dropout_0_25_R2plus1D_18_MotionNet())\n",
    "    if model_name == \"dropout-0_10-R2plus1DMotionSegNet_model.pth\":\n",
    "        model = torch.nn.DataParallel(dropout_0_10_R2plus1D_18_MotionNet())\n",
    "    \n",
    "    # dropout vs models\n",
    "    if model_name == 'dropout_v2-0_00-R2plus1DMotionSegNet_model.pth':\n",
    "        model = torch.nn.DataParallel(dropout_v2_0_00_R2plus1D_18_MotionNet())\n",
    "    if model_name == 'dropout_v2-0_10-R2plus1DMotionSegNet_model.pth':\n",
    "        model = torch.nn.DataParallel(dropout_v2_0_10_R2plus1D_18_MotionNet())\n",
    "    if model_name == 'dropout_v2-0_25-R2plus1DMotionSegNet_model.pth':\n",
    "        model = torch.nn.DataParallel(dropout_v2_0_25_R2plus1D_18_MotionNet())\n",
    "\n",
    "\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "    print(f'{model_name} has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "    model.eval();\n",
    "    \n",
    "    loaded_in_models.append((model_name, model))\n",
    "\n",
    "print(len(loaded_in_models))\n",
    "\n",
    "\n",
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "def segment_a_video_with_fusion(curr_model, log_file, video, interpolate_last=True, step=1, num_clips=10, \n",
    "                                fuse_method=\"simple\", class_list=[0, 1]):\n",
    "    if video.shape[1] < 32 + num_clips * step:\n",
    "        num_clips = (video.shape[1] - 32) // step\n",
    "    if num_clips < 0:\n",
    "        log_file.write(\"Video is too short\\n\")\n",
    "        num_clips = 1\n",
    "    all_consecutive_clips = []\n",
    "\n",
    "    for shift_dis in range(0, num_clips * step, step):\n",
    "        shifted_video = video[:, shift_dis:]\n",
    "        consecutive_clips = divide_to_consecutive_clips(shifted_video, interpolate_last=interpolate_last)\n",
    "        all_consecutive_clips.append(consecutive_clips)\n",
    "\n",
    "    all_consecutive_clips = np.array(all_consecutive_clips)\n",
    "    all_segmentations = []\n",
    "\n",
    "    for i in range(len(all_consecutive_clips)):\n",
    "        consecutive_clips = all_consecutive_clips[i]\n",
    "        segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "\n",
    "        for i in range(consecutive_clips.shape[0]):\n",
    "            one_clip = np.expand_dims(consecutive_clips[i], 0)\n",
    "            segmentation_output, motion_output = curr_model(torch.Tensor(one_clip))\n",
    "            segmentation_output = F.softmax(segmentation_output, 1)\n",
    "            segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        segmentation_outputs = segmentation_outputs[1:]\n",
    "\n",
    "        all_segmentations.append(segmentation_outputs)\n",
    "\n",
    "    for i in range(len(all_segmentations)):\n",
    "        all_segmentations[i] = all_segmentations[i].transpose([1, 0, 2, 3, 4])\n",
    "        all_segmentations[i] = all_segmentations[i].reshape(2, -1, 112, 112)\n",
    "\n",
    "    all_interpolated_segmentations = []\n",
    "    for i in range(0, len(all_consecutive_clips)):\n",
    "        video_clip = video[:, i * step:]\n",
    "        if interpolate_last and (video_clip.shape[1] % 32 != 0):\n",
    "            interpolated_segmentations = torch.Tensor(all_segmentations[i]).unsqueeze(0)\n",
    "            interpolated_segmentations = F.interpolate(interpolated_segmentations, size=(video_clip.shape[1], 112, 112), \n",
    "                                                       mode=\"trilinear\", align_corners=False)\n",
    "            interpolated_segmentations = interpolated_segmentations.squeeze(0).numpy()\n",
    "            all_interpolated_segmentations.append(np.argmax(interpolated_segmentations, 0))\n",
    "        else:\n",
    "            all_interpolated_segmentations.append(np.argmax(all_segmentations[i], 0))\n",
    "\n",
    "    fused_segmentations = [all_interpolated_segmentations[0][0]]\n",
    "\n",
    "    for i in range(1, video.shape[1]):\n",
    "        if step - 1 < i:\n",
    "            images_to_fuse = []\n",
    "            for index in range(min(i, len(all_interpolated_segmentations))):\n",
    "                if i - index * step < 0:\n",
    "                    break\n",
    "                images_to_fuse.append(itk.GetImageFromArray(all_interpolated_segmentations[index][i - index * step].astype(\"uint8\"),\n",
    "                                                            isVector=False))\n",
    "            if len(images_to_fuse) <= 1:\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(images_to_fuse[0]))\n",
    "            else:\n",
    "                fused_image = fuse_images(images_to_fuse, fuse_method, class_list=class_list)\n",
    "                # If using SIMPLE, the fused image might be in type \"float\"\n",
    "                # So convert it to uint\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(fused_image).astype(\"uint8\"))\n",
    "\n",
    "    fused_segmentations = np.array(fused_segmentations)\n",
    "    \n",
    "    return fused_segmentations\n",
    "\n",
    "\n",
    "def compute_ef_using_putative_clips(fused_segmentations, test_pat_index, log_file):\n",
    "    size = np.sum(fused_segmentations, axis=(1, 2)).ravel()\n",
    "    _05cut, _85cut, _95cut = np.percentile(size, [5, 85, 95]) \n",
    "\n",
    "    trim_min = _05cut\n",
    "    trim_max = _95cut\n",
    "    trim_range = trim_max - trim_min\n",
    "    systole = find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "    diastole = find_peaks(size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "\n",
    "    # keep only real diastoles..\n",
    "    diastole = [x for x in diastole if size[x] >= _85cut]\n",
    "    # Add first frame\n",
    "    if np.mean(size[:3]) >= _85cut:\n",
    "        diastole = [0] + diastole\n",
    "    diastole = np.array(diastole)\n",
    "\n",
    "    clip_pairs = EDESpairs(diastole, systole)\n",
    "\n",
    "    one_array_of_segmentations = fused_segmentations.reshape(-1, 112, 112)\n",
    "\n",
    "    predicted_efs = []\n",
    "\n",
    "    for i in range(len(clip_pairs)):\n",
    "        output_ED = one_array_of_segmentations[clip_pairs[i][0]]\n",
    "        output_ES = one_array_of_segmentations[clip_pairs[i][1]]\n",
    "        \n",
    "        length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "        length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "\n",
    "        edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "        esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "        ef_predicted = (edv - esv) / edv * 100\n",
    "        \n",
    "        if ef_predicted < 0:\n",
    "            log_file.write(\"Negative EF at patient:{:04d}\".format(test_pat_index))\n",
    "            continue\n",
    "\n",
    "        predicted_efs.append(ef_predicted)\n",
    "\n",
    "    return predicted_efs\n",
    "\n",
    "\n",
    "def compute_ef_using_reported_clip(segmentations, ed_index, es_index):\n",
    "    output_ED = segmentations[ed_index]\n",
    "    output_ES = segmentations[es_index]\n",
    "\n",
    "    lv_ed_dice = categorical_dice((output_ED), ed_label, 1)\n",
    "    lv_es_dice = categorical_dice((output_ES), es_label, 1)\n",
    "\n",
    "    length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "    length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "    \n",
    "    edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "    esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "    ef_predicted = (edv - esv) / edv * 100\n",
    "    \n",
    "    return ef_predicted, lv_ed_dice, lv_es_dice\n",
    "\n",
    "\n",
    "\n",
    "log_file_name = \"./warren-random/quantifying-performance/performance_log.txt\"\n",
    "\n",
    "with open(log_file_name, \"a\") as file:\n",
    "    for j in range(len(loaded_in_models)):\n",
    "        model = loaded_in_models[j][1]\n",
    "        model_name = loaded_in_models[j][0]\n",
    "\n",
    "        ###########\n",
    "        patient_filename = []\n",
    "\n",
    "        EF_list = []\n",
    "        true_EF_list = []\n",
    "        mean_EF_list = []\n",
    "\n",
    "        lv_ed_dice = []\n",
    "        lv_es_dice = []\n",
    "\n",
    "        num_clips = 5\n",
    "        step = 1\n",
    "        interpolate_last = True\n",
    "        fuse_method = \"simple\"\n",
    "        class_list = [0, 1]\n",
    "        # class_list = None\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(test_dataset)):\n",
    "            test_pat_index = i\n",
    "            try:\n",
    "                video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "            except:\n",
    "                file.write(\"Get exception when trying to read the video from patient:{:04d}\\n\".format(i))\n",
    "                continue\n",
    "\n",
    "            if test_pat_index == 1053:\n",
    "                video = video[:, :80]\n",
    "\n",
    "            segmentations = segment_a_video_with_fusion(model, file, video, interpolate_last=interpolate_last, \n",
    "                                                        step=step, num_clips=num_clips,\n",
    "                                                        fuse_method=fuse_method, class_list=class_list)\n",
    "\n",
    "            predicted_efs = compute_ef_using_putative_clips(segmentations, test_pat_index=test_pat_index, log_file=file)\n",
    "\n",
    "            _, ed_dice, es_dice = compute_ef_using_reported_clip(segmentations, ed_index, es_index)\n",
    "\n",
    "            lv_ed_dice.append(ed_dice)\n",
    "            lv_es_dice.append(es_dice)\n",
    "\n",
    "            if len(predicted_efs) == 0:\n",
    "                file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "                continue\n",
    "\n",
    "            if np.isnan(np.nanmean(predicted_efs)):\n",
    "                file.write(\"Cannot identify clips at patient:{:04d}\\n\".format(test_pat_index))\n",
    "                continue\n",
    "\n",
    "            EF_list.append(predicted_efs)\n",
    "            true_EF_list.append(EF)\n",
    "            mean_EF_list.append(np.nanmean(predicted_efs))\n",
    "            patient_filename.append(filename[:-4])\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        file.write(f\"Model Name: {model_name}\\n\")\n",
    "        file.write(f\"Used time = {(end - start) // 60:.0f} mins {(end - start) % 60:.0f} secs\\n\")\n",
    "        ## ------------start of --------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "        errors = np.array(np.array(true_EF_list) - np.array(mean_EF_list))\n",
    "        abs_errors = abs(errors)\n",
    "\n",
    "        file.write(\"Mean absolute error (standard deviation):  {:.4f} ({:.4f}) %\\n\".format(np.mean(abs_errors), np.std(abs_errors)))\n",
    "        file.write(\"Median absolute error:  {:.4f} %\\n\".format(np.median(abs_errors)))\n",
    "        file.write(\"Bias +- 1.96 x std:  {:.4f} +- {:.4f}\\n\".format(np.mean(errors), 1.96 * np.std(errors)))\n",
    "        file.write(\"Percentile of mae 50%: {:6.4f}  75%: {:6.4f}  95%: {:6.4f}\\n\".format(np.percentile(abs_errors, 50), np.percentile(abs_errors, 75),\n",
    "                                                                            np.percentile(abs_errors, 95)))\n",
    "\n",
    "\n",
    "        file.write(\"Average ED {:.4f} ({:.4f})\\n\".format(np.mean(lv_ed_dice), np.std(lv_ed_dice)))\n",
    "        file.write(\"Median ED {:.4f}\\n\".format(np.median(lv_ed_dice)))\n",
    "        file.write(\"Average ES {:.4f} ({:.4f})\\n\".format(np.mean(lv_es_dice), np.std(lv_es_dice)))\n",
    "        file.write(\"Median ES {:.4f}\\n\".format(np.median(lv_es_dice)))\n",
    "        ## ---------------end of ----------- Label fusion of 5 clips with step 1 using full video segmentation\n",
    "        file.write('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab8a2b-9c74-4124-b944-8464de5ff800",
   "metadata": {},
   "source": [
    "## After performance check on new models in tmp folder, move all models in `tmp_save_model` into `save_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd3856-05f9-4670-9011-87e3dde2949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!for i in $(ls ./tmp_save_models | grep .pth); do mv ./tmp_save_models/$i ./save_models; done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
